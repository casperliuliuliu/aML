{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "dataset_path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\MNIST_data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 60000\n",
      "Filtered dataset: 24559\n"
     ]
    }
   ],
   "source": [
    "def filter_indices(dataset, classes):\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.targets[i] in classes:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "full_dataset = datasets.MNIST(root=dataset_path, train=True, download=False, transform=transform)\n",
    "\n",
    "filtered_indices = filter_indices(full_dataset, [1, 3, 5, 7])\n",
    "filtered_dataset = Subset(full_dataset, filtered_indices)\n",
    "print(f\"Full dataset: {len(full_dataset)}\")\n",
    "print(f\"Filtered dataset: {len(filtered_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_image_and_tensor(dataset, index):\n",
    "    image, label = dataset[index]  # Get the first image and its label\n",
    "    \n",
    "    print(\"Label:\", label)\n",
    "    image_np = image.squeeze().numpy()  \n",
    "\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()\n",
    "\n",
    "display_image_and_tensor(full_dataset, 1)\n",
    "display_image_and_tensor(filtered_dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_two_tensors(tensor1, tensor2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))  # Create a figure with two subplots\n",
    "\n",
    "    axes[0].imshow(tensor1.squeeze(), cmap='gray')  # Remove channel dimension if exists and plot\n",
    "    axes[0].set_title(f'Before:')\n",
    "    axes[0].axis('off')  # Hide axes ticks\n",
    "\n",
    "    axes[1].imshow(tensor2.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(f'After:')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "plot_two_tensors(full_dataset[0][0], full_dataset[1][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [12235, 16413, 3163, 5348, 21316]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 24559 datapoints\n",
      "Number of train samples: 307 batches/ 19647 datapoints\n",
      "Number of val samples: 77 batches/ 4912 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [56774, 32710, 56427, 49406, 20372]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 188 batches/ 12000 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders\n",
    "dataloaders = get_dataloaders(filtered_dataset, 0.8, 1, 64, 6452916)\n",
    "full_dataloaders = get_dataloaders(full_dataset, 0.8, 1, 64, 6452916)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Output: 16 x 28 x 28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                # Output: 16 x 14 x 14\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),# Output: 32 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, len([1, 3, 5, 7]))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_net(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {1: 0, 3: 1, 5: 2, 7: 3}\n",
    "\n",
    "def transform_labels(labels):\n",
    "    transformed_labels = torch.tensor([label_mapping[label.item()] for label in labels])\n",
    "    return transformed_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DigitClassifier\n",
      "model total parameters: 206,148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/307 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m transform_labels(labels)\n\u001b[0;32m     17\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 19\u001b[0m, in \u001b[0;36mDigitClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_net(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DigitClassifier().to(device)\n",
    "print(f\"Model: DigitClassifier\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloaders['train']):\n",
    "        labels = transform_labels(labels)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Accuracy: {100 * correct / total:.2f}%, Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloaders['val']):\n",
    "            labels = transform_labels(labels)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Accuracy: {100 * correct / total:.2f}%, Loss: {running_loss / total}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abnormal Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 188/188 [00:01<00:00, 167.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 57.96% \n",
      "Anomaly_threshold: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 188/188 [00:01<00:00, 167.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 56.97% \n",
      "Anomaly_threshold: 0.5\n",
      "\n",
      "Anomaly detection accuracy: 57.96% \n",
      "Anomaly_threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normal_labels = torch.tensor([1, 3, 5, 7], device=device)\n",
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "segment_size = 2\n",
    "for ii in range(segment_size):\n",
    "    anomaly_threshold = ii / float(segment_size)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    anomalies = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(full_dataloaders['val']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            max_probs, predictions = torch.max(probabilities, dim=1)\n",
    "            anomaly_mask = max_probs > anomaly_threshold\n",
    "\n",
    "            # Check if each label in the batch is from the trained set (1, 3, 5, 7)\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (is_normal & ~anomaly_mask) | (~is_normal & anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_anomaly_threshold = anomaly_threshold\n",
    "    print(f'Anomaly detection accuracy: {accuracy:.2f}% \\nAnomaly_threshold: {anomaly_threshold}\\n')\n",
    "\n",
    "print(f'Anomaly detection accuracy: {best_acc:.2f}% \\nAnomaly_threshold: {best_anomaly_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train normal autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoencoder\n",
      "model total parameters: 219,804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 137.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 0.01524902626643771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 135.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 135.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 135.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 127.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 128.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 175.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 128.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Loss: 0.014590421875554526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Loss: 0.014590421435656654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014633682705097944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder().to(device)\n",
    "print(f\"Model: Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # images.requires_grad_()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            # images.requires_grad_()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 63\n",
    "with torch.no_grad():\n",
    "    for images, _ in dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        print(len(outputs))\n",
    "        print(outputs.shape)\n",
    "        plot_two_tensors(images[index].cpu().view(28,28), outputs[index].cpu().view(28,28))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 57.96% at threshold: 0.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.05\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.1\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.15\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.2\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.25\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.3\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.35\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.4\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.45\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.5\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.55\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.6\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.65\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.7\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.75\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.8\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.85\n",
      "Anomaly detection accuracy: 55.02% at threshold: 0.9\n",
      "Anomaly detection accuracy: 31.39% at threshold: 0.95\n",
      "Best anomaly detection accuracy: 57.96% at threshold: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "segment_size = 20\n",
    "for ii in range(segment_size):\n",
    "    anomaly_threshold = ii / float(segment_size)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "            \n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Denoise_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "    return noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Denoise_Autoencoder\n",
      "model total parameters: 219,804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 0.015605006490025792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.01463321721369358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Loss: 0.014589000419908524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014627362416407962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Loss: 0.014582626800315765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.01462507122939495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Loss: 0.014576504131840792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.01461797724523451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Loss: 0.014574568323332548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014617373517650734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Loss: 0.014569943470706611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014609281507590695\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 130.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Loss: 0.014565554203110724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014608634848159765\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Loss: 0.014565073437209456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014608074974644844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Loss: 0.014563034550011561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014604828402351479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Loss: 0.014561366651442515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014604558421948056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Loss: 0.014561131466871218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 175.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014604212940709987\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Loss: 0.014551357891794172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014588254464468662\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Loss: 0.014532082343687627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014547040461523136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 134.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Loss: 0.014462813983019625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 187.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014484296634744744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Loss: 0.014435312202117908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.01446522528408793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 135.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Loss: 0.01440462237212132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014429672314794521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Loss: 0.014371461402176416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014401374876207948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Loss: 0.01435110284371152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014386484417639648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 132.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Loss: 0.014335985622885586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 179.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014371770625199868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:02<00:00, 133.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Loss: 0.014325008948364895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 183.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.014364577016826561\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Denoise_Autoencoder().to(device)\n",
    "print(f\"Model: Denoise_Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images)\n",
    "\n",
    "        images = images.view(images.size(0), -1)\n",
    "        noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "\n",
    "        outputs = model(noisy_images)\n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            noisy_images = add_noise(images)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            noisy_images = noisy_images.view(images.size(0), -1)\n",
    "            \n",
    "            outputs = model(noisy_images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_tensors(tensor1, tensor2, tensor3):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))  # Create a figure with two subplots\n",
    "\n",
    "    axes[0].imshow(tensor1.squeeze(), cmap='gray')  # Remove channel dimension if exists and plot\n",
    "    axes[0].set_title(f'Before:')\n",
    "    axes[0].axis('off')  # Hide axes ticks\n",
    "\n",
    "    axes[1].imshow(tensor2.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(f'Add Noise:')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(tensor3.squeeze(), cmap='gray')\n",
    "    axes[2].set_title(f'After:')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 63\n",
    "with torch.no_grad():\n",
    "    for images, label in dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "        \n",
    "        outputs = model(noisy_images)\n",
    "        print(\"Label:\", label[index].item())\n",
    "        print(outputs.shape)\n",
    "        plot_three_tensors(images[index].cpu().view(28,28), noisy_images[index].cpu().view(28,28), outputs[index].cpu().view(28,28))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 57.96% at threshold: 0.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.05\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.1\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.15\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.2\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.25\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.3\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.35\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.4\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.45\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.5\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.55\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.6\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.65\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.7\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.75\n",
      "Anomaly detection accuracy: 57.96% at threshold: 0.8\n",
      "Anomaly detection accuracy: 57.97% at threshold: 0.85\n",
      "Anomaly detection accuracy: 49.30% at threshold: 0.9\n",
      "Anomaly detection accuracy: 41.42% at threshold: 0.95\n",
      "Best anomaly detection accuracy: 57.97% at threshold: 0.85\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "segment_size = 20\n",
    "for ii in range(segment_size):\n",
    "    anomaly_threshold = ii / float(segment_size)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            noisy_images = add_noise(images)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            images = images.view(images.size(0), -1)\n",
    "            noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "\n",
    "            outputs = model(noisy_images)\n",
    "\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "            \n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.fc_mean = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, 400)\n",
    "        self.fc3 = nn.Linear(400, input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc_mean(h1), self.fc_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h2 = F.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))  # Assuming input is 28x28 (for MNIST)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    assert torch.max(recon_x) <= 1 and torch.min(recon_x) >= 0, \"Recon_x out of bounds\"\n",
    "    assert torch.max(x) <= 1 and torch.min(x) >= 0, \"x out of bounds\"\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VAE\n",
      "model total parameters: 652,824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [00:01<00:00, 198.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 159.85039641071157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77/77 [00:00<00:00, 308.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 123.94095107087871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "latent_dim = 20  # Latent dimensionality\n",
    "input_dim = 784  # 28x28 images flattened\n",
    "model = VAE(input_dim,latent_dim).to(device)\n",
    "print(f\"Model: VAE\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        recon_batch, mu, logvar = model(images)\n",
    "        loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(images)\n",
    "            loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in full_dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(images)\n",
    "        print(recon_batch.shape)\n",
    "        plot_two_tensors(images[index].cpu().view(28,28), recon_batch[index].cpu().view(28,28))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 57.96% at threshold: 1000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 2000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 3000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 4000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 5000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 6000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 7000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 8000.0\n",
      "Anomaly detection accuracy: 57.96% at threshold: 9000.0\n",
      "Anomaly detection accuracy: 57.79% at threshold: 10000.0\n",
      "Best anomaly detection accuracy: 57.96% at threshold: 1000.0\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "normal_labels = torch.tensor([1, 3, 5, 7], device=device)\n",
    "\n",
    "segment_size = 10 \n",
    "for ii in range(segment_size):\n",
    "    anomaly_threshold = ii / float(segment_size) * 10000 + 1000\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            recon_batch, mu, logvar = model(images)\n",
    "\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Isolated Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.06      0.06      0.06       180\n",
      "           1       0.90      0.90      0.90      1617\n",
      "\n",
      "    accuracy                           0.81      1797\n",
      "   macro avg       0.48      0.48      0.48      1797\n",
      "weighted avg       0.81      0.81      0.81      1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data  # Feature matrix where each row is a flattened image\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.1), random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X)\n",
    "\n",
    "# Predictions on the training data\n",
    "y_pred_train = clf.predict(X)\n",
    "\n",
    "# -1 for outliers and 1 for inliers.\n",
    "outliers = X[y_pred_train == -1]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume we have some labels indicating 1 for normal and -1 for anomalies\n",
    "true_labels = np.full(X.shape[0], 1)\n",
    "true_labels[::10] = -1  # Example: mark every 10th sample as an outlier\n",
    "\n",
    "print(classification_report(true_labels, y_pred_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Isolated Forrest with pre-trained feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
