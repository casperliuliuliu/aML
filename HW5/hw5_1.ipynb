{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import classification_report\n",
    "dataset_path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\MNIST_data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 60000\n",
      "Filtered dataset: 24559\n"
     ]
    }
   ],
   "source": [
    "def filter_indices(dataset, classes):\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.targets[i] in classes:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "full_dataset = datasets.MNIST(root=dataset_path, train=True, download=False, transform=transform)\n",
    "\n",
    "filtered_indices = filter_indices(full_dataset, [1, 3, 5, 7])\n",
    "filtered_dataset = Subset(full_dataset, filtered_indices)\n",
    "print(f\"Full dataset: {len(full_dataset)}\")\n",
    "print(f\"Filtered dataset: {len(filtered_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_image_and_tensor(dataset, index):\n",
    "    image, label = dataset[index]  # Get the first image and its label\n",
    "    \n",
    "    print(\"Label:\", label)\n",
    "    image_np = image.squeeze().numpy()  \n",
    "\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()\n",
    "\n",
    "display_image_and_tensor(full_dataset, 1)\n",
    "display_image_and_tensor(filtered_dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_two_tensors(tensor1, tensor2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))  # Create a figure with two subplots\n",
    "\n",
    "    axes[0].imshow(tensor1.squeeze(), cmap='gray')  # Remove channel dimension if exists and plot\n",
    "    axes[0].set_title(f'Before:')\n",
    "    axes[0].axis('off')  # Hide axes ticks\n",
    "\n",
    "    axes[1].imshow(tensor2.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(f'After:')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "plot_two_tensors(full_dataset[0][0], full_dataset[1][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [7928, 6963, 6078, 12524, 11416]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 24559 datapoints\n",
      "Number of train samples: 307 batches/ 19647 datapoints\n",
      "Number of val samples: 77 batches/ 4912 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [41608, 20621, 29401, 29192, 11631]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 188 batches/ 12000 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders\n",
    "dataloaders = get_dataloaders(filtered_dataset, 0.8, 1, 64, 666)\n",
    "full_dataloaders = get_dataloaders(full_dataset, 0.8, 1, 64, 666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Output: 16 x 28 x 28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                # Output: 16 x 14 x 14\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),# Output: 32 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, len([1, 3, 5, 7]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_net(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {1: 0, 3: 1, 5: 2, 7: 3}\n",
    "\n",
    "def transform_labels(labels):\n",
    "    transformed_labels = torch.tensor([label_mapping[label.item()] for label in labels])\n",
    "    return transformed_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DigitClassifier\n",
      "model total parameters: 206,148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 109.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Accuracy: 96.60%, Loss: 0.001476280243835085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 163.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.17%, Loss: 0.000462797638519928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Accuracy: 99.33%, Loss: 0.000332061496989632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 208.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.25%, Loss: 0.0004173415026690334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 123.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Accuracy: 99.44%, Loss: 0.00027719731000243775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 202.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.12%, Loss: 0.0005808669945393343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 128.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Accuracy: 99.61%, Loss: 0.00015913603088181364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 167.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.39%, Loss: 0.0004007304664943676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 122.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Accuracy: 99.68%, Loss: 0.000154874956995257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 240.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.19%, Loss: 0.0006171989073236572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 114.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Accuracy: 99.69%, Loss: 0.0001647646231939415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 175.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.04%, Loss: 0.0007820705905848429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Accuracy: 99.66%, Loss: 0.0001605367180623339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 256.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.17%, Loss: 0.0006000938243537808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Accuracy: 99.73%, Loss: 0.00012714832299670726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 202.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.35%, Loss: 0.000623743294167161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 126.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Accuracy: 99.81%, Loss: 0.00010010011794269735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 256.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.14%, Loss: 0.0005613523454503548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 135.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Accuracy: 99.91%, Loss: 3.8295681671658965e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 256.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.29%, Loss: 0.0006048665941830522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 124.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Accuracy: 99.83%, Loss: 7.107943000411764e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 163.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.43%, Loss: 0.0007090218739110645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Accuracy: 99.69%, Loss: 0.00012840658532531138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 175.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.29%, Loss: 0.0008017047235645561\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Accuracy: 99.80%, Loss: 9.268568126569696e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 171.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.49%, Loss: 0.0006176646055741184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 127.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Accuracy: 99.84%, Loss: 6.698195227559004e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 179.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.57%, Loss: 0.0005384372929249028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 127.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Accuracy: 99.89%, Loss: 5.7285271152931635e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 157.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.31%, Loss: 0.0007006610046797455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Accuracy: 99.79%, Loss: 0.0001264447900043468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 192.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.43%, Loss: 0.0006493377341250359\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 130.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Accuracy: 99.90%, Loss: 5.4634662063376906e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 171.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.53%, Loss: 0.0005508033263459112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Accuracy: 99.93%, Loss: 3.298686145770722e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 240.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.47%, Loss: 0.0008189570451003433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 138.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Accuracy: 99.91%, Loss: 5.096133018306114e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 192.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.63%, Loss: 0.0006104458307364895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Accuracy: 99.84%, Loss: 0.00011279166199204552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 137.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Accuracy: 99.39%, Loss: 0.00047607632021803007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DigitClassifier().to(device)\n",
    "print(f\"Model: DigitClassifier\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloaders['train']):\n",
    "        labels = transform_labels(labels)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Accuracy: {100 * correct / total:.2f}%, Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloaders['val']):\n",
    "            labels = transform_labels(labels)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Accuracy: {100 * correct / total:.2f}%, Loss: {running_loss / total}\\n')\n",
    "\n",
    "model_scripted = torch.jit.script(model.cpu()) # Export to TorchScript\n",
    "model_scripted.save('model_image_classifier.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abnormal Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:01<00:00, 167.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 57.96% \n",
      "Anomaly_threshold: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:01<00:00, 167.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 56.97% \n",
      "Anomaly_threshold: 0.5\n",
      "\n",
      "Anomaly detection accuracy: 57.96% \n",
      "Anomaly_threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normal_labels = torch.tensor([1, 3, 5, 7], device=device)\n",
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "\n",
    "segment_size = 2\n",
    "for ii in range(segment_size):\n",
    "    anomaly_threshold = ii / float(segment_size)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    anomalies = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(full_dataloaders['val']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            max_probs, predictions = torch.max(probabilities, dim=1)\n",
    "            anomaly_mask = max_probs > anomaly_threshold\n",
    "\n",
    "            # Check if each label in the batch is from the trained set (1, 3, 5, 7)\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (is_normal & ~anomaly_mask) | (~is_normal & anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_anomaly_threshold = anomaly_threshold\n",
    "    print(f'Anomaly detection accuracy: {accuracy:.2f}% \\nAnomaly_threshold: {anomaly_threshold}\\n')\n",
    "\n",
    "print(f'Anomaly detection accuracy: {best_acc:.2f}% \\nAnomaly_threshold: {best_anomaly_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train normal autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoencoder\n",
      "model total parameters: 219,804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 134.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 0.0009712617382260808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 248.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0006130127371243332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 132.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Loss: 0.0004822666321982984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 175.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0004117119851058003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 132.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Loss: 0.00037238030028084116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 213.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.000347565846284093\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 137.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Loss: 0.0003246845969590982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 220.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0003076923706990451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 130.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Loss: 0.00029230593778725803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 175.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.000280437401551996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 130.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Loss: 0.00027071089864584146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 248.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00026198755456771736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 130.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Loss: 0.0002548785832046783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 233.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00024977523730370044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 140.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Loss: 0.00024271955454905307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 187.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00024037175290043852\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 136.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Loss: 0.00023214606325609808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 213.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00023082806975262222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 149.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Loss: 0.0002233603876401688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 248.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0002232209725475321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 136.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Loss: 0.00021591459681054935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 179.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0002160630045164413\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 127.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Loss: 0.00020873278916798312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 265.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00020916830044518853\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 142.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Loss: 0.00020197274925317778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 274.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00020368282451121446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 149.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Loss: 0.00019645932553680263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 202.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0001994537209321408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 144.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Loss: 0.00019200286339874964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 275.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00019569257035372166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:01<00:00, 158.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Loss: 0.00018823424434495918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 197.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0001928719059800796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 144.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Loss: 0.0001850560272045461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 275.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00019004234702080634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 144.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Loss: 0.0001821605697325024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 233.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00018714822571670048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 144.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Loss: 0.00017958891293080507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 275.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0001850396623009822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Loss: 0.00017716867125200584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 275.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0001832043110462761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder().to(device)\n",
    "print(f\"Model: Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # images.requires_grad_()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            # images.requires_grad_()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')\n",
    "model_scripted = torch.jit.script(model.cpu()) # Export to TorchScript\n",
    "model_scripted.save('model_AE.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu4AAAFeCAYAAADaP5oiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb0klEQVR4nO3de4zV9Z3/8feZO2AUUEZcWB0BbbG0tZWuRrxAKrpbaKPW0DWx1U1s3DXupbtK1ESRNdG4xq7s1lrcumLpVKpGGxuLt1VrL7Qs2dC6bKmj0XqpIoyC3GE45/eHcX7L4uX9NZ4OH3w8EpL2zJPDh5lxzsuvA99ao9FoBAAAsFdrGeoDAAAA781wBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwZ0hcf/31MWHChGhtbY2jjz56qI8DwIfM4sWL46Mf/Wi0t7fHyJEjh/o4kGK4864WLVoUtVpttx/d3d0xY8aMWLp06ft6zoceeijmzp0b06ZNi9tuuy2uueaaD/jUAHzYffOb34xarRbHHnvsHm9bvXp1nHfeeTFx4sT4t3/7t7jllltiy5YtcdVVV8Xjjz/+hz8sJLUN9QEowz/+4z/G4YcfHo1GI9asWROLFi2Kz33uc/HDH/4wZs+eXem5Hn300WhpaYlbb701Ojo6mnRiAD7Ment7o6enJ5YvXx5PP/10TJo0afBtjz/+eNTr9ViwYMHg4+vWrYv58+dHRMT06dOH4sjwnlxxJ+XP/uzP4pxzzokvf/nLcfHFF8dPfvKTaG9vjzvuuKPyc7366qsxbNiwD2y0NxqN2Lp16wfyXACU79lnn42f//zn8fWvfz3GjBkTvb29u7391VdfjYj4g3yLzObNm5v+a/DhYbjzvowcOTKGDRsWbW3//z/a1Ov1uPHGG+NjH/tYdHV1xcEHHxwXXHBBvP7664NNrVaL2267LTZv3jz4rTeLFi2KiIiBgYG4+uqrY+LEidHZ2Rk9PT1x+eWXx/bt23f7tXt6emL27Nnx4IMPxtSpU2PYsGGxcOHCiIhYv359/N3f/V388R//cXR2dsakSZPiuuuui3q9vttzvPzyy7F69erYuXNnk95DAAyV3t7eGDVqVMyaNSvOOuus3YZ7T09PzJs3LyIixowZE7VaLc4777wYM2ZMRETMnz9/8PXpqquuGvx5q1evjrPOOitGjx4dXV1dMXXq1Ljvvvt2+3Xf+vbSH//4x3HhhRdGd3d3jB8/PiIitmzZEqtXr45169Y1+XfPvsxwJ2XDhg2xbt26WLt2baxatSr+6q/+KjZt2hTnnHPOYHPBBRfEJZdcEtOmTYsFCxbEX/zFX0Rvb2+cdtppgwN58eLFceKJJ0ZnZ2csXrw4Fi9eHCeddFJERJx//vlx5ZVXxqc//en453/+5zj55JPj2muvjT//8z/f4zy//e1v4+yzz46ZM2fGggUL4uijj44tW7bEySefHN/97nfjK1/5SvzLv/xLTJs2LS677LL4+7//+91+/mWXXRaTJ0+Ol156qYnvNQCGQm9vb5x55pnR0dERZ599dvT19cV//ud/RkTEjTfeGGeccUZERNx8882xePHi+NrXvhY333xzREScccYZg69PZ555ZkRErFq1Ko477rj4zW9+E5deemnccMMNMWLEiDj99NPj3nvv3ePXv/DCC+N//ud/4sorr4xLL700IiKWL18ekydPjm984xt/iHcB+6oGvIvbbrutERF7/Ojs7GwsWrRosPvJT37SiIhGb2/vbj//gQce2OPxc889tzFixIjdupUrVzYionH++efv9vjFF1/ciIjGo48+OvjYYYcd1oiIxgMPPLBbe/XVVzdGjBjReOqpp3Z7/NJLL220trY2nn/++d3OEBGNZ599tto7BIC92ooVKxoR0Xj44YcbjUajUa/XG+PHj2/87d/+7WAzb968RkQ01q5dO/jY2rVrGxHRmDdv3h7P+dnPfrbx8Y9/vLFt27bBx+r1euP4449vHHHEEYOPvfWaecIJJzQGBgZ2e47HHnvsHZ8fslxxJ+Wmm26Khx9+OB5++OH47ne/GzNmzIjzzz8/7rnnnoiIuOuuu+KAAw6ImTNnxrp16wZ/HHPMMbHffvvFY4899q7P/6Mf/SgiYo8r4//wD/8QERH333//bo8ffvjhcdppp+322F133RUnnnhijBo1arcznHLKKbFr16544oknBttFixZFo9GInp6e9/X+AGDv1NvbGwcffHDMmDEjIt78Fs0vfelLsWTJkti1a1fl53vttdfi0UcfjTlz5sTGjRsHX1v6+/vjtNNOi76+vj3+6+1Xv/rVaG1t3e2x6dOnR6PR2O3bb6Aqf6sMKX/yJ38SU6dOHfz/Z599dnzqU5+Kiy66KGbPnh19fX2xYcOG6O7uftuf/9YfBHonv/vd76KlpWW3P/UfETF27NgYOXJk/O53v9vt8cMPP3yP5+jr64tf//rXg9+nWPUMAJRt165dsWTJkpgxY0Y8++yzg48fe+yxccMNN8R//Md/xKmnnlrpOZ9++uloNBpxxRVXxBVXXPG2zauvvhrjxo0b/P9v9xoFHwTDnfelpaUlZsyYEQsWLIi+vr6o1+vR3d29x5/cf8s7jen/q1arpbphw4bt8Vi9Xo+ZM2fG3Llz3/bnHHnkkannBqBMjz76aLz88suxZMmSWLJkyR5v7+3trTzc3/rLDS6++OI9/kvvW/7vRae3e42CD4Lhzvs2MDAQERGbNm2KiRMnxiOPPBLTpk17X1+wDjvssKjX69HX1xeTJ08efHzNmjWxfv36OOyww97zOSZOnBibNm2KU045pfKvD0D5ent7o7u7O2666aY93nbPPffEvffeG9/61rfe9ue+04WjCRMmREREe3u71xeGnO9x533ZuXNnPPTQQ9HR0RGTJ0+OOXPmxK5du+Lqq6/eox0YGIj169e/6/N97nOfi4g3/7T///b1r389IiJmzZr1nmeaM2dOLFu2LB588ME93rZ+/frBf9GI8NdBAuxrtm7dGvfcc0/Mnj07zjrrrD1+XHTRRbFx48Y9/grHtwwfPjwiYo/Xq+7u7pg+fXosXLgwXn755T1+3tq1a1Pn89dB8kFwxZ2UpUuXxurVqyPize/l+973vhd9fX1x6aWXxv777x8nn3xyXHDBBXHttdfGypUr49RTT4329vbo6+uLu+66KxYsWBBnnXXWOz7/Jz/5yTj33HPjlltuifXr18fJJ58cy5cvj9tvvz1OP/30wT9k9G4uueSSuO+++2L27Nlx3nnnxTHHHBObN2+OJ598Mu6+++547rnn4qCDDoqIN/86yNtvvz2effZZf0AVYB9w3333xcaNG+MLX/jC2779uOOOG7wZ06c//ek93j5s2LA46qij4vvf/34ceeSRMXr06JgyZUpMmTIlbrrppjjhhBPi4x//eHz1q1+NCRMmxJo1a2LZsmXx4osvxq9+9av3PN/y5ctjxowZMW/ePH9AlffNcCflyiuvHPzfXV1d8dGPfjRuvvnmuOCCCwYf/9a3vhXHHHNMLFy4MC6//PJoa2uLnp6eOOecc2LatGnv+Wt8+9vfjgkTJsSiRYvi3nvvjbFjx8Zll102eKOM9zJ8+PD48Y9/HNdcc03cdddd8Z3vfCf233//OPLII2P+/PlxwAEHVP+NA1CE3t7e6OrqipkzZ77t21taWmLWrFnR29v7jt9++e1vfzv++q//Or72ta/Fjh07Yt68eTFlypQ46qijYsWKFTF//vxYtGhR9Pf3R3d3d3zqU5/a7fURmq3WaDQaQ30IAADg3fkedwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFAAwx0AAAqQvgFTrVZr5jkA/qDcwmLf4LUJ2FdkXpdccQcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAoQNtQH4B93913351uzzjjjErP3dKS/3fPBx98MN1eddVV6fYXv/hFugWgPB0dHem2q6sr3R566KHp9rnnnku3mzZtSreUxRV3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFqDUajUYqrNWafRYKctxxx6Xbhx56KN0OHz680jmqfF4mP9UjImLDhg3p9rrrrku3//RP/5Ruaa4qnw/svfbl16ZmfX1raclfs6vX6+m2ivb29nRb5byHHHJIup06dWq6bW1tTbeTJk1Kt0cddVS6/chHPpJuq3zu3H///en29ttvT7fPPPNMuuVNmX+OXXEHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFCAtqE+AGWaPHlyuh0+fHgTT9IcBxxwQLr98pe/nG5vuummdLt58+Z0C+x7Mrc/f0tLS/46XJXnbW9vT7cdHR3pdmBgIN22teWnSpXf2+9///t0O2fOnHT7xhtvpNtNmzal26VLl6bbVatWpdunnnoq3fb396fbYcOGpdt6vZ5ud+3alW6rfJ6VwhV3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFyN9HGPZCy5cvT7c33HBDuv3Lv/zLdDt9+vQhPwNAM1S5Ff22bduaeJKctWvXptsq5/3e976Xbs8888x0u2bNmqa0L7zwQrp9/vnn0+2WLVvS7c6dO9Nto9FItx92rrgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAAChArZG8z2ytVmv2WSjI+PHj0+3ChQvT7a9+9atK57j88ssr9VknnXRSur3zzjvTbWtra7qdPn16ul21alW65U1usb1v8NrUXFXev836WLS1tTWlbW9vT7eHHHJIuh03blxTnrfK723Tpk3p9mc/+1m67e/vT7c7d+5Mt74evynzfnDFHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAfL3z4X/5cUXX0y3s2bNauJJmuOJJ55It//+7/+ebufOnZtuR40alW4BmqHKreirtLVaLd0ODAyk2yp27dqVbtevX59ut27dmm47OjrSbVdXV7qt1+vptsrHokpb5fOBPFfcAQCgAIY7AAAUwHAHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUoG2oDwClu/POO9Pt3Llzm3gSgDI0Go10W6vVmnKG4cOHp9uurq50O3LkyHTb3d2dbseMGZNuX3rppXRbr9eb0tIcrrgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAAChA21AfAAD4cKnVaum2rS0/VUaNGpVuu7q60u3IkSPT7ZQpU9Lt2LFj0+348ePT7caNG9NtT09Put2wYUO63blzZ7olzxV3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFyN9HGADgA9Da2ppuR48enW7b2vKzZtiwYem2ynmrtFOmTEm3Y8eOTbdTp05NtytWrEi3r7zySrp96aWX0u3AwEC6bTQa6XZf5Io7AAAUwHAHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIAC5O8NDADwDlpa8tcC99tvv6acob+/P92uXbs23XZ1daXb9vb2dPulL30p3f7RH/1Ruh03bly6XbNmTbo99NBD0+26devSbaPRSLfNMjAwkG6rfK7X6/X3c5x3/rU/0GcDAACawnAHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACtA31AaB0nZ2d6bZWqzXxJABDp8pt4Ldt25Zut2zZkm6r3La+iiq3rX/ttdfS7Q9+8IN029PTk25PPfXUdLtixYp0W+XjVkWV92+j0WhKW0WV837QXHEHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFCAtqE+AG9qb29Pt/vvv3+67e/vfz/H+dAbOXJkur388svT7fbt29Ptjh070i1AM7S05K/vVWlHjx6dbjdu3Jhuq9yKvkpb5fc2fPjwdLty5cp0+8ILL6Tb559/Pt0ODAyk2zfeeCPdtra2plvyXHEHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFCAtqE+wL6so6Mj3c6fPz/dfuUrX0m3DzzwQLpdsWJFur355pvT7d6irS3/6X7LLbek21mzZqXb++67L90uX7483QL7nlqt1pS2s7Mz3Y4YMSLdjh49Ot1+4hOfSLeHHHJIuv3v//7vdLthw4Z0W+X1vMr7t8rzTpw4Md1OmjQp3XZ3d6fbKq+jjz/+eLqt1+vpdvv27el2x44d6bYUrrgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAAChArdFoNFJhhdsp86a5c+em22uuuaaJJ8l58cUX0+0vf/nLdPuv//qv6XbZsmXpNqLa7aKXLFmSbmfNmpVun3vuuXR7wgknpNtXXnkl3VJd8ksfe7m94bWpyhlaW1vTbWdnZ7qdMmVKuh07dmy6HT16dLrt6elJt4cddli6Pfzww9Nte3t7ul25cmW63blzZ7rdtm1bun3qqafS7Wc+85l0W+Vj0dbWlm537NiRbm+99dZ0+5vf/Cbdrl27tint3iDzuuSKOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAuTvc0tlH/nIR4b6CJWMHz++Ke0Xv/jFdPvII4+k24hqtxr/7Gc/W+m5s5588sl0+8orrzTlDMDQaW1tHfL2+OOPT7dz5sxJt/vtt1+6HTNmTLpt1q3ou7q60m2V14StW7em2zvuuCPdbt68Od2+/vrr6banpyfdHnjggen2mWeeSbeTJk1Kt7/97W/T7bhx49LtunXr0m2j0Ui3Q8kVdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABWgb6gNQ3SWXXJJu+/v70+3f/M3fpNujjz463VZxyimnVOprtVq6rXI74yq3lr7iiivSLVCGlpb8da329vZ029nZmW47OjrS7QsvvJBuly5dmm5POOGEdHvggQem27Fjx6bber2ebrds2ZJuBwYG0u3BBx+cbj/xiU+k29WrV6fbZ555Jt0ecsgh6baKX/7yl+n2lVdeSbdVtkqV31uzNsJQcsUdAAAKYLgDAEABDHcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEAB2ob6AFRX5ba83/nOd9Ltf/3Xf6Xbxx57LN2OGjUq3e4tqpx54cKF6fbGG29Mt1Xex+vWrUu3wHtra8u/PHZ2dqbblpb89bIJEyak2w0bNqTbvr6+dHvAAQek2xEjRqTbI444It1W+b1t37493Y4cOTLdvvbaa+l22LBh6bbK59mYMWPS7fjx49Pt008/nW5ff/31dPvkk0+m2/Xr16fbKp8PXV1d6XbLli3pdii54g4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAACiA4Q4AAAUw3AEAoAC1RqPRSIW1WrPPss+59dZb0+25556bbrdu3Zpuf//736fb/fbbL90efPDB6baZqnxeJj/V9xovvfRSul25cmW6vf7669PtT3/603RbmtI+H3h7zXptqvK8VW5xf+CBB6bbY489Nt2eeOKJ6Xbq1Knpdvv27em2yvuhvb093Y4aNaopbWtra7rdvHlzuv3Rj36Ubp9++ul0O2LEiHQ7fvz4dPvEE0+k22XLlqXbKud98skn0+3OnTvTbZXPsx07dqTbZsm8LrniDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAG1DfYB92YoVK9Ltueeem26r3FZ64sSJ6XZvUOV9FhFx+umnp9sqt18+5phj0u3nP//5dHvEEUek2yq3JR83bly6Xbp0abr96U9/mm5hX5K59fhbtm7dmm7XrVuXbvv7+9Ntla8tBx10ULodPXp0uu3s7Ey327ZtS7dVvnYPHz483Va5xf2GDRvS7VNPPZVuV61alW6reO6559Ltyy+/nG6r/HMxMDCQbnfu3NmUM1T5GJfCFXcAACiA4Q4AAAUw3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAACiA4Q4AAAWoNZL3jq3Vas0+yz6npSX/70V33nlnuj399NPfx2k+WL/+9a/T7Z/+6Z+m2/Xr11c6x95wO+PW1tamtO3t7e/nOO+pyu3Z6/V6U86wN6hy22z2XqW9NlU5b0dHR7qdOXNmuv3MZz6Tbj/5yU+m29GjR6fbjRs3ptvx48en2yr/XC9btizdPvLII+n2scceS7dvvPFGuq2iyudZs74WDgwMNOV592WZj4Ur7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAAClBrJO91W9ptpQHeTbNu880fltemN7W05K/DVXmftbW1pduOjo50W6/X0+2oUaPSbX9/f7rdsWNHut21a1e63RtU+Rj7Wrj3yHwsXHEHAIACGO4AAFAAwx0AAApguAMAQAEMdwAAKIDhDgAABTDcAQCgAIY7AAAUwHAHAIACGO4AAFCAWiN5r1u3lQb2JW7zvW/w2gTsKzKvS664AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgAIY7gAAUADDHQAACmC4AwBAAQx3AAAogOEOAAAFMNwBAKAAhjsAABTAcAcAgALUGo1GY6gPAQAAvDtX3AEAoACGOwAAFMBwBwCAAhjuAABQAMMdAAAKYLgDAEABDHcAACiA4Q4AAAUw3AEAoAD/DxiaVy9tjN8UAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.jit.load('model_AE.pt')\n",
    "index = 0\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for images, _ in full_dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        print(len(outputs))\n",
    "        print(outputs.shape)\n",
    "        plot_two_tensors(images[index].cpu().view(28,28), outputs[index].cpu().view(28,28))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 59.37% at threshold: 0.0006870387005619705\n",
      "Anomaly detection accuracy: 67.52% at threshold: 0.003437556345395896\n",
      "Anomaly detection accuracy: 72.36% at threshold: 0.006188073990229821\n",
      "Anomaly detection accuracy: 77.07% at threshold: 0.008938591635063747\n",
      "Anomaly detection accuracy: 82.08% at threshold: 0.011689109279897672\n",
      "Anomaly detection accuracy: 86.38% at threshold: 0.014439626924731597\n",
      "Anomaly detection accuracy: 88.48% at threshold: 0.017190144569565524\n",
      "Anomaly detection accuracy: 88.66% at threshold: 0.019940662214399447\n",
      "Anomaly detection accuracy: 87.03% at threshold: 0.022691179859233374\n",
      "Anomaly detection accuracy: 84.44% at threshold: 0.0254416975040673\n",
      "Anomaly detection accuracy: 80.46% at threshold: 0.028192215148901224\n",
      "Anomaly detection accuracy: 76.34% at threshold: 0.03094273279373515\n",
      "Anomaly detection accuracy: 71.68% at threshold: 0.03369325043856908\n",
      "Anomaly detection accuracy: 67.28% at threshold: 0.036443768083403\n",
      "Anomaly detection accuracy: 63.02% at threshold: 0.039194285728236924\n",
      "Anomaly detection accuracy: 59.02% at threshold: 0.041944803373070855\n",
      "Anomaly detection accuracy: 55.52% at threshold: 0.04469532101790478\n",
      "Anomaly detection accuracy: 52.08% at threshold: 0.0474458386627387\n",
      "Anomaly detection accuracy: 49.72% at threshold: 0.05019635630757263\n",
      "Anomaly detection accuracy: 47.60% at threshold: 0.052946873952406555\n",
      "Anomaly detection accuracy: 45.73% at threshold: 0.05569739159724048\n",
      "Anomaly detection accuracy: 44.41% at threshold: 0.05844790924207441\n",
      "Anomaly detection accuracy: 43.37% at threshold: 0.06119842688690833\n",
      "Anomaly detection accuracy: 42.65% at threshold: 0.06394894453174226\n",
      "Anomaly detection accuracy: 42.00% at threshold: 0.06669946217657619\n",
      "Anomaly detection accuracy: 41.61% at threshold: 0.0694499798214101\n",
      "Anomaly detection accuracy: 41.33% at threshold: 0.07220049746624403\n",
      "Anomaly detection accuracy: 41.08% at threshold: 0.07495101511107796\n",
      "Anomaly detection accuracy: 40.92% at threshold: 0.07770153275591188\n",
      "Anomaly detection accuracy: 40.81% at threshold: 0.08045205040074581\n",
      "Anomaly detection accuracy: 40.79% at threshold: 0.08320256804557974\n",
      "Anomaly detection accuracy: 40.74% at threshold: 0.08595308569041366\n",
      "Anomaly detection accuracy: 40.73% at threshold: 0.08870360333524759\n",
      "Anomaly detection accuracy: 40.68% at threshold: 0.09145412098008152\n",
      "Anomaly detection accuracy: 40.67% at threshold: 0.09420463862491543\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.09695515626974936\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.09970567391458329\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.10245619155941721\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.10520670920425114\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.10795722684908507\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11070774449391899\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11345826213875292\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11620877978358685\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11895929742842076\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.12170981507325469\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.12446033271808862\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.12721085036292254\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.12996136800775646\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.1327118856525904\n",
      "Anomaly detection accuracy: 40.64% at threshold: 0.13546240329742432\n",
      "Best anomaly detection accuracy: 88.66% at threshold: 0.019940662214399447\n"
     ]
    }
   ],
   "source": [
    "model = torch.jit.load('model_AE.pt').cuda()\n",
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "loss_min = 1\n",
    "loss_max = 0\n",
    "for images, labels in full_dataloaders['val']:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images = images.view(images.size(0), -1)\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "    if loss.min() < loss_min:\n",
    "        loss_min = loss.min()\n",
    "    if loss.max() > loss_max:\n",
    "        loss_max = loss.max()\n",
    "\n",
    "thresholds = np.linspace(loss_min.cpu().detach().numpy(), loss_max.cpu().detach().numpy(), num=50)\n",
    "\n",
    "\n",
    "for anomaly_threshold in thresholds:\n",
    "    # anomaly_threshold = ii / float(segment_size)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "            \n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Denoise_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "    return noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Denoise_Autoencoder\n",
      "model total parameters: 219,804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 111.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 0.0010284853066437334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 150.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0008121350012622556\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Loss: 0.0006578301979323565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 145.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0005633040037042543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Loss: 0.0005255455003588013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 187.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00048730962648866423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 113.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Loss: 0.00046135660894406613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 183.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0004413075110319917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 113.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Loss: 0.0004234699993239545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 256.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00041090556214099406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 115.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Loss: 0.000399874144628581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 135.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00039327058771242927\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 112.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Loss: 0.0003820027337490757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 174.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00037480371621974323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 113.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Loss: 0.00036901623145638515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 163.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00036592562857360036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 114.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Loss: 0.00035707678886845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 171.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00035363680141574496\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Loss: 0.0003475223943282431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 240.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00034501639434442844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 130.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Loss: 0.00033724811203990536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 202.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00033786315402677374\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 124.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Loss: 0.00032953397412257516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 226.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00032975660228998634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 123.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Loss: 0.00032149968564437536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 256.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00031943138118481403\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Loss: 0.00031530953825791263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 197.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00031342196349564224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Loss: 0.0003098587085957314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 171.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00030801516042943403\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 118.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Loss: 0.0003046230106086605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 179.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.0003037511186025042\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Loss: 0.00030021038121237745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 142.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00030066779718342985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 115.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Loss: 0.0002965340437256162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 174.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00029523554997494387\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Loss: 0.00029176204360237664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 202.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00029333791747106794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 121.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Loss: 0.0002882516031694508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 179.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 0.00029264548558329234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Denoise_Autoencoder().to(device)\n",
    "print(f\"Model: Denoise_Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images)\n",
    "\n",
    "        images = images.view(images.size(0), -1)\n",
    "        noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "\n",
    "        outputs = model(noisy_images)\n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            noisy_images = add_noise(images)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            noisy_images = noisy_images.view(images.size(0), -1)\n",
    "            \n",
    "            outputs = model(noisy_images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')\n",
    "\n",
    "model_scripted = torch.jit.script(model.cpu()) # Export to TorchScript\n",
    "model_scripted.save('model_DAE.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_tensors(tensor1, tensor2, tensor3):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))  # Create a figure with two subplots\n",
    "\n",
    "    axes[0].imshow(tensor1.squeeze(), cmap='gray')  # Remove channel dimension if exists and plot\n",
    "    axes[0].set_title(f'Before:')\n",
    "    axes[0].axis('off')  # Hide axes ticks\n",
    "\n",
    "    axes[1].imshow(tensor2.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(f'Add Noise:')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(tensor3.squeeze(), cmap='gray')\n",
    "    axes[2].set_title(f'After:')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.jit.load('model_DAE.pt').cuda()\n",
    "index = 2\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for images, label in full_dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "        \n",
    "        outputs = model(noisy_images)\n",
    "        print(\"Label:\", label[index].item())\n",
    "        print(outputs.shape)\n",
    "        plot_three_tensors(images[index].cpu().view(28,28), noisy_images[index].cpu().view(28,28), outputs[index].cpu().view(28,28))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 59.38% at threshold: 0.00196940079331398\n",
      "Anomaly detection accuracy: 61.62% at threshold: 0.004442765852626489\n",
      "Anomaly detection accuracy: 65.92% at threshold: 0.006916130911938998\n",
      "Anomaly detection accuracy: 69.47% at threshold: 0.009389495971251507\n",
      "Anomaly detection accuracy: 72.02% at threshold: 0.011862861030564015\n",
      "Anomaly detection accuracy: 75.02% at threshold: 0.014336226089876523\n",
      "Anomaly detection accuracy: 77.96% at threshold: 0.016809591149189035\n",
      "Anomaly detection accuracy: 81.05% at threshold: 0.019282956208501543\n",
      "Anomaly detection accuracy: 83.58% at threshold: 0.02175632126781405\n",
      "Anomaly detection accuracy: 85.59% at threshold: 0.02422968632712656\n",
      "Anomaly detection accuracy: 86.49% at threshold: 0.026703051386439067\n",
      "Anomaly detection accuracy: 86.58% at threshold: 0.029176416445751578\n",
      "Anomaly detection accuracy: 85.71% at threshold: 0.03164978150506409\n",
      "Anomaly detection accuracy: 83.92% at threshold: 0.034123146564376594\n",
      "Anomaly detection accuracy: 81.63% at threshold: 0.036596511623689106\n",
      "Anomaly detection accuracy: 78.58% at threshold: 0.03906987668300161\n",
      "Anomaly detection accuracy: 75.12% at threshold: 0.04154324174231412\n",
      "Anomaly detection accuracy: 71.79% at threshold: 0.04401660680162663\n",
      "Anomaly detection accuracy: 68.10% at threshold: 0.04648997186093914\n",
      "Anomaly detection accuracy: 64.50% at threshold: 0.04896333692025165\n",
      "Anomaly detection accuracy: 61.02% at threshold: 0.05143670197956415\n",
      "Anomaly detection accuracy: 57.62% at threshold: 0.053910067038876665\n",
      "Anomaly detection accuracy: 54.58% at threshold: 0.056383432098189176\n",
      "Anomaly detection accuracy: 51.73% at threshold: 0.05885679715750168\n",
      "Anomaly detection accuracy: 49.62% at threshold: 0.06133016221681419\n",
      "Anomaly detection accuracy: 47.43% at threshold: 0.0638035272761267\n",
      "Anomaly detection accuracy: 46.03% at threshold: 0.06627689233543921\n",
      "Anomaly detection accuracy: 44.58% at threshold: 0.06875025739475171\n",
      "Anomaly detection accuracy: 43.57% at threshold: 0.07122362245406423\n",
      "Anomaly detection accuracy: 42.58% at threshold: 0.07369698751337674\n",
      "Anomaly detection accuracy: 42.12% at threshold: 0.07617035257268924\n",
      "Anomaly detection accuracy: 41.65% at threshold: 0.07864371763200176\n",
      "Anomaly detection accuracy: 41.37% at threshold: 0.08111708269131426\n",
      "Anomaly detection accuracy: 41.12% at threshold: 0.08359044775062677\n",
      "Anomaly detection accuracy: 40.98% at threshold: 0.08606381280993929\n",
      "Anomaly detection accuracy: 40.83% at threshold: 0.08853717786925179\n",
      "Anomaly detection accuracy: 40.77% at threshold: 0.0910105429285643\n",
      "Anomaly detection accuracy: 40.72% at threshold: 0.09348390798787681\n",
      "Anomaly detection accuracy: 40.70% at threshold: 0.09595727304718932\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.09843063810650182\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.10090400316581433\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.10337736822512684\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.10585073328443935\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.10832409834375185\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11079746340306437\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11327082846237688\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.11574419352168938\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.1182175585810019\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.1206909236403144\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.12316428869962692\n",
      "Best anomaly detection accuracy: 86.58% at threshold: 0.029176416445751578\n"
     ]
    }
   ],
   "source": [
    "model = torch.jit.load('model_DAE.pt').cuda()\n",
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "loss_min = 1\n",
    "loss_max = 0\n",
    "for images, labels in full_dataloaders['val']:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images = images.view(images.size(0), -1)\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "    if loss.min() < loss_min:\n",
    "        loss_min = loss.min()\n",
    "    if loss.max() > loss_max:\n",
    "        loss_max = loss.max()\n",
    "\n",
    "thresholds = np.linspace(loss_min.cpu().detach().numpy(), loss_max.cpu().detach().numpy(), num=50)\n",
    "\n",
    "for anomaly_threshold in thresholds:\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            noisy_images = add_noise(images)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            images = images.view(images.size(0), -1)\n",
    "            noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "\n",
    "            outputs = model(noisy_images)\n",
    "\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = criterion(outputs, images).mean(dim=1)  # Get per image loss\n",
    "            \n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.fc_mean = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, 400)\n",
    "        self.fc3 = nn.Linear(400, input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc_mean(h1), self.fc_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h2 = F.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))  # Assuming input is 28x28 (for MNIST)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    assert torch.max(recon_x) <= 1 and torch.min(recon_x) >= 0, \"Recon_x out of bounds\"\n",
    "    assert torch.max(x) <= 1 and torch.min(x) >= 0, \"x out of bounds\"\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VAE\n",
      "model total parameters: 652,824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:03<00:00, 100.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[TRAIN] Loss: 159.62365673572143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 167.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 124.3431087332362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 103.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "[TRAIN] Loss: 115.97339445013074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 183.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 110.03504511432462\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 102.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "[TRAIN] Loss: 106.55921705603909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 197.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 104.30408907713253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 107.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "[TRAIN] Loss: 101.97458106776003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 154.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 100.54721228385205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 110.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "[TRAIN] Loss: 99.14669644319902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 153.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 98.47272812038757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 108.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "[TRAIN] Loss: 97.14220603018272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 226.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 96.91359022308251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 105.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "[TRAIN] Loss: 95.90626334590682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 167.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 95.82907611461727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 110.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "[TRAIN] Loss: 94.77211968489941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 192.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 94.93993375200402\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 112.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "[TRAIN] Loss: 94.07595066100264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 150.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 94.57755252204423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 112.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "[TRAIN] Loss: 93.4466006409614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 171.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 93.89971695194804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 115.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "[TRAIN] Loss: 92.92318543384486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 163.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 93.49599305193277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 111.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "[TRAIN] Loss: 92.40576626679646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 208.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 93.03425632936559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 121.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "[TRAIN] Loss: 92.0338864503407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 197.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 92.83527853900524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "[TRAIN] Loss: 91.69711799988548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 192.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 92.52539609231856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 114.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "[TRAIN] Loss: 91.38210084256662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 233.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 92.37054453299955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 105.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "[TRAIN] Loss: 91.11261579375986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 197.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 92.26981177314485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 106.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "[TRAIN] Loss: 90.85300967347113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 183.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 92.17168374325631\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 119.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "[TRAIN] Loss: 90.68046181112257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 160.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 91.83411817519595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 125.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "[TRAIN] Loss: 90.45532051350874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 183.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 91.7094604293376\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:02<00:00, 124.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "[TRAIN] Loss: 90.27012734231849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 183.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] Loss: 91.31622105700961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "latent_dim = 20  # Latent dimensionality\n",
    "input_dim = 784  # 28x28 images flattened\n",
    "model = VAE(input_dim,latent_dim).to(device)\n",
    "print(f\"Model: VAE\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for images, _ in tqdm(dataloaders['train']):\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        recon_batch, mu, logvar = model(images)\n",
    "        loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += images.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n[TRAIN] Loss: {running_loss / total}')\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloaders['val']):\n",
    "            images = images.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(images)\n",
    "            loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "            total += images.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'[VALID] Loss: {running_loss / total}\\n')\n",
    "model_scripted = torch.jit.script(model.cpu()) # Export to TorchScript\n",
    "model_scripted.save('model_VAE.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.jit.load(\"model_VAE.pt\").cuda()\n",
    "index = 4\n",
    "model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in full_dataloaders['val']:\n",
    "        images = images.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(images)\n",
    "        print(recon_batch.shape)\n",
    "        plot_two_tensors(images[index].cpu().view(28,28), recon_batch[index].cpu().view(28,28))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 59.37% at threshold: 0.0009143076604232192\n",
      "Anomaly detection accuracy: 60.43% at threshold: 0.002594370508984644\n",
      "Anomaly detection accuracy: 64.54% at threshold: 0.0042744333575460685\n",
      "Anomaly detection accuracy: 68.88% at threshold: 0.005954496206107492\n",
      "Anomaly detection accuracy: 72.67% at threshold: 0.007634559054668917\n",
      "Anomaly detection accuracy: 76.78% at threshold: 0.009314621903230342\n",
      "Anomaly detection accuracy: 80.51% at threshold: 0.010994684751791765\n",
      "Anomaly detection accuracy: 83.61% at threshold: 0.012674747600353191\n",
      "Anomaly detection accuracy: 85.28% at threshold: 0.014354810448914615\n",
      "Anomaly detection accuracy: 85.38% at threshold: 0.01603487329747604\n",
      "Anomaly detection accuracy: 84.89% at threshold: 0.017714936146037464\n",
      "Anomaly detection accuracy: 83.08% at threshold: 0.01939499899459889\n",
      "Anomaly detection accuracy: 80.62% at threshold: 0.02107506184316031\n",
      "Anomaly detection accuracy: 77.17% at threshold: 0.022755124691721737\n",
      "Anomaly detection accuracy: 73.83% at threshold: 0.024435187540283163\n",
      "Anomaly detection accuracy: 69.74% at threshold: 0.026115250388844585\n",
      "Anomaly detection accuracy: 66.10% at threshold: 0.02779531323740601\n",
      "Anomaly detection accuracy: 62.16% at threshold: 0.029475376085967436\n",
      "Anomaly detection accuracy: 58.58% at threshold: 0.031155438934528858\n",
      "Anomaly detection accuracy: 55.77% at threshold: 0.03283550178309028\n",
      "Anomaly detection accuracy: 52.57% at threshold: 0.03451556463165171\n",
      "Anomaly detection accuracy: 50.18% at threshold: 0.036195627480213134\n",
      "Anomaly detection accuracy: 47.94% at threshold: 0.03787569032877456\n",
      "Anomaly detection accuracy: 46.22% at threshold: 0.03955575317733598\n",
      "Anomaly detection accuracy: 44.88% at threshold: 0.041235816025897404\n",
      "Anomaly detection accuracy: 43.78% at threshold: 0.04291587887445883\n",
      "Anomaly detection accuracy: 43.00% at threshold: 0.044595941723020255\n",
      "Anomaly detection accuracy: 42.30% at threshold: 0.04627600457158168\n",
      "Anomaly detection accuracy: 41.92% at threshold: 0.047956067420143106\n",
      "Anomaly detection accuracy: 41.42% at threshold: 0.049636130268704524\n",
      "Anomaly detection accuracy: 41.09% at threshold: 0.05131619311726595\n",
      "Anomaly detection accuracy: 41.03% at threshold: 0.052996255965827375\n",
      "Anomaly detection accuracy: 40.86% at threshold: 0.0546763188143888\n",
      "Anomaly detection accuracy: 40.77% at threshold: 0.056356381662950226\n",
      "Anomaly detection accuracy: 40.73% at threshold: 0.05803644451151165\n",
      "Anomaly detection accuracy: 40.73% at threshold: 0.05971650736007308\n",
      "Anomaly detection accuracy: 40.67% at threshold: 0.061396570208634496\n",
      "Anomaly detection accuracy: 40.68% at threshold: 0.06307663305719592\n",
      "Anomaly detection accuracy: 40.67% at threshold: 0.06475669590575735\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.06643675875431877\n",
      "Anomaly detection accuracy: 40.66% at threshold: 0.0681168216028802\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.06979688445144162\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.07147694730000305\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.07315701014856447\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.0748370729971259\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.07651713584568731\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.07819719869424874\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.07987726154281016\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.08155732439137159\n",
      "Anomaly detection accuracy: 40.65% at threshold: 0.08323738723993301\n",
      "Best anomaly detection accuracy: 85.38% at threshold: 0.01603487329747604\n"
     ]
    }
   ],
   "source": [
    "model = torch.jit.load(\"model_VAE.pt\").cuda()\n",
    "best_acc = 0\n",
    "best_anomaly_threshold = 0\n",
    "normal_labels = torch.tensor([1, 3, 5, 7], device=device)\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "loss_min = 1\n",
    "loss_max = 0\n",
    "for images, labels in full_dataloaders['val']:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images = images.view(images.size(0), -1)\n",
    "\n",
    "    recon_batch, mu, logvar = model(images)\n",
    "\n",
    "    loss = criterion(recon_batch, images).mean(dim=1)  # Get per image loss\n",
    "    if loss.min() < loss_min:\n",
    "        loss_min = loss.min()\n",
    "    if loss.max() > loss_max:\n",
    "        loss_max = loss.max()\n",
    "\n",
    "thresholds = np.linspace(loss_min.cpu().detach().numpy(), loss_max.cpu().detach().numpy(), num=50)\n",
    "\n",
    "for anomaly_threshold in thresholds:\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_dataloaders['val']:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            recon_batch, mu, logvar = model(images)\n",
    "            # Calculate loss per image (mean across each image's feature dimension)\n",
    "            loss = criterion(recon_batch, images).mean(dim=1)  # Get per image loss\n",
    "            # loss = vae_loss(recon_batch, images, mu, logvar)\n",
    "            anomaly_mask = loss > anomaly_threshold\n",
    "\n",
    "            is_normal = torch.isin(labels, normal_labels)\n",
    "\n",
    "            correct_preds = (~is_normal & anomaly_mask) | (is_normal & ~anomaly_mask)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_anomaly_threshold = anomaly_threshold\n",
    "        print(f'Anomaly detection accuracy: {accuracy:.2f}% at threshold: {anomaly_threshold}')\n",
    "\n",
    "print(f'Best anomaly detection accuracy: {best_acc:.2f}% at threshold: {best_anomaly_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Isolated Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pytorch dataset to sklearn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X = full_dataset.data.numpy().reshape(len(full_dataset), -1)\n",
    "y = full_dataset.targets.numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X , y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 0.1111 * 0.9 = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Train dataset shape\n",
      "(48000, 784)\n",
      "(48000,)\n",
      "------------\n",
      "Val dataset shape\n",
      "(6000, 784)\n",
      "(6000,)\n",
      "------------\n",
      "Test dataset shape\n",
      "(6000, 784)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------\")\n",
    "print(\"Train dataset shape\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Val dataset shape\")\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Test dataset shape\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train isolated forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Total number of data: 60000\n",
      "Total number of normal data: 24559\n",
      "Total number of abnormal data: 35441\n",
      "------------------\n",
      "Number of abnormalities detected: 37140\n",
      "Number of normalities detected: 22860\n",
      "accuracy: 70.085 %\n"
     ]
    }
   ],
   "source": [
    "X_full = full_dataset.data.numpy().reshape(len(full_dataset), -1)  # Flatten the images\n",
    "y_full = full_dataset.targets.numpy()\n",
    "\n",
    "# Filter the dataset to only have digits 1, 3, 5, 7\n",
    "target_digits = [1, 3, 5, 7]\n",
    "mask = np.isin(y_full, target_digits)\n",
    "X_train_mask = X_full[mask]\n",
    "y_train_mask = y_full[mask]\n",
    "\n",
    "# Train Isolation Forest on digits 1, 3, 5, 7\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.4, random_state=42)\n",
    "clf.fit(X_train_mask)\n",
    "\n",
    "# Use the trained model to predict over the full dataset\n",
    "y_pred = clf.predict(X_full)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(f\"Total number of data: {len(y_full)}\")\n",
    "print(f\"Total number of normal data: {len(y_train_mask)}\")\n",
    "print(f\"Total number of abnormal data: {len(y_full) - len(y_train_mask)}\")\n",
    "print(\"------------------\")\n",
    "abnormal_indices = np.where(y_pred == -1)[0]\n",
    "normal_indices = np.where(y_pred == 1)[0]\n",
    "print(\"Number of abnormalities detected:\", len(abnormal_indices))\n",
    "print(\"Number of normalities detected:\", len(normal_indices))\n",
    "\n",
    "abnormal_digits = y_full[abnormal_indices]\n",
    "normal_digits = y_full[normal_indices]\n",
    "\n",
    "# You might want to check how many of the abnormal detections are actually from digits other than 1, 3, 5, 7\n",
    "actual_abnormals = np.isin(abnormal_digits, target_digits, invert=True)\n",
    "actual_normals = np.isin(normal_digits, target_digits)\n",
    "# print(np.sum(actual_abnormals))\n",
    "# print(np.sum(actual_normals))\n",
    "print(f\"accuracy: {(np.sum(actual_abnormals) + np.sum(actual_normals)) * 100 / len(y_full)} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abnormal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Threshold: -0.247 --> Detected abnormalities: 0, Detected normals: 60000, Accuracy: 40.93%\n",
      "Threshold: -0.240 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.233 --> Detected abnormalities: 2, Detected normals: 59998, Accuracy: 40.94%\n",
      "Threshold: -0.226 --> Detected abnormalities: 3, Detected normals: 59997, Accuracy: 40.94%\n",
      "Threshold: -0.219 --> Detected abnormalities: 4, Detected normals: 59996, Accuracy: 40.94%\n",
      "Threshold: -0.212 --> Detected abnormalities: 12, Detected normals: 59988, Accuracy: 40.95%\n",
      "Threshold: -0.205 --> Detected abnormalities: 22, Detected normals: 59978, Accuracy: 40.97%\n",
      "Threshold: -0.198 --> Detected abnormalities: 44, Detected normals: 59956, Accuracy: 41.01%\n",
      "Threshold: -0.191 --> Detected abnormalities: 92, Detected normals: 59908, Accuracy: 41.08%\n",
      "Threshold: -0.184 --> Detected abnormalities: 154, Detected normals: 59846, Accuracy: 41.18%\n",
      "Threshold: -0.177 --> Detected abnormalities: 259, Detected normals: 59741, Accuracy: 41.36%\n",
      "Threshold: -0.170 --> Detected abnormalities: 401, Detected normals: 59599, Accuracy: 41.58%\n",
      "Threshold: -0.163 --> Detected abnormalities: 579, Detected normals: 59421, Accuracy: 41.87%\n",
      "Threshold: -0.156 --> Detected abnormalities: 843, Detected normals: 59157, Accuracy: 42.28%\n",
      "Threshold: -0.149 --> Detected abnormalities: 1169, Detected normals: 58831, Accuracy: 42.79%\n",
      "Threshold: -0.142 --> Detected abnormalities: 1578, Detected normals: 58422, Accuracy: 43.41%\n",
      "Threshold: -0.135 --> Detected abnormalities: 2156, Detected normals: 57844, Accuracy: 44.28%\n",
      "Threshold: -0.128 --> Detected abnormalities: 2821, Detected normals: 57179, Accuracy: 45.26%\n",
      "Threshold: -0.122 --> Detected abnormalities: 3572, Detected normals: 56428, Accuracy: 46.34%\n",
      "Threshold: -0.115 --> Detected abnormalities: 4418, Detected normals: 55582, Accuracy: 47.52%\n",
      "Threshold: -0.108 --> Detected abnormalities: 5428, Detected normals: 54572, Accuracy: 48.92%\n",
      "Threshold: -0.101 --> Detected abnormalities: 6567, Detected normals: 53433, Accuracy: 50.41%\n",
      "Threshold: -0.094 --> Detected abnormalities: 7792, Detected normals: 52208, Accuracy: 51.92%\n",
      "Threshold: -0.087 --> Detected abnormalities: 9131, Detected normals: 50869, Accuracy: 53.58%\n",
      "Threshold: -0.080 --> Detected abnormalities: 10618, Detected normals: 49382, Accuracy: 55.21%\n",
      "Threshold: -0.073 --> Detected abnormalities: 12228, Detected normals: 47772, Accuracy: 56.92%\n",
      "Threshold: -0.066 --> Detected abnormalities: 13911, Detected normals: 46089, Accuracy: 58.45%\n",
      "Threshold: -0.059 --> Detected abnormalities: 15783, Detected normals: 44217, Accuracy: 60.20%\n",
      "Threshold: -0.052 --> Detected abnormalities: 17783, Detected normals: 42217, Accuracy: 61.68%\n",
      "Threshold: -0.045 --> Detected abnormalities: 19976, Detected normals: 40024, Accuracy: 63.31%\n",
      "Threshold: -0.038 --> Detected abnormalities: 22265, Detected normals: 37735, Accuracy: 64.72%\n",
      "Threshold: -0.031 --> Detected abnormalities: 24681, Detected normals: 35319, Accuracy: 66.12%\n",
      "Threshold: -0.024 --> Detected abnormalities: 27328, Detected normals: 32672, Accuracy: 67.33%\n",
      "Threshold: -0.017 --> Detected abnormalities: 30090, Detected normals: 29910, Accuracy: 68.29%\n",
      "Threshold: -0.010 --> Detected abnormalities: 32938, Detected normals: 27062, Accuracy: 69.16%\n",
      "Threshold: -0.003 --> Detected abnormalities: 35843, Detected normals: 24157, Accuracy: 69.79%\n",
      "Threshold: 0.004 --> Detected abnormalities: 38716, Detected normals: 21284, Accuracy: 70.23%\n",
      "Threshold: 0.011 --> Detected abnormalities: 41549, Detected normals: 18451, Accuracy: 70.59%\n",
      "Threshold: 0.018 --> Detected abnormalities: 44276, Detected normals: 15724, Accuracy: 70.69%\n",
      "Threshold: 0.025 --> Detected abnormalities: 46742, Detected normals: 13258, Accuracy: 70.50%\n",
      "Threshold: 0.032 --> Detected abnormalities: 48914, Detected normals: 11086, Accuracy: 70.04%\n",
      "Threshold: 0.039 --> Detected abnormalities: 50848, Detected normals: 9152, Accuracy: 69.56%\n",
      "Threshold: 0.046 --> Detected abnormalities: 52468, Detected normals: 7532, Accuracy: 68.90%\n",
      "Threshold: 0.053 --> Detected abnormalities: 53733, Detected normals: 6267, Accuracy: 68.12%\n",
      "Threshold: 0.060 --> Detected abnormalities: 54769, Detected normals: 5231, Accuracy: 67.18%\n",
      "Threshold: 0.066 --> Detected abnormalities: 55732, Detected normals: 4268, Accuracy: 65.97%\n",
      "Threshold: 0.073 --> Detected abnormalities: 56744, Detected normals: 3256, Accuracy: 64.46%\n",
      "Threshold: 0.080 --> Detected abnormalities: 58120, Detected normals: 1880, Accuracy: 62.19%\n",
      "Threshold: 0.087 --> Detected abnormalities: 59557, Detected normals: 443, Accuracy: 59.81%\n",
      "Threshold: 0.094 --> Detected abnormalities: 59999, Detected normals: 1, Accuracy: 59.07%\n",
      "The best threshold: 0.01775462811612255, with Accuracy: 70.685%\n"
     ]
    }
   ],
   "source": [
    "# Obtain anomaly scores (the lower, the more abnormal)\n",
    "scores = clf.decision_function(X_full)\n",
    "\n",
    "# Define a range of thresholds from the anomaly scores to classify as normal or abnormal\n",
    "thresholds = np.linspace(scores.min(), scores.max(), num=50)\n",
    "print(\"------------------\")\n",
    "best_acc = 0 \n",
    "best_threshold = 0\n",
    "# Evaluate accuracy at each threshold\n",
    "for threshold in thresholds:\n",
    "    y_pred = np.where(scores < threshold, -1, 1)\n",
    "\n",
    "    abnormal_indices = np.where(y_pred == -1)[0]\n",
    "    normal_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "    abnormal_digits = y_full[abnormal_indices]\n",
    "    normal_digits = y_full[normal_indices]\n",
    "\n",
    "    # Check how many detected abnormalities are actual abnormalities\n",
    "    actual_abnormals = np.isin(abnormal_digits, target_digits, invert=True)\n",
    "    actual_normals = np.isin(normal_digits, target_digits)\n",
    "\n",
    "    accuracy = (np.sum(actual_abnormals) + np.sum(actual_normals)) / len(y_full) * 100\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_threshold = threshold\n",
    "    print(f\"Threshold: {threshold:.3f} --> Detected abnormalities: {len(abnormal_indices)}, Detected normals: {len(normal_indices)}, Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"The best threshold: {best_threshold}, with Accuracy: {best_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Isolated Forrest with pre-trained feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_clf(features, y_full):\n",
    "    X_full = features  # Flatten the images\n",
    "\n",
    "    target_digits = [1, 3, 5, 7]\n",
    "    mask = np.isin(y_full, target_digits)\n",
    "    X_train_mask = X_full[mask]\n",
    "    y_train_mask = y_full[mask]\n",
    "\n",
    "    # Train Isolation Forest on digits 1, 3, 5, 7\n",
    "    clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.5, random_state=42)\n",
    "    clf.fit(X_train_mask)\n",
    "\n",
    "    # Use the trained model to predict over the full dataset\n",
    "    y_pred = clf.predict(X_full)\n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(f\"Total number of data: {len(y_full)}\")\n",
    "    print(f\"Total number of normal data: {len(y_train_mask)}\")\n",
    "    print(f\"Total number of abnormal data: {len(y_full) - len(y_train_mask)}\")\n",
    "    print(\"------------------\")\n",
    "    abnormal_indices = np.where(y_pred == -1)[0]\n",
    "    normal_indices = np.where(y_pred == 1)[0]\n",
    "    print(\"Number of abnormalities detected:\", len(abnormal_indices))\n",
    "    print(\"Number of normalities detected:\", len(normal_indices))\n",
    "\n",
    "    abnormal_digits = y_full[abnormal_indices]\n",
    "    normal_digits = y_full[normal_indices]\n",
    "\n",
    "    # You might want to check how many of the abnormal detections are actually from digits other than 1, 3, 5, 7\n",
    "    actual_abnormals = np.isin(abnormal_digits, target_digits, invert=True)\n",
    "    actual_normals = np.isin(normal_digits, target_digits)\n",
    "    # print(np.sum(actual_abnormals))\n",
    "    # print(np.sum(actual_normals))\n",
    "    print(f\"accuracy: {(np.sum(actual_abnormals) + np.sum(actual_normals)) * 100 / len(y_full)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_different_th(features, y_full):\n",
    "    X_full = features\n",
    "    target_digits = [1, 3, 5, 7]\n",
    "    mask = np.isin(y_full, target_digits)\n",
    "    X_train_mask = X_full[mask]\n",
    "    # y_train_mask = y_full[mask]\n",
    "\n",
    "    clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.4, random_state=42)\n",
    "    clf.fit(X_train_mask)\n",
    "    # Obtain anomaly scores (the lower, the more abnormal)\n",
    "    scores = clf.decision_function(X_full)\n",
    "\n",
    "    # Define a range of thresholds from the anomaly scores to classify as normal or abnormal\n",
    "    thresholds = np.linspace(scores.min(), scores.max(), num=50)\n",
    "    print(\"------------------\")\n",
    "    best_acc = 0 \n",
    "    best_threshold = 0\n",
    "    # Evaluate accuracy at each threshold\n",
    "    for threshold in thresholds:\n",
    "        y_pred = np.where(scores < threshold, -1, 1)\n",
    "\n",
    "        abnormal_indices = np.where(y_pred == -1)[0]\n",
    "        normal_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "        abnormal_digits = y_full[abnormal_indices]\n",
    "        normal_digits = y_full[normal_indices]\n",
    "\n",
    "        # Check how many detected abnormalities are actual abnormalities\n",
    "        actual_abnormals = np.isin(abnormal_digits, target_digits, invert=True)\n",
    "        actual_normals = np.isin(normal_digits, target_digits)\n",
    "\n",
    "        accuracy = (np.sum(actual_abnormals) + np.sum(actual_normals)) / len(y_full) * 100\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_threshold = threshold\n",
    "        print(f\"Threshold: {threshold:.3f} --> Detected abnormalities: {len(abnormal_indices)}, Detected normals: {len(normal_indices)}, Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"The best threshold: {best_threshold}, with Accuracy: {best_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classifier as feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DigitClassifier\n",
      "model total parameters: 205,632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 32/750 [00:00<00:04, 151.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:04<00:00, 179.42it/s]\n",
      "100%|██████████| 188/188 [00:01<00:00, 164.98it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 128)\n",
      "(60000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('model_image_classifier.pt')\n",
    "model = DigitClassifier().to(device)\n",
    "model.load_state_dict(model.state_dict())\n",
    "model = model.cuda()\n",
    "model.fc = nn.Identity()\n",
    "print(f\"Model: DigitClassifier\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "model.eval()\n",
    "features = []\n",
    "forest_labels = []\n",
    "with torch.no_grad():\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        for images, labels in tqdm(full_dataloaders[phase]):\n",
    "            # labels = transform_labels(labels)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            forest_labels.append(labels.cpu().numpy())\n",
    "            # print(outputs[1])\n",
    "            # break\n",
    "features = np.concatenate(features, axis=0)\n",
    "forest_labels = np.concatenate(forest_labels, axis=0)\n",
    "\n",
    "print(features.shape)\n",
    "print(forest_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Total number of data: 60000\n",
      "Total number of normal data: 24559\n",
      "Total number of abnormal data: 35441\n",
      "------------------\n",
      "Number of abnormalities detected: 42059\n",
      "Number of normalities detected: 17941\n",
      "accuracy: 70.1 %\n",
      "------------------\n",
      "Threshold: -0.155 --> Detected abnormalities: 0, Detected normals: 60000, Accuracy: 40.93%\n",
      "Threshold: -0.151 --> Detected abnormalities: 2, Detected normals: 59998, Accuracy: 40.94%\n",
      "Threshold: -0.147 --> Detected abnormalities: 5, Detected normals: 59995, Accuracy: 40.94%\n",
      "Threshold: -0.142 --> Detected abnormalities: 9, Detected normals: 59991, Accuracy: 40.95%\n",
      "Threshold: -0.138 --> Detected abnormalities: 16, Detected normals: 59984, Accuracy: 40.96%\n",
      "Threshold: -0.134 --> Detected abnormalities: 30, Detected normals: 59970, Accuracy: 40.98%\n",
      "Threshold: -0.129 --> Detected abnormalities: 55, Detected normals: 59945, Accuracy: 41.02%\n",
      "Threshold: -0.125 --> Detected abnormalities: 95, Detected normals: 59905, Accuracy: 41.09%\n",
      "Threshold: -0.121 --> Detected abnormalities: 147, Detected normals: 59853, Accuracy: 41.17%\n",
      "Threshold: -0.116 --> Detected abnormalities: 230, Detected normals: 59770, Accuracy: 41.31%\n",
      "Threshold: -0.112 --> Detected abnormalities: 348, Detected normals: 59652, Accuracy: 41.50%\n",
      "Threshold: -0.107 --> Detected abnormalities: 495, Detected normals: 59505, Accuracy: 41.74%\n",
      "Threshold: -0.103 --> Detected abnormalities: 691, Detected normals: 59309, Accuracy: 42.05%\n",
      "Threshold: -0.099 --> Detected abnormalities: 954, Detected normals: 59046, Accuracy: 42.47%\n",
      "Threshold: -0.094 --> Detected abnormalities: 1267, Detected normals: 58733, Accuracy: 42.95%\n",
      "Threshold: -0.090 --> Detected abnormalities: 1674, Detected normals: 58326, Accuracy: 43.55%\n",
      "Threshold: -0.086 --> Detected abnormalities: 2153, Detected normals: 57847, Accuracy: 44.26%\n",
      "Threshold: -0.081 --> Detected abnormalities: 2790, Detected normals: 57210, Accuracy: 45.21%\n",
      "Threshold: -0.077 --> Detected abnormalities: 3490, Detected normals: 56510, Accuracy: 46.20%\n",
      "Threshold: -0.073 --> Detected abnormalities: 4316, Detected normals: 55684, Accuracy: 47.39%\n",
      "Threshold: -0.068 --> Detected abnormalities: 5265, Detected normals: 54735, Accuracy: 48.70%\n",
      "Threshold: -0.064 --> Detected abnormalities: 6235, Detected normals: 53765, Accuracy: 50.01%\n",
      "Threshold: -0.060 --> Detected abnormalities: 7415, Detected normals: 52585, Accuracy: 51.65%\n",
      "Threshold: -0.055 --> Detected abnormalities: 8697, Detected normals: 51303, Accuracy: 53.24%\n",
      "Threshold: -0.051 --> Detected abnormalities: 10049, Detected normals: 49951, Accuracy: 54.83%\n",
      "Threshold: -0.046 --> Detected abnormalities: 11537, Detected normals: 48463, Accuracy: 56.54%\n",
      "Threshold: -0.042 --> Detected abnormalities: 13271, Detected normals: 46729, Accuracy: 58.33%\n",
      "Threshold: -0.038 --> Detected abnormalities: 15148, Detected normals: 44852, Accuracy: 60.13%\n",
      "Threshold: -0.033 --> Detected abnormalities: 17153, Detected normals: 42847, Accuracy: 62.09%\n",
      "Threshold: -0.029 --> Detected abnormalities: 19391, Detected normals: 40609, Accuracy: 64.04%\n",
      "Threshold: -0.025 --> Detected abnormalities: 21683, Detected normals: 38317, Accuracy: 65.61%\n",
      "Threshold: -0.020 --> Detected abnormalities: 24229, Detected normals: 35771, Accuracy: 67.19%\n",
      "Threshold: -0.016 --> Detected abnormalities: 26843, Detected normals: 33157, Accuracy: 68.57%\n",
      "Threshold: -0.012 --> Detected abnormalities: 29576, Detected normals: 30424, Accuracy: 69.47%\n",
      "Threshold: -0.007 --> Detected abnormalities: 32397, Detected normals: 27603, Accuracy: 70.26%\n",
      "Threshold: -0.003 --> Detected abnormalities: 35413, Detected normals: 24587, Accuracy: 70.56%\n",
      "Threshold: 0.001 --> Detected abnormalities: 38534, Detected normals: 21466, Accuracy: 70.61%\n",
      "Threshold: 0.006 --> Detected abnormalities: 41705, Detected normals: 18295, Accuracy: 70.17%\n",
      "Threshold: 0.010 --> Detected abnormalities: 44979, Detected normals: 15021, Accuracy: 69.09%\n",
      "Threshold: 0.015 --> Detected abnormalities: 48065, Detected normals: 11935, Accuracy: 67.75%\n",
      "Threshold: 0.019 --> Detected abnormalities: 51099, Detected normals: 8901, Accuracy: 65.88%\n",
      "Threshold: 0.023 --> Detected abnormalities: 53656, Detected normals: 6344, Accuracy: 64.28%\n",
      "Threshold: 0.028 --> Detected abnormalities: 55715, Detected normals: 4285, Accuracy: 62.79%\n",
      "Threshold: 0.032 --> Detected abnormalities: 57378, Detected normals: 2622, Accuracy: 61.42%\n",
      "Threshold: 0.036 --> Detected abnormalities: 58583, Detected normals: 1417, Accuracy: 60.37%\n",
      "Threshold: 0.041 --> Detected abnormalities: 59347, Detected normals: 653, Accuracy: 59.65%\n",
      "Threshold: 0.045 --> Detected abnormalities: 59756, Detected normals: 244, Accuracy: 59.29%\n",
      "Threshold: 0.049 --> Detected abnormalities: 59933, Detected normals: 67, Accuracy: 59.13%\n",
      "Threshold: 0.054 --> Detected abnormalities: 59987, Detected normals: 13, Accuracy: 59.06%\n",
      "Threshold: 0.058 --> Detected abnormalities: 59999, Detected normals: 1, Accuracy: 59.07%\n",
      "The best threshold: 0.0014598495864815986, with Accuracy: 70.61166666666666%\n"
     ]
    }
   ],
   "source": [
    "fit_with_clf(features, forest_labels)\n",
    "fit_with_different_th(features, forest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal autoencoder as feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoencoder\n",
      "model total parameters: 109,516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 31/750 [00:00<00:02, 281.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 193.80it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 241.03it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('model_AE.pt')\n",
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(model.state_dict())\n",
    "model = model.cuda()\n",
    "model.decoder = nn.Identity()\n",
    "print(f\"Model: Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "model.eval()\n",
    "features = []\n",
    "forest_labels = []\n",
    "with torch.no_grad():\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        for images, labels in tqdm(full_dataloaders[phase]):\n",
    "            images = images.view(images.size(0), -1)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            forest_labels.append(labels.cpu().numpy())\n",
    "features = np.concatenate(features, axis=0)\n",
    "forest_labels = np.concatenate(forest_labels, axis=0)\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Total number of data: 60000\n",
      "Total number of normal data: 24559\n",
      "Total number of abnormal data: 35441\n",
      "------------------\n",
      "Number of abnormalities detected: 34817\n",
      "Number of normalities detected: 25183\n",
      "accuracy: 58.03 %\n",
      "------------------\n",
      "Threshold: -0.225 --> Detected abnormalities: 0, Detected normals: 60000, Accuracy: 40.93%\n",
      "Threshold: -0.219 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.213 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.207 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.200 --> Detected abnormalities: 5, Detected normals: 59995, Accuracy: 40.94%\n",
      "Threshold: -0.194 --> Detected abnormalities: 6, Detected normals: 59994, Accuracy: 40.94%\n",
      "Threshold: -0.188 --> Detected abnormalities: 6, Detected normals: 59994, Accuracy: 40.94%\n",
      "Threshold: -0.182 --> Detected abnormalities: 12, Detected normals: 59988, Accuracy: 40.95%\n",
      "Threshold: -0.175 --> Detected abnormalities: 14, Detected normals: 59986, Accuracy: 40.95%\n",
      "Threshold: -0.169 --> Detected abnormalities: 23, Detected normals: 59977, Accuracy: 40.95%\n",
      "Threshold: -0.163 --> Detected abnormalities: 37, Detected normals: 59963, Accuracy: 40.97%\n",
      "Threshold: -0.157 --> Detected abnormalities: 56, Detected normals: 59944, Accuracy: 40.99%\n",
      "Threshold: -0.150 --> Detected abnormalities: 82, Detected normals: 59918, Accuracy: 41.01%\n",
      "Threshold: -0.144 --> Detected abnormalities: 127, Detected normals: 59873, Accuracy: 41.06%\n",
      "Threshold: -0.138 --> Detected abnormalities: 197, Detected normals: 59803, Accuracy: 41.15%\n",
      "Threshold: -0.132 --> Detected abnormalities: 288, Detected normals: 59712, Accuracy: 41.24%\n",
      "Threshold: -0.125 --> Detected abnormalities: 408, Detected normals: 59592, Accuracy: 41.37%\n",
      "Threshold: -0.119 --> Detected abnormalities: 594, Detected normals: 59406, Accuracy: 41.57%\n",
      "Threshold: -0.113 --> Detected abnormalities: 811, Detected normals: 59189, Accuracy: 41.78%\n",
      "Threshold: -0.107 --> Detected abnormalities: 1111, Detected normals: 58889, Accuracy: 42.05%\n",
      "Threshold: -0.100 --> Detected abnormalities: 1464, Detected normals: 58536, Accuracy: 42.45%\n",
      "Threshold: -0.094 --> Detected abnormalities: 1923, Detected normals: 58077, Accuracy: 42.88%\n",
      "Threshold: -0.088 --> Detected abnormalities: 2491, Detected normals: 57509, Accuracy: 43.40%\n",
      "Threshold: -0.082 --> Detected abnormalities: 3132, Detected normals: 56868, Accuracy: 44.04%\n",
      "Threshold: -0.075 --> Detected abnormalities: 3875, Detected normals: 56125, Accuracy: 44.63%\n",
      "Threshold: -0.069 --> Detected abnormalities: 4840, Detected normals: 55160, Accuracy: 45.42%\n",
      "Threshold: -0.063 --> Detected abnormalities: 5971, Detected normals: 54029, Accuracy: 46.40%\n",
      "Threshold: -0.057 --> Detected abnormalities: 7274, Detected normals: 52726, Accuracy: 47.26%\n",
      "Threshold: -0.050 --> Detected abnormalities: 8715, Detected normals: 51285, Accuracy: 48.34%\n",
      "Threshold: -0.044 --> Detected abnormalities: 10395, Detected normals: 49605, Accuracy: 49.45%\n",
      "Threshold: -0.038 --> Detected abnormalities: 12350, Detected normals: 47650, Accuracy: 50.59%\n",
      "Threshold: -0.032 --> Detected abnormalities: 14519, Detected normals: 45481, Accuracy: 51.83%\n",
      "Threshold: -0.026 --> Detected abnormalities: 16852, Detected normals: 43148, Accuracy: 52.89%\n",
      "Threshold: -0.019 --> Detected abnormalities: 19428, Detected normals: 40572, Accuracy: 53.92%\n",
      "Threshold: -0.013 --> Detected abnormalities: 22308, Detected normals: 37692, Accuracy: 54.82%\n",
      "Threshold: -0.007 --> Detected abnormalities: 25456, Detected normals: 34544, Accuracy: 55.71%\n",
      "Threshold: -0.001 --> Detected abnormalities: 28871, Detected normals: 31129, Accuracy: 56.76%\n",
      "Threshold: 0.006 --> Detected abnormalities: 32403, Detected normals: 27597, Accuracy: 57.61%\n",
      "Threshold: 0.012 --> Detected abnormalities: 36122, Detected normals: 23878, Accuracy: 58.13%\n",
      "Threshold: 0.018 --> Detected abnormalities: 39989, Detected normals: 20011, Accuracy: 58.65%\n",
      "Threshold: 0.024 --> Detected abnormalities: 43723, Detected normals: 16277, Accuracy: 59.11%\n",
      "Threshold: 0.031 --> Detected abnormalities: 47410, Detected normals: 12590, Accuracy: 59.14%\n",
      "Threshold: 0.037 --> Detected abnormalities: 50835, Detected normals: 9165, Accuracy: 59.15%\n",
      "Threshold: 0.043 --> Detected abnormalities: 53861, Detected normals: 6139, Accuracy: 59.24%\n",
      "Threshold: 0.049 --> Detected abnormalities: 56319, Detected normals: 3681, Accuracy: 59.14%\n",
      "Threshold: 0.056 --> Detected abnormalities: 58123, Detected normals: 1877, Accuracy: 58.94%\n",
      "Threshold: 0.062 --> Detected abnormalities: 59278, Detected normals: 722, Accuracy: 58.94%\n",
      "Threshold: 0.068 --> Detected abnormalities: 59842, Detected normals: 158, Accuracy: 59.03%\n",
      "Threshold: 0.074 --> Detected abnormalities: 59980, Detected normals: 20, Accuracy: 59.06%\n",
      "Threshold: 0.081 --> Detected abnormalities: 59999, Detected normals: 1, Accuracy: 59.07%\n",
      "The best threshold: 0.043194292471997076, with Accuracy: 59.23666666666667%\n"
     ]
    }
   ],
   "source": [
    "fit_with_clf(features, forest_labels)\n",
    "fit_with_different_th(features, forest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising autoencoder as feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Denoise_Autoencoder\n",
      "model total parameters: 109,516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 196.85it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 247.37it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('model_DAE.pt')\n",
    "model = Denoise_Autoencoder().to(device)\n",
    "model.load_state_dict(model.state_dict())\n",
    "model = model.cuda()\n",
    "model.decoder = nn.Identity()\n",
    "print(f\"Model: Denoise_Autoencoder\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "model.eval()\n",
    "features = []\n",
    "forest_labels = []\n",
    "with torch.no_grad():\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        for images, labels in tqdm(full_dataloaders[phase]):\n",
    "            images = images.view(images.size(0), -1)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            forest_labels.append(labels.cpu().numpy())\n",
    "features = np.concatenate(features, axis=0)\n",
    "forest_labels = np.concatenate(forest_labels, axis=0)\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Total number of data: 60000\n",
      "Total number of normal data: 24559\n",
      "Total number of abnormal data: 35441\n",
      "------------------\n",
      "Number of abnormalities detected: 41314\n",
      "Number of normalities detected: 18686\n",
      "accuracy: 68.85833333333333 %\n",
      "------------------\n",
      "Threshold: -0.265 --> Detected abnormalities: 0, Detected normals: 60000, Accuracy: 40.93%\n",
      "Threshold: -0.258 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.251 --> Detected abnormalities: 1, Detected normals: 59999, Accuracy: 40.93%\n",
      "Threshold: -0.244 --> Detected abnormalities: 4, Detected normals: 59996, Accuracy: 40.94%\n",
      "Threshold: -0.237 --> Detected abnormalities: 6, Detected normals: 59994, Accuracy: 40.94%\n",
      "Threshold: -0.230 --> Detected abnormalities: 11, Detected normals: 59989, Accuracy: 40.95%\n",
      "Threshold: -0.222 --> Detected abnormalities: 24, Detected normals: 59976, Accuracy: 40.97%\n",
      "Threshold: -0.215 --> Detected abnormalities: 35, Detected normals: 59965, Accuracy: 40.99%\n",
      "Threshold: -0.208 --> Detected abnormalities: 57, Detected normals: 59943, Accuracy: 41.03%\n",
      "Threshold: -0.201 --> Detected abnormalities: 96, Detected normals: 59904, Accuracy: 41.08%\n",
      "Threshold: -0.194 --> Detected abnormalities: 145, Detected normals: 59855, Accuracy: 41.16%\n",
      "Threshold: -0.187 --> Detected abnormalities: 217, Detected normals: 59783, Accuracy: 41.27%\n",
      "Threshold: -0.180 --> Detected abnormalities: 330, Detected normals: 59670, Accuracy: 41.44%\n",
      "Threshold: -0.173 --> Detected abnormalities: 483, Detected normals: 59517, Accuracy: 41.66%\n",
      "Threshold: -0.165 --> Detected abnormalities: 692, Detected normals: 59308, Accuracy: 41.95%\n",
      "Threshold: -0.158 --> Detected abnormalities: 979, Detected normals: 59021, Accuracy: 42.34%\n",
      "Threshold: -0.151 --> Detected abnormalities: 1353, Detected normals: 58647, Accuracy: 42.86%\n",
      "Threshold: -0.144 --> Detected abnormalities: 1767, Detected normals: 58233, Accuracy: 43.43%\n",
      "Threshold: -0.137 --> Detected abnormalities: 2284, Detected normals: 57716, Accuracy: 44.14%\n",
      "Threshold: -0.130 --> Detected abnormalities: 2852, Detected normals: 57148, Accuracy: 44.92%\n",
      "Threshold: -0.123 --> Detected abnormalities: 3544, Detected normals: 56456, Accuracy: 45.81%\n",
      "Threshold: -0.116 --> Detected abnormalities: 4362, Detected normals: 55638, Accuracy: 46.87%\n",
      "Threshold: -0.108 --> Detected abnormalities: 5217, Detected normals: 54783, Accuracy: 47.97%\n",
      "Threshold: -0.101 --> Detected abnormalities: 6293, Detected normals: 53707, Accuracy: 49.26%\n",
      "Threshold: -0.094 --> Detected abnormalities: 7523, Detected normals: 52477, Accuracy: 50.70%\n",
      "Threshold: -0.087 --> Detected abnormalities: 8813, Detected normals: 51187, Accuracy: 52.18%\n",
      "Threshold: -0.080 --> Detected abnormalities: 10252, Detected normals: 49748, Accuracy: 53.72%\n",
      "Threshold: -0.073 --> Detected abnormalities: 11832, Detected normals: 48168, Accuracy: 55.30%\n",
      "Threshold: -0.066 --> Detected abnormalities: 13614, Detected normals: 46386, Accuracy: 56.90%\n",
      "Threshold: -0.059 --> Detected abnormalities: 15536, Detected normals: 44464, Accuracy: 58.60%\n",
      "Threshold: -0.051 --> Detected abnormalities: 17612, Detected normals: 42388, Accuracy: 60.26%\n",
      "Threshold: -0.044 --> Detected abnormalities: 19785, Detected normals: 40215, Accuracy: 61.85%\n",
      "Threshold: -0.037 --> Detected abnormalities: 22092, Detected normals: 37908, Accuracy: 63.39%\n",
      "Threshold: -0.030 --> Detected abnormalities: 24527, Detected normals: 35473, Accuracy: 64.80%\n",
      "Threshold: -0.023 --> Detected abnormalities: 27108, Detected normals: 32892, Accuracy: 66.03%\n",
      "Threshold: -0.016 --> Detected abnormalities: 29861, Detected normals: 30139, Accuracy: 67.11%\n",
      "Threshold: -0.009 --> Detected abnormalities: 32719, Detected normals: 27281, Accuracy: 68.08%\n",
      "Threshold: -0.002 --> Detected abnormalities: 35578, Detected normals: 24422, Accuracy: 68.46%\n",
      "Threshold: 0.006 --> Detected abnormalities: 38637, Detected normals: 21363, Accuracy: 68.92%\n",
      "Threshold: 0.013 --> Detected abnormalities: 41603, Detected normals: 18397, Accuracy: 68.92%\n",
      "Threshold: 0.020 --> Detected abnormalities: 44424, Detected normals: 15576, Accuracy: 68.77%\n",
      "Threshold: 0.027 --> Detected abnormalities: 47208, Detected normals: 12792, Accuracy: 68.06%\n",
      "Threshold: 0.034 --> Detected abnormalities: 49937, Detected normals: 10063, Accuracy: 66.98%\n",
      "Threshold: 0.041 --> Detected abnormalities: 52429, Detected normals: 7571, Accuracy: 65.65%\n",
      "Threshold: 0.048 --> Detected abnormalities: 54730, Detected normals: 5270, Accuracy: 64.20%\n",
      "Threshold: 0.055 --> Detected abnormalities: 56757, Detected normals: 3243, Accuracy: 62.52%\n",
      "Threshold: 0.063 --> Detected abnormalities: 58398, Detected normals: 1602, Accuracy: 60.86%\n",
      "Threshold: 0.070 --> Detected abnormalities: 59447, Detected normals: 553, Accuracy: 59.73%\n",
      "Threshold: 0.077 --> Detected abnormalities: 59920, Detected normals: 80, Accuracy: 59.19%\n",
      "Threshold: 0.084 --> Detected abnormalities: 59999, Detected normals: 1, Accuracy: 59.07%\n",
      "The best threshold: 0.005522786643118338, with Accuracy: 68.91666666666667%\n"
     ]
    }
   ],
   "source": [
    "fit_with_clf(features, forest_labels)\n",
    "fit_with_different_th(features, forest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoder as feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VAE\n",
      "model total parameters: 330,040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 193.30it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 216.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('model_DAE.pt')\n",
    "model = VAE(784,20).to(device)\n",
    "model.load_state_dict(model.state_dict())\n",
    "model = model.cuda()\n",
    "model.fc2 = nn.Identity()\n",
    "model.fc3 = nn.Identity()\n",
    "print(f\"Model: VAE\")\n",
    "model_parameters_amount = count_parameters(model)\n",
    "print(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "model.eval()\n",
    "features = []\n",
    "forest_labels = []\n",
    "with torch.no_grad():\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        for images, labels in tqdm(full_dataloaders[phase]):\n",
    "            images = images.view(images.size(0), -1)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            recon_batch, mu, logvar = model(images)\n",
    "\n",
    "            features.append(recon_batch.cpu().numpy())\n",
    "            forest_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "features = np.concatenate(features, axis=0)\n",
    "forest_labels = np.concatenate(forest_labels, axis=0)\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Total number of data: 60000\n",
      "Total number of normal data: 24559\n",
      "Total number of abnormal data: 35441\n",
      "------------------\n",
      "Number of abnormalities detected: 29751\n",
      "Number of normalities detected: 30249\n",
      "accuracy: 49.586666666666666 %\n",
      "------------------\n",
      "Threshold: -0.163 --> Detected abnormalities: 0, Detected normals: 60000, Accuracy: 40.93%\n",
      "Threshold: -0.157 --> Detected abnormalities: 2, Detected normals: 59998, Accuracy: 40.93%\n",
      "Threshold: -0.152 --> Detected abnormalities: 4, Detected normals: 59996, Accuracy: 40.94%\n",
      "Threshold: -0.146 --> Detected abnormalities: 4, Detected normals: 59996, Accuracy: 40.94%\n",
      "Threshold: -0.141 --> Detected abnormalities: 9, Detected normals: 59991, Accuracy: 40.94%\n",
      "Threshold: -0.135 --> Detected abnormalities: 13, Detected normals: 59987, Accuracy: 40.94%\n",
      "Threshold: -0.130 --> Detected abnormalities: 28, Detected normals: 59972, Accuracy: 40.95%\n",
      "Threshold: -0.124 --> Detected abnormalities: 54, Detected normals: 59946, Accuracy: 40.94%\n",
      "Threshold: -0.119 --> Detected abnormalities: 76, Detected normals: 59924, Accuracy: 40.96%\n",
      "Threshold: -0.113 --> Detected abnormalities: 118, Detected normals: 59882, Accuracy: 40.99%\n",
      "Threshold: -0.108 --> Detected abnormalities: 174, Detected normals: 59826, Accuracy: 41.02%\n",
      "Threshold: -0.102 --> Detected abnormalities: 245, Detected normals: 59755, Accuracy: 41.04%\n",
      "Threshold: -0.097 --> Detected abnormalities: 362, Detected normals: 59638, Accuracy: 41.08%\n",
      "Threshold: -0.092 --> Detected abnormalities: 497, Detected normals: 59503, Accuracy: 41.09%\n",
      "Threshold: -0.086 --> Detected abnormalities: 696, Detected normals: 59304, Accuracy: 41.12%\n",
      "Threshold: -0.081 --> Detected abnormalities: 934, Detected normals: 59066, Accuracy: 41.21%\n",
      "Threshold: -0.075 --> Detected abnormalities: 1256, Detected normals: 58744, Accuracy: 41.28%\n",
      "Threshold: -0.070 --> Detected abnormalities: 1698, Detected normals: 58302, Accuracy: 41.43%\n",
      "Threshold: -0.064 --> Detected abnormalities: 2294, Detected normals: 57706, Accuracy: 41.57%\n",
      "Threshold: -0.059 --> Detected abnormalities: 3018, Detected normals: 56982, Accuracy: 41.76%\n",
      "Threshold: -0.053 --> Detected abnormalities: 3872, Detected normals: 56128, Accuracy: 42.02%\n",
      "Threshold: -0.048 --> Detected abnormalities: 4919, Detected normals: 55081, Accuracy: 42.37%\n",
      "Threshold: -0.042 --> Detected abnormalities: 6181, Detected normals: 53819, Accuracy: 42.74%\n",
      "Threshold: -0.037 --> Detected abnormalities: 7634, Detected normals: 52366, Accuracy: 43.16%\n",
      "Threshold: -0.031 --> Detected abnormalities: 9310, Detected normals: 50690, Accuracy: 43.70%\n",
      "Threshold: -0.026 --> Detected abnormalities: 11248, Detected normals: 48752, Accuracy: 44.28%\n",
      "Threshold: -0.021 --> Detected abnormalities: 13409, Detected normals: 46591, Accuracy: 44.97%\n",
      "Threshold: -0.015 --> Detected abnormalities: 15882, Detected normals: 44118, Accuracy: 45.75%\n",
      "Threshold: -0.010 --> Detected abnormalities: 18630, Detected normals: 41370, Accuracy: 46.48%\n",
      "Threshold: -0.004 --> Detected abnormalities: 21482, Detected normals: 38518, Accuracy: 47.29%\n",
      "Threshold: 0.001 --> Detected abnormalities: 24604, Detected normals: 35396, Accuracy: 48.10%\n",
      "Threshold: 0.007 --> Detected abnormalities: 27766, Detected normals: 32234, Accuracy: 49.05%\n",
      "Threshold: 0.012 --> Detected abnormalities: 31056, Detected normals: 28944, Accuracy: 50.04%\n",
      "Threshold: 0.018 --> Detected abnormalities: 34383, Detected normals: 25617, Accuracy: 51.01%\n",
      "Threshold: 0.023 --> Detected abnormalities: 37900, Detected normals: 22100, Accuracy: 52.11%\n",
      "Threshold: 0.029 --> Detected abnormalities: 41213, Detected normals: 18787, Accuracy: 53.26%\n",
      "Threshold: 0.034 --> Detected abnormalities: 44365, Detected normals: 15635, Accuracy: 54.10%\n",
      "Threshold: 0.040 --> Detected abnormalities: 47336, Detected normals: 12664, Accuracy: 55.05%\n",
      "Threshold: 0.045 --> Detected abnormalities: 50097, Detected normals: 9903, Accuracy: 56.01%\n",
      "Threshold: 0.050 --> Detected abnormalities: 52483, Detected normals: 7517, Accuracy: 56.77%\n",
      "Threshold: 0.056 --> Detected abnormalities: 54555, Detected normals: 5445, Accuracy: 57.32%\n",
      "Threshold: 0.061 --> Detected abnormalities: 56128, Detected normals: 3872, Accuracy: 57.81%\n",
      "Threshold: 0.067 --> Detected abnormalities: 57468, Detected normals: 2532, Accuracy: 58.16%\n",
      "Threshold: 0.072 --> Detected abnormalities: 58480, Detected normals: 1520, Accuracy: 58.46%\n",
      "Threshold: 0.078 --> Detected abnormalities: 59125, Detected normals: 875, Accuracy: 58.75%\n",
      "Threshold: 0.083 --> Detected abnormalities: 59561, Detected normals: 439, Accuracy: 58.84%\n",
      "Threshold: 0.089 --> Detected abnormalities: 59838, Detected normals: 162, Accuracy: 58.98%\n",
      "Threshold: 0.094 --> Detected abnormalities: 59951, Detected normals: 49, Accuracy: 59.04%\n",
      "Threshold: 0.100 --> Detected abnormalities: 59988, Detected normals: 12, Accuracy: 59.07%\n",
      "Threshold: 0.105 --> Detected abnormalities: 59999, Detected normals: 1, Accuracy: 59.07%\n",
      "The best threshold: 0.09967363529581569, with Accuracy: 59.068333333333335%\n"
     ]
    }
   ],
   "source": [
    "fit_with_clf(features, forest_labels)\n",
    "fit_with_different_th(features, forest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
