{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in '/Users/liushiwen/Desktop/大四下/hw2/test_data':\n",
      "0000001.png\n",
      "0000000.png\n",
      "0000002.png\n",
      "0018388.txt\n",
      "0018389.txt\n",
      "0000003.png\n",
      "0000007.png\n",
      "0000006.png\n",
      "0000005.png\n",
      "0000000.txt\n",
      "0000001.txt\n",
      "0018389.png\n",
      "0000003.txt\n",
      "0000002.txt\n",
      "0018388.png\n",
      "0000006.txt\n",
      "0000007.txt\n",
      "0000005.txt\n",
      "0018383.png\n",
      "0000009.txt\n",
      "0000008.txt\n",
      "0018382.png\n",
      "0018381.png\n",
      "0018385.png\n",
      "0018386.png\n",
      "0018387.png\n",
      "0000008.png\n",
      "0018382.txt\n",
      "0018383.txt\n",
      "0000009.png\n",
      "0018381.txt\n",
      "0018385.txt\n",
      "0018387.txt\n",
      "0018386.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "path = \"/Users/liushiwen/Desktop/大四下/hw2/test_data\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(f\"Files and directories in '{path}':\")\n",
    "for item in dir_list:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.7734375\t0.4296875\t0.21875\t0.21875\n",
      "\n",
      "(128, 128, 4)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACAAIABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiux8DeBV8WQarf3uqxaVpWmRB7i6ePzME52gLkZ6H+Q610OneAfAniO6TStB8bTtqzofLF1ZFIpnHYHjGfqTwevSvNtQsLnS9RuLC8iMVzbyNFKh6qwOCKrUUUUUUUUUUUUV3+pONC+Dej2McLLNr93LeXEjLj5ITsRR6jndXH6G80Wv6bJblhMt1E0ZXqGDDGPfNdV8Y1C/FnXgoAHmRnj/rklcNRRRRRRRRRRRRXq+nWeh/EDwPoGnXvizT9EvtEWaJ0vQEWRHfKlSWGcAc1Pa2PgT4aMdW/wCEhg8S6/CpNnb2qhrdJONrswJHB5+968cV5VqN/c6rqVzqF5KZbm5kaWVz3YnJqtRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAABfklEQVR4Ae3WPSxDURTA8eNhIkKYGlMNIhJLh3ZpKkZtJBIJg53EZhKTRGI3YWA1WYhEE7HQNoil4mOxsIjXpZUuUpd+uO89nSxOl/9b7rm3w//kN1WEDwEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgxQJtuv14djiVPJbcuW7W1nqOyoWSqX5ld8a+qQpsLYg8uCVxJuU9nrcr6J2jrnkeH+wWcdYqXwd9emFbipnPpZ9548Mk7bPemTB7XuzJ7DZmx3v6/2FdLr1IWmLqC4RDxVtvgTM7KQrMh0+zNuufigvMFTf9rjcpLiCPF17WH/QW6Or0q4FJb4HZoUBWpqTSuOotEMxLJCWrrVwgstybSf/aSOEyUczZSvu+eYnai955fzdQj41tXxmTsN0OOyicIyevtUq0XwqH1wq95sT0Te2/SPWrvK00/6ZzD+Xr/Z1FnRwVBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQOCPAt+lp1OwIXhrjwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(f\"{path}/0000002.txt\", \"r\")\n",
    "print(f.read())\n",
    "img = Image.open(f'{path}/0000002.png')\n",
    "img = np.array(img)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img[:,:,2])\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_files = [f for f in os.listdir(data_folder) if f.endswith(\".png\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_folder, self.image_files[idx])\n",
    "        image = read_image(image_path)\n",
    "        image = image[0].float()\n",
    "        image = image.reshape(1, 128, 128)\n",
    "        \n",
    "        txt_file = os.path.splitext(self.image_files[idx])[0] + \".txt\"\n",
    "        txt_path = os.path.join(self.data_folder, txt_file)\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "            class_label = int(first_line.split()[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_label\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                ])\n",
    "data_folder = path\n",
    "custom_dataset = CustomImageDataset(data_folder, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(custom_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(custom_dataset, batch_size=4, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.1324817933200393e-05\n",
      "Epoch 2/1000, Loss: 0.0\n",
      "Epoch 3/1000, Loss: 0.0\n",
      "Epoch 4/1000, Loss: 0.0\n",
      "Epoch 5/1000, Loss: 0.0\n",
      "Epoch 6/1000, Loss: 0.0\n",
      "Epoch 7/1000, Loss: 0.0\n",
      "Epoch 8/1000, Loss: 3.576278118089249e-07\n",
      "Epoch 9/1000, Loss: 0.0\n",
      "Epoch 10/1000, Loss: 0.0\n",
      "Epoch 20/1000, Loss: 0.0\n",
      "Epoch 30/1000, Loss: 0.0\n",
      "Epoch 40/1000, Loss: 0.0\n",
      "Epoch 50/1000, Loss: 0.0\n",
      "Epoch 60/1000, Loss: 0.0\n",
      "Epoch 70/1000, Loss: 0.0\n",
      "Epoch 80/1000, Loss: 0.0\n",
      "Epoch 90/1000, Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(labels, outputs)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/casper_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/casper_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 64 * 64)  # Flatten the feature maps\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(labels, outputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 9 or epoch < 10:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "tensor([[-0.0370, -0.0774, -0.1944, -0.1077, -0.0653,  0.0758, -0.0353,  0.0713,\n",
      "          0.0339, -0.0830],\n",
      "        [-0.0468, -0.0486, -0.1199, -0.0517, -0.0887,  0.0585, -0.0269,  0.0273,\n",
      "          0.0667, -0.1007],\n",
      "        [-0.0311, -0.0728, -0.1561, -0.0592, -0.0468,  0.0428, -0.0086,  0.0467,\n",
      "          0.0123, -0.0589],\n",
      "        [-0.0295, -0.0325, -0.1210, -0.0421, -0.0297,  0.0357, -0.0792, -0.0095,\n",
      "         -0.0038, -0.0641]], grad_fn=<AddmmBackward0>)\n",
      "tensor([8, 7, 9, 8])\n",
      "Epoch [1/10], Loss: 2.268433094024658\n",
      "tensor([[-2.3636,  0.2015, -0.5841, -1.2966, -2.4347,  0.1105, -1.8953,  2.1597,\n",
      "          4.9252,  0.2118],\n",
      "        [-2.6802,  0.2150, -0.5860, -1.3900, -2.7268,  0.1192, -2.0908,  2.2224,\n",
      "          6.0034,  0.5378],\n",
      "        [-2.7443,  0.1947, -0.6499, -1.3994, -2.5631,  0.2552, -2.1583,  2.3136,\n",
      "          5.5021, -0.0383],\n",
      "        [-3.8124,  0.3114, -0.8124, -1.8925, -3.5374,  0.3578, -2.9969,  3.0534,\n",
      "          8.2173,  0.2968]], grad_fn=<AddmmBackward0>)\n",
      "tensor([6, 9, 2, 8])\n",
      "Epoch [2/10], Loss: 4.65770149230957\n",
      "tensor([[-2.9355, -0.5820, -0.2533, -3.2248, -4.2405, -1.8743,  0.0324,  2.2684,\n",
      "          3.4523,  3.1918],\n",
      "        [-2.1612, -0.3855, -0.0988, -2.3152, -3.1473, -1.3154, -0.0277,  1.5821,\n",
      "          2.7716,  2.4024],\n",
      "        [-3.0576, -0.6184, -0.0837, -3.0380, -4.1624, -1.5700, -0.1084,  2.2424,\n",
      "          3.7516,  2.9168],\n",
      "        [-2.8416, -0.4968, -0.1491, -3.0535, -4.1647, -1.8029,  0.0044,  2.1403,\n",
      "          3.7702,  3.2354]], grad_fn=<AddmmBackward0>)\n",
      "tensor([3, 9, 5, 5])\n",
      "Epoch [3/10], Loss: 5.162044525146484\n",
      "tensor([[-2.7184, -1.1989,  0.1078, -2.8238, -3.7564, -0.7584,  0.6344,  1.8593,\n",
      "          0.8316,  2.6407],\n",
      "        [-3.6701, -1.7524,  0.1778, -3.3852, -4.9624, -0.8686,  0.7808,  2.8275,\n",
      "          0.7897,  2.7497],\n",
      "        [-3.1690, -1.3465,  0.4556, -2.8023, -4.0268, -0.3352,  0.4254,  2.0831,\n",
      "          1.2892,  2.3321],\n",
      "        [-2.3743, -1.0535, -0.1297, -2.3027, -3.2676, -0.6312,  0.6841,  1.9579,\n",
      "          0.3616,  1.8339]], grad_fn=<AddmmBackward0>)\n",
      "tensor([8, 4, 0, 1])\n",
      "Epoch [4/10], Loss: 5.372462749481201\n",
      "tensor([[-1.7404, -0.8357,  0.2520, -1.9790, -2.2123, -0.1348,  0.5633,  0.7694,\n",
      "         -0.3366,  1.4861],\n",
      "        [-2.7113, -1.3138,  0.6520, -2.8527, -3.1120,  0.1797,  0.4138,  1.1128,\n",
      "          0.1198,  1.9972],\n",
      "        [-2.1033, -0.7970,  0.4400, -2.2718, -2.4430,  0.1462,  0.6350,  0.6689,\n",
      "         -0.0413,  1.7677],\n",
      "        [-1.3406, -0.6515,  0.0940, -1.6293, -1.8653, -0.2279,  0.6164,  0.7172,\n",
      "         -0.5786,  1.1875]], grad_fn=<AddmmBackward0>)\n",
      "tensor([4, 0, 3, 2])\n",
      "Epoch [5/10], Loss: 4.356569290161133\n",
      "tensor([[-1.4337, -0.9235,  1.1268, -1.1674, -1.3270,  0.9097, -0.1814,  0.2824,\n",
      "         -0.4705,  0.5409],\n",
      "        [-1.0668, -0.5229,  0.5985, -1.1227, -0.9813,  0.4523,  0.1962,  0.1981,\n",
      "         -0.6799,  0.6447],\n",
      "        [-2.3099, -1.6449,  1.6977, -1.7287, -2.0108,  1.3055, -0.3682,  0.7028,\n",
      "         -0.6836,  0.5903],\n",
      "        [-1.0365, -0.5184,  0.5816, -1.0628, -0.9895,  0.5149,  0.2015,  0.1303,\n",
      "         -0.6243,  0.6763]], grad_fn=<AddmmBackward0>)\n",
      "tensor([2, 8, 0, 6])\n",
      "Epoch [6/10], Loss: 2.8425076007843018\n",
      "tensor([[-0.3078, -0.0334,  0.3193, -0.3222, -0.2680,  0.1197,  0.1133, -0.2186,\n",
      "         -0.2757,  0.3254],\n",
      "        [-0.6716, -0.5409,  0.8582, -0.5266, -0.8166,  0.8096, -0.3034, -0.3052,\n",
      "         -0.2669,  0.2568],\n",
      "        [-0.7243, -0.6605,  0.9063, -0.9161, -1.0744,  0.6547, -0.3106, -0.2629,\n",
      "         -0.5863,  0.5825],\n",
      "        [-0.7114, -0.7431,  0.9856, -0.9699, -1.1426,  0.7168, -0.4121, -0.2899,\n",
      "         -0.5836,  0.5993]], grad_fn=<AddmmBackward0>)\n",
      "tensor([7, 3, 2, 7])\n",
      "Epoch [7/10], Loss: 2.3396124839782715\n",
      "tensor([[-0.4911, -0.3059,  0.8395, -0.2643, -0.6785,  0.3713, -0.3410,  0.2468,\n",
      "         -0.5091, -0.1058],\n",
      "        [-0.3699, -0.3439,  0.6433, -0.4084, -0.2113,  0.2301, -0.0807, -0.7181,\n",
      "         -0.1991,  0.7331],\n",
      "        [-0.7620, -0.6187,  1.4717, -0.2269, -0.9979,  0.7811, -0.6407,  0.3836,\n",
      "         -0.7511, -0.3296],\n",
      "        [-0.4793, -0.5108,  1.3156, -0.4598, -0.7587,  0.5967, -0.6067, -0.5636,\n",
      "         -0.3716,  0.4997]], grad_fn=<AddmmBackward0>)\n",
      "tensor([6, 1, 6, 3])\n",
      "Epoch [8/10], Loss: 2.828756332397461\n",
      "tensor([[-0.1732, -0.4104,  0.8099,  1.0578, -0.2806,  0.0454,  0.3682, -0.9486,\n",
      "         -0.0188, -0.4078],\n",
      "        [-0.2592, -0.2447,  0.6381,  0.5643, -0.4440,  0.0655,  0.0961,  0.2170,\n",
      "         -0.4278, -0.6376],\n",
      "        [-0.3710, -0.3657,  0.8786,  0.2299, -0.4248, -0.1334,  0.3529, -0.5289,\n",
      "         -0.5167, -0.2464],\n",
      "        [-0.2266, -0.6355,  1.1146,  0.5308, -0.7564, -0.1338,  0.1811, -0.4519,\n",
      "         -0.5451, -0.4433]], grad_fn=<AddmmBackward0>)\n",
      "tensor([5, 5, 7, 1])\n",
      "Epoch [9/10], Loss: 2.631314516067505\n",
      "tensor([[-0.4685,  0.1277,  0.1836, -0.1387, -0.4346, -0.2250,  0.4009,  0.3106,\n",
      "         -0.7385, -0.5431],\n",
      "        [-0.2413,  0.0903,  0.1894,  0.4789, -0.6357, -0.2998,  0.2462,  0.2477,\n",
      "         -0.6048, -0.6523],\n",
      "        [-0.1276,  0.1647, -0.0290,  0.7751, -0.2867, -0.2746,  0.3838, -0.3855,\n",
      "         -0.1211, -0.3908],\n",
      "        [-0.5656,  0.0113,  0.0770,  0.0751, -0.6488, -0.1356,  0.4780,  0.5762,\n",
      "         -0.8533, -0.7695]], grad_fn=<AddmmBackward0>)\n",
      "tensor([4, 4, 9, 6])\n",
      "Epoch [10/10], Loss: 2.512491226196289\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 64 * 64)  # Flatten the feature maps\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset (you can replace this with your own dataset)\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (you can adjust the number of epochs)\n",
    "for epoch in range(10):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on a validation/test dataset\n",
    "# (similar to the training loop but using validation/test data)\n",
    "# ...\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
