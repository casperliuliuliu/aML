{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in '/Users/liushiwen/Desktop/大四下/hw2/test_data':\n",
      "0000001.png\n",
      "0000000.png\n",
      "0000002.png\n",
      "0018388.txt\n",
      "0018389.txt\n",
      "0000003.png\n",
      "0000007.png\n",
      "0000006.png\n",
      "0000005.png\n",
      "0000000.txt\n",
      "0000001.txt\n",
      "0018389.png\n",
      "0000003.txt\n",
      "0000002.txt\n",
      "0018388.png\n",
      "0000006.txt\n",
      "0000007.txt\n",
      "0000005.txt\n",
      "0018383.png\n",
      "0000009.txt\n",
      "0000008.txt\n",
      "0018382.png\n",
      "0018381.png\n",
      "0018385.png\n",
      "0018386.png\n",
      "0018387.png\n",
      "0000008.png\n",
      "0018382.txt\n",
      "0018383.txt\n",
      "0000009.png\n",
      "0018381.txt\n",
      "0018385.txt\n",
      "0018387.txt\n",
      "0018386.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "path = \"/Users/liushiwen/Desktop/大四下/hw2/test_data\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(f\"Files and directories in '{path}':\")\n",
    "for item in dir_list:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.7734375\t0.4296875\t0.21875\t0.21875\n",
      "\n",
      "(128, 128, 4)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACAAIABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiux8DeBV8WQarf3uqxaVpWmRB7i6ePzME52gLkZ6H+Q610OneAfAniO6TStB8bTtqzofLF1ZFIpnHYHjGfqTwevSvNtQsLnS9RuLC8iMVzbyNFKh6qwOCKrUUUUUUUUUUUUV3+pONC+Dej2McLLNr93LeXEjLj5ITsRR6jndXH6G80Wv6bJblhMt1E0ZXqGDDGPfNdV8Y1C/FnXgoAHmRnj/rklcNRRRRRRRRRRRRXq+nWeh/EDwPoGnXvizT9EvtEWaJ0vQEWRHfKlSWGcAc1Pa2PgT4aMdW/wCEhg8S6/CpNnb2qhrdJONrswJHB5+968cV5VqN/c6rqVzqF5KZbm5kaWVz3YnJqtRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAABfklEQVR4Ae3WPSxDURTA8eNhIkKYGlMNIhJLh3ZpKkZtJBIJg53EZhKTRGI3YWA1WYhEE7HQNoil4mOxsIjXpZUuUpd+uO89nSxOl/9b7rm3w//kN1WEDwEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgxQJtuv14djiVPJbcuW7W1nqOyoWSqX5ld8a+qQpsLYg8uCVxJuU9nrcr6J2jrnkeH+wWcdYqXwd9emFbipnPpZ9548Mk7bPemTB7XuzJ7DZmx3v6/2FdLr1IWmLqC4RDxVtvgTM7KQrMh0+zNuufigvMFTf9rjcpLiCPF17WH/QW6Or0q4FJb4HZoUBWpqTSuOotEMxLJCWrrVwgstybSf/aSOEyUczZSvu+eYnai955fzdQj41tXxmTsN0OOyicIyevtUq0XwqH1wq95sT0Te2/SPWrvK00/6ZzD+Xr/Z1FnRwVBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQOCPAt+lp1OwIXhrjwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(f\"{path}/0000002.txt\", \"r\")\n",
    "print(f.read())\n",
    "img = Image.open(f'{path}/0000002.png')\n",
    "img = np.array(img)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img[:,:,2])\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_files = [f for f in os.listdir(data_folder) if f.endswith(\".png\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_folder, self.image_files[idx])\n",
    "        image = read_image(image_path)\n",
    "        image = image[0].float()\n",
    "        image = image.reshape(1, 128, 128)\n",
    "        \n",
    "        txt_file = os.path.splitext(self.image_files[idx])[0] + \".txt\"\n",
    "        txt_path = os.path.join(self.data_folder, txt_file)\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "            class_label = int(first_line.split()[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_label\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                ])\n",
    "data_folder = path\n",
    "custom_dataset = CustomImageDataset(data_folder, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(custom_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(custom_dataset, batch_size=4, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.1324817933200393e-05\n",
      "Epoch 2/1000, Loss: 0.0\n",
      "Epoch 3/1000, Loss: 0.0\n",
      "Epoch 4/1000, Loss: 0.0\n",
      "Epoch 5/1000, Loss: 0.0\n",
      "Epoch 6/1000, Loss: 0.0\n",
      "Epoch 7/1000, Loss: 0.0\n",
      "Epoch 8/1000, Loss: 3.576278118089249e-07\n",
      "Epoch 9/1000, Loss: 0.0\n",
      "Epoch 10/1000, Loss: 0.0\n",
      "Epoch 20/1000, Loss: 0.0\n",
      "Epoch 30/1000, Loss: 0.0\n",
      "Epoch 40/1000, Loss: 0.0\n",
      "Epoch 50/1000, Loss: 0.0\n",
      "Epoch 60/1000, Loss: 0.0\n",
      "Epoch 70/1000, Loss: 0.0\n",
      "Epoch 80/1000, Loss: 0.0\n",
      "Epoch 90/1000, Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(labels, outputs)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/casper_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/casper_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 64 * 64)  # Flatten the feature maps\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(labels, outputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 9 or epoch < 10:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 2.273475170135498\n",
      "Epoch [2/10], Loss: 15.71927547454834\n",
      "Epoch [3/10], Loss: 7.32181453704834\n",
      "Epoch [4/10], Loss: 4.093669891357422\n",
      "Epoch [5/10], Loss: 4.283098220825195\n",
      "Epoch [6/10], Loss: 2.4698548316955566\n",
      "Epoch [7/10], Loss: 2.48203182220459\n",
      "Epoch [8/10], Loss: 2.167850971221924\n",
      "Epoch [9/10], Loss: 3.3266916275024414\n",
      "Epoch [10/10], Loss: 2.9277188777923584\n",
      "Epoch [11/10], Loss: 2.4954380989074707\n",
      "Epoch [12/10], Loss: 2.4133386611938477\n",
      "Epoch [13/10], Loss: 3.1566269397735596\n",
      "Epoch [14/10], Loss: 2.3089001178741455\n",
      "Epoch [15/10], Loss: 3.1153807640075684\n",
      "Epoch [16/10], Loss: 3.164628028869629\n",
      "Epoch [17/10], Loss: 2.816922664642334\n",
      "Epoch [18/10], Loss: 2.5045666694641113\n",
      "Epoch [19/10], Loss: 2.2221832275390625\n",
      "Epoch [20/10], Loss: 2.5082828998565674\n",
      "Epoch [21/10], Loss: 2.1594483852386475\n",
      "Epoch [22/10], Loss: 2.7962653636932373\n",
      "Epoch [23/10], Loss: 2.342620611190796\n",
      "Epoch [24/10], Loss: 2.525022268295288\n",
      "Epoch [25/10], Loss: 1.9973974227905273\n",
      "Epoch [26/10], Loss: 2.3781309127807617\n",
      "Epoch [27/10], Loss: 2.208195924758911\n",
      "Epoch [28/10], Loss: 2.307586669921875\n",
      "Epoch [29/10], Loss: 2.428415060043335\n",
      "Epoch [30/10], Loss: 2.214451789855957\n",
      "Epoch [31/10], Loss: 2.115446090698242\n",
      "Epoch [32/10], Loss: 2.447688579559326\n",
      "Epoch [33/10], Loss: 2.616774559020996\n",
      "Epoch [34/10], Loss: 2.5933685302734375\n",
      "Epoch [35/10], Loss: 2.3941187858581543\n",
      "Epoch [36/10], Loss: 2.2343385219573975\n",
      "Epoch [37/10], Loss: 2.5091142654418945\n",
      "Epoch [38/10], Loss: 2.152698040008545\n",
      "Epoch [39/10], Loss: 2.2811336517333984\n",
      "Epoch [40/10], Loss: 2.0279531478881836\n",
      "Epoch [41/10], Loss: 2.024364471435547\n",
      "Epoch [42/10], Loss: 2.052638530731201\n",
      "Epoch [43/10], Loss: 2.2868056297302246\n",
      "Epoch [44/10], Loss: 2.458653688430786\n",
      "Epoch [45/10], Loss: 2.407616138458252\n",
      "Epoch [46/10], Loss: 2.1426541805267334\n",
      "Epoch [47/10], Loss: 2.2747130393981934\n",
      "Epoch [48/10], Loss: 2.4429678916931152\n",
      "Epoch [49/10], Loss: 2.3353796005249023\n",
      "Epoch [50/10], Loss: 2.064222812652588\n",
      "Epoch [51/10], Loss: 2.6878888607025146\n",
      "Epoch [52/10], Loss: 2.526495933532715\n",
      "Epoch [53/10], Loss: 2.4331421852111816\n",
      "Epoch [54/10], Loss: 2.0183584690093994\n",
      "Epoch [55/10], Loss: 2.0609476566314697\n",
      "Epoch [56/10], Loss: 2.321291446685791\n",
      "Epoch [57/10], Loss: 2.827314615249634\n",
      "Epoch [58/10], Loss: 2.25909423828125\n",
      "Epoch [59/10], Loss: 2.3433520793914795\n",
      "Epoch [60/10], Loss: 2.275461435317993\n",
      "Epoch [61/10], Loss: 2.47715163230896\n",
      "Epoch [62/10], Loss: 2.528886318206787\n",
      "Epoch [63/10], Loss: 2.0096993446350098\n",
      "Epoch [64/10], Loss: 2.1526098251342773\n",
      "Epoch [65/10], Loss: 2.507323741912842\n",
      "Epoch [66/10], Loss: 2.6311819553375244\n",
      "Epoch [67/10], Loss: 1.8703769445419312\n",
      "Epoch [68/10], Loss: 2.1241726875305176\n",
      "Epoch [69/10], Loss: 2.399935722351074\n",
      "Epoch [70/10], Loss: 2.730358123779297\n",
      "Epoch [71/10], Loss: 2.613340139389038\n",
      "Epoch [72/10], Loss: 2.419595956802368\n",
      "Epoch [73/10], Loss: 2.152984619140625\n",
      "Epoch [74/10], Loss: 2.217165470123291\n",
      "Epoch [75/10], Loss: 2.2607343196868896\n",
      "Epoch [76/10], Loss: 2.3564562797546387\n",
      "Epoch [77/10], Loss: 2.400040864944458\n",
      "Epoch [78/10], Loss: 2.067589282989502\n",
      "Epoch [79/10], Loss: 2.1380245685577393\n",
      "Epoch [80/10], Loss: 2.4604363441467285\n",
      "Epoch [81/10], Loss: 2.06927752494812\n",
      "Epoch [82/10], Loss: 2.0021698474884033\n",
      "Epoch [83/10], Loss: 2.338944435119629\n",
      "Epoch [84/10], Loss: 2.0568766593933105\n",
      "Epoch [85/10], Loss: 2.198791980743408\n",
      "Epoch [86/10], Loss: 2.3424675464630127\n",
      "Epoch [87/10], Loss: 2.082231044769287\n",
      "Epoch [88/10], Loss: 1.668031096458435\n",
      "Epoch [89/10], Loss: 2.5311994552612305\n",
      "Epoch [90/10], Loss: 2.590280771255493\n",
      "Epoch [91/10], Loss: 2.272603988647461\n",
      "Epoch [92/10], Loss: 1.9568226337432861\n",
      "Epoch [93/10], Loss: 1.9080828428268433\n",
      "Epoch [94/10], Loss: 2.1880862712860107\n",
      "Epoch [95/10], Loss: 1.7222082614898682\n",
      "Epoch [96/10], Loss: 2.3800957202911377\n",
      "Epoch [97/10], Loss: 2.0252604484558105\n",
      "Epoch [98/10], Loss: 2.1883416175842285\n",
      "Epoch [99/10], Loss: 2.4317197799682617\n",
      "Epoch [100/10], Loss: 2.126441478729248\n",
      "Epoch [101/10], Loss: 2.350245714187622\n",
      "Epoch [102/10], Loss: 2.047546148300171\n",
      "Epoch [103/10], Loss: 1.7250821590423584\n",
      "Epoch [104/10], Loss: 2.2805347442626953\n",
      "Epoch [105/10], Loss: 2.4723246097564697\n",
      "Epoch [106/10], Loss: 2.3630964756011963\n",
      "Epoch [107/10], Loss: 2.6046502590179443\n",
      "Epoch [108/10], Loss: 2.6381778717041016\n",
      "Epoch [109/10], Loss: 2.794156074523926\n",
      "Epoch [110/10], Loss: 2.352806568145752\n",
      "Epoch [111/10], Loss: 2.3576085567474365\n",
      "Epoch [112/10], Loss: 2.61063814163208\n",
      "Epoch [113/10], Loss: 2.1748433113098145\n",
      "Epoch [114/10], Loss: 2.505199432373047\n",
      "Epoch [115/10], Loss: 1.9864661693572998\n",
      "Epoch [116/10], Loss: 2.1683361530303955\n",
      "Epoch [117/10], Loss: 2.348788022994995\n",
      "Epoch [118/10], Loss: 2.0537166595458984\n",
      "Epoch [119/10], Loss: 2.058649778366089\n",
      "Epoch [120/10], Loss: 2.668023109436035\n",
      "Epoch [121/10], Loss: 2.0488638877868652\n",
      "Epoch [122/10], Loss: 2.4430251121520996\n",
      "Epoch [123/10], Loss: 2.250349998474121\n",
      "Epoch [124/10], Loss: 2.437751293182373\n",
      "Epoch [125/10], Loss: 2.2361040115356445\n",
      "Epoch [126/10], Loss: 2.5748157501220703\n",
      "Epoch [127/10], Loss: 2.1850578784942627\n",
      "Epoch [128/10], Loss: 1.7801190614700317\n",
      "Epoch [129/10], Loss: 2.2879912853240967\n",
      "Epoch [130/10], Loss: 1.8210481405258179\n",
      "Epoch [131/10], Loss: 1.796069622039795\n",
      "Epoch [132/10], Loss: 2.3333163261413574\n",
      "Epoch [133/10], Loss: 1.8641488552093506\n",
      "Epoch [134/10], Loss: 2.153811454772949\n",
      "Epoch [135/10], Loss: 2.109711170196533\n",
      "Epoch [136/10], Loss: 2.885028839111328\n",
      "Epoch [137/10], Loss: 2.78267765045166\n",
      "Epoch [138/10], Loss: 1.9777042865753174\n",
      "Epoch [139/10], Loss: 2.0803003311157227\n",
      "Epoch [140/10], Loss: 2.353592872619629\n",
      "Epoch [141/10], Loss: 2.185494899749756\n",
      "Epoch [142/10], Loss: 2.697284698486328\n",
      "Epoch [143/10], Loss: 2.4067370891571045\n",
      "Epoch [144/10], Loss: 2.1206626892089844\n",
      "Epoch [145/10], Loss: 2.3899993896484375\n",
      "Epoch [146/10], Loss: 2.4504358768463135\n",
      "Epoch [147/10], Loss: 2.005082368850708\n",
      "Epoch [148/10], Loss: 1.9363936185836792\n",
      "Epoch [149/10], Loss: 2.3246703147888184\n",
      "Epoch [150/10], Loss: 2.173022747039795\n",
      "Epoch [151/10], Loss: 2.1405601501464844\n",
      "Epoch [152/10], Loss: 1.9804201126098633\n",
      "Epoch [153/10], Loss: 2.2796239852905273\n",
      "Epoch [154/10], Loss: 2.2249979972839355\n",
      "Epoch [155/10], Loss: 2.022793769836426\n",
      "Epoch [156/10], Loss: 1.8166215419769287\n",
      "Epoch [157/10], Loss: 2.2099781036376953\n",
      "Epoch [158/10], Loss: 2.136880397796631\n",
      "Epoch [159/10], Loss: 1.5063397884368896\n",
      "Epoch [160/10], Loss: 2.010084629058838\n",
      "Epoch [161/10], Loss: 2.2273108959198\n",
      "Epoch [162/10], Loss: 2.201207160949707\n",
      "Epoch [163/10], Loss: 2.2156596183776855\n",
      "Epoch [164/10], Loss: 1.8676186800003052\n",
      "Epoch [165/10], Loss: 2.276841163635254\n",
      "Epoch [166/10], Loss: 2.0272064208984375\n",
      "Epoch [167/10], Loss: 2.480440139770508\n",
      "Epoch [168/10], Loss: 2.304384231567383\n",
      "Epoch [169/10], Loss: 2.5684690475463867\n",
      "Epoch [170/10], Loss: 2.1346471309661865\n",
      "Epoch [171/10], Loss: 2.224693775177002\n",
      "Epoch [172/10], Loss: 2.1285061836242676\n",
      "Epoch [173/10], Loss: 2.548464775085449\n",
      "Epoch [174/10], Loss: 2.4806981086730957\n",
      "Epoch [175/10], Loss: 2.5451667308807373\n",
      "Epoch [176/10], Loss: 1.9885387420654297\n",
      "Epoch [177/10], Loss: 2.313985824584961\n",
      "Epoch [178/10], Loss: 1.698956847190857\n",
      "Epoch [179/10], Loss: 2.011504650115967\n",
      "Epoch [180/10], Loss: 3.0753660202026367\n",
      "Epoch [181/10], Loss: 1.7341845035552979\n",
      "Epoch [182/10], Loss: 2.020029306411743\n",
      "Epoch [183/10], Loss: 1.8671221733093262\n",
      "Epoch [184/10], Loss: 2.004835367202759\n",
      "Epoch [185/10], Loss: 1.9779795408248901\n",
      "Epoch [186/10], Loss: 2.2480363845825195\n",
      "Epoch [187/10], Loss: 2.1578338146209717\n",
      "Epoch [188/10], Loss: 1.8286851644515991\n",
      "Epoch [189/10], Loss: 1.991153359413147\n",
      "Epoch [190/10], Loss: 1.9081554412841797\n",
      "Epoch [191/10], Loss: 1.588737964630127\n",
      "Epoch [192/10], Loss: 2.254744052886963\n",
      "Epoch [193/10], Loss: 2.9420909881591797\n",
      "Epoch [194/10], Loss: 2.177117347717285\n",
      "Epoch [195/10], Loss: 1.9735019207000732\n",
      "Epoch [196/10], Loss: 2.7207601070404053\n",
      "Epoch [197/10], Loss: 1.6737005710601807\n",
      "Epoch [198/10], Loss: 2.480487823486328\n",
      "Epoch [199/10], Loss: 1.808781385421753\n",
      "Epoch [200/10], Loss: 2.621696949005127\n",
      "Epoch [201/10], Loss: 1.8850011825561523\n",
      "Epoch [202/10], Loss: 2.892495632171631\n",
      "Epoch [203/10], Loss: 1.8386917114257812\n",
      "Epoch [204/10], Loss: 1.591361165046692\n",
      "Epoch [205/10], Loss: 2.0252885818481445\n",
      "Epoch [206/10], Loss: 2.4916000366210938\n",
      "Epoch [207/10], Loss: 2.109513759613037\n",
      "Epoch [208/10], Loss: 2.218975067138672\n",
      "Epoch [209/10], Loss: 2.39080810546875\n",
      "Epoch [210/10], Loss: 1.779137134552002\n",
      "Epoch [211/10], Loss: 2.157768964767456\n",
      "Epoch [212/10], Loss: 1.7275404930114746\n",
      "Epoch [213/10], Loss: 2.109272003173828\n",
      "Epoch [214/10], Loss: 2.228926181793213\n",
      "Epoch [215/10], Loss: 1.624309778213501\n",
      "Epoch [216/10], Loss: 1.8043935298919678\n",
      "Epoch [217/10], Loss: 2.2170095443725586\n",
      "Epoch [218/10], Loss: 2.0477871894836426\n",
      "Epoch [219/10], Loss: 2.282593250274658\n",
      "Epoch [220/10], Loss: 1.964766502380371\n",
      "Epoch [221/10], Loss: 2.2467432022094727\n",
      "Epoch [222/10], Loss: 2.4510302543640137\n",
      "Epoch [223/10], Loss: 2.3667988777160645\n",
      "Epoch [224/10], Loss: 2.135622262954712\n",
      "Epoch [225/10], Loss: 2.1040008068084717\n",
      "Epoch [226/10], Loss: 2.1027820110321045\n",
      "Epoch [227/10], Loss: 2.0179948806762695\n",
      "Epoch [228/10], Loss: 1.8486078977584839\n",
      "Epoch [229/10], Loss: 2.2353549003601074\n",
      "Epoch [230/10], Loss: 2.5267646312713623\n",
      "Epoch [231/10], Loss: 1.9315946102142334\n",
      "Epoch [232/10], Loss: 2.366321086883545\n",
      "Epoch [233/10], Loss: 2.139827251434326\n",
      "Epoch [234/10], Loss: 2.214482307434082\n",
      "Epoch [235/10], Loss: 2.153193473815918\n",
      "Epoch [236/10], Loss: 1.90171217918396\n",
      "Epoch [237/10], Loss: 1.74538254737854\n",
      "Epoch [238/10], Loss: 2.6152687072753906\n",
      "Epoch [239/10], Loss: 2.1418585777282715\n",
      "Epoch [240/10], Loss: 2.429248809814453\n",
      "Epoch [241/10], Loss: 2.032676935195923\n",
      "Epoch [242/10], Loss: 2.4367880821228027\n",
      "Epoch [243/10], Loss: 2.3790395259857178\n",
      "Epoch [244/10], Loss: 1.8644583225250244\n",
      "Epoch [245/10], Loss: 2.203775405883789\n",
      "Epoch [246/10], Loss: 2.344719886779785\n",
      "Epoch [247/10], Loss: 2.161128282546997\n",
      "Epoch [248/10], Loss: 1.8168684244155884\n",
      "Epoch [249/10], Loss: 2.0196855068206787\n",
      "Epoch [250/10], Loss: 2.104275703430176\n",
      "Epoch [251/10], Loss: 1.596883773803711\n",
      "Epoch [252/10], Loss: 2.107231616973877\n",
      "Epoch [253/10], Loss: 1.6530365943908691\n",
      "Epoch [254/10], Loss: 1.7923533916473389\n",
      "Epoch [255/10], Loss: 2.337461471557617\n",
      "Epoch [256/10], Loss: 1.7817531824111938\n",
      "Epoch [257/10], Loss: 2.014833927154541\n",
      "Epoch [258/10], Loss: 2.112578868865967\n",
      "Epoch [259/10], Loss: 2.3834586143493652\n",
      "Epoch [260/10], Loss: 1.840489149093628\n",
      "Epoch [261/10], Loss: 2.118833065032959\n",
      "Epoch [262/10], Loss: 1.887993574142456\n",
      "Epoch [263/10], Loss: 1.8498046398162842\n",
      "Epoch [264/10], Loss: 2.1689352989196777\n",
      "Epoch [265/10], Loss: 1.4435584545135498\n",
      "Epoch [266/10], Loss: 1.859941840171814\n",
      "Epoch [267/10], Loss: 1.883658528327942\n",
      "Epoch [268/10], Loss: 1.608680248260498\n",
      "Epoch [269/10], Loss: 2.442629814147949\n",
      "Epoch [270/10], Loss: 2.262559652328491\n",
      "Epoch [271/10], Loss: 2.916860818862915\n",
      "Epoch [272/10], Loss: 1.7500529289245605\n",
      "Epoch [273/10], Loss: 1.8301725387573242\n",
      "Epoch [274/10], Loss: 1.4762953519821167\n",
      "Epoch [275/10], Loss: 2.915811061859131\n",
      "Epoch [276/10], Loss: 1.696439504623413\n",
      "Epoch [277/10], Loss: 1.8124415874481201\n",
      "Epoch [278/10], Loss: 1.6243456602096558\n",
      "Epoch [279/10], Loss: 2.6215779781341553\n",
      "Epoch [280/10], Loss: 2.3493385314941406\n",
      "Epoch [281/10], Loss: 1.6512556076049805\n",
      "Epoch [282/10], Loss: 2.5285279750823975\n",
      "Epoch [283/10], Loss: 2.0909550189971924\n",
      "Epoch [284/10], Loss: 2.5294599533081055\n",
      "Epoch [285/10], Loss: 1.8381242752075195\n",
      "Epoch [286/10], Loss: 1.8545093536376953\n",
      "Epoch [287/10], Loss: 2.251596212387085\n",
      "Epoch [288/10], Loss: 2.0224151611328125\n",
      "Epoch [289/10], Loss: 2.076240301132202\n",
      "Epoch [290/10], Loss: 1.8134267330169678\n",
      "Epoch [291/10], Loss: 1.8119224309921265\n",
      "Epoch [292/10], Loss: 1.6129928827285767\n",
      "Epoch [293/10], Loss: 2.6530861854553223\n",
      "Epoch [294/10], Loss: 2.2266111373901367\n",
      "Epoch [295/10], Loss: 2.349982261657715\n",
      "Epoch [296/10], Loss: 1.9723093509674072\n",
      "Epoch [297/10], Loss: 2.172095537185669\n",
      "Epoch [298/10], Loss: 1.8794291019439697\n",
      "Epoch [299/10], Loss: 1.972105860710144\n",
      "Epoch [300/10], Loss: 1.8949440717697144\n",
      "Epoch [301/10], Loss: 1.6139607429504395\n",
      "Epoch [302/10], Loss: 2.457340717315674\n",
      "Epoch [303/10], Loss: 1.5622587203979492\n",
      "Epoch [304/10], Loss: 2.161776304244995\n",
      "Epoch [305/10], Loss: 2.0937929153442383\n",
      "Epoch [306/10], Loss: 1.8940337896347046\n",
      "Epoch [307/10], Loss: 2.085160732269287\n",
      "Epoch [308/10], Loss: 1.8580282926559448\n",
      "Epoch [309/10], Loss: 1.8057713508605957\n",
      "Epoch [310/10], Loss: 1.7444109916687012\n",
      "Epoch [311/10], Loss: 2.007068634033203\n",
      "Epoch [312/10], Loss: 2.2757959365844727\n",
      "Epoch [313/10], Loss: 2.1864311695098877\n",
      "Epoch [314/10], Loss: 2.1770482063293457\n",
      "Epoch [315/10], Loss: 2.9070053100585938\n",
      "Epoch [316/10], Loss: 1.9426902532577515\n",
      "Epoch [317/10], Loss: 2.494703769683838\n",
      "Epoch [318/10], Loss: 2.4171156883239746\n",
      "Epoch [319/10], Loss: 1.9481250047683716\n",
      "Epoch [320/10], Loss: 1.902453899383545\n",
      "Epoch [321/10], Loss: 1.3982696533203125\n",
      "Epoch [322/10], Loss: 1.832373023033142\n",
      "Epoch [323/10], Loss: 2.9287097454071045\n",
      "Epoch [324/10], Loss: 2.3524937629699707\n",
      "Epoch [325/10], Loss: 1.5198098421096802\n",
      "Epoch [326/10], Loss: 2.3444337844848633\n",
      "Epoch [327/10], Loss: 1.388765811920166\n",
      "Epoch [328/10], Loss: 1.5068812370300293\n",
      "Epoch [329/10], Loss: 2.2155110836029053\n",
      "Epoch [330/10], Loss: 2.01590895652771\n",
      "Epoch [331/10], Loss: 2.3811089992523193\n",
      "Epoch [332/10], Loss: 1.7947648763656616\n",
      "Epoch [333/10], Loss: 2.366267681121826\n",
      "Epoch [334/10], Loss: 2.425246000289917\n",
      "Epoch [335/10], Loss: 2.3810794353485107\n",
      "Epoch [336/10], Loss: 1.821002721786499\n",
      "Epoch [337/10], Loss: 2.295222759246826\n",
      "Epoch [338/10], Loss: 2.593543529510498\n",
      "Epoch [339/10], Loss: 1.9370194673538208\n",
      "Epoch [340/10], Loss: 2.2618465423583984\n",
      "Epoch [341/10], Loss: 1.8687989711761475\n",
      "Epoch [342/10], Loss: 2.485639810562134\n",
      "Epoch [343/10], Loss: 2.4446287155151367\n",
      "Epoch [344/10], Loss: 2.1889429092407227\n",
      "Epoch [345/10], Loss: 1.8572965860366821\n",
      "Epoch [346/10], Loss: 2.1669936180114746\n",
      "Epoch [347/10], Loss: 2.0763888359069824\n",
      "Epoch [348/10], Loss: 2.1783218383789062\n",
      "Epoch [349/10], Loss: 2.456380844116211\n",
      "Epoch [350/10], Loss: 2.131474256515503\n",
      "Epoch [351/10], Loss: 2.8440184593200684\n",
      "Epoch [352/10], Loss: 2.19433856010437\n",
      "Epoch [353/10], Loss: 2.044802188873291\n",
      "Epoch [354/10], Loss: 2.0286824703216553\n",
      "Epoch [355/10], Loss: 1.9996707439422607\n",
      "Epoch [356/10], Loss: 2.227679967880249\n",
      "Epoch [357/10], Loss: 1.9951298236846924\n",
      "Epoch [358/10], Loss: 2.4466629028320312\n",
      "Epoch [359/10], Loss: 2.175445556640625\n",
      "Epoch [360/10], Loss: 2.014507293701172\n",
      "Epoch [361/10], Loss: 2.1392481327056885\n",
      "Epoch [362/10], Loss: 1.9178993701934814\n",
      "Epoch [363/10], Loss: 2.4024391174316406\n",
      "Epoch [364/10], Loss: 2.0764074325561523\n",
      "Epoch [365/10], Loss: 1.9323105812072754\n",
      "Epoch [366/10], Loss: 2.0979785919189453\n",
      "Epoch [367/10], Loss: 1.7048414945602417\n",
      "Epoch [368/10], Loss: 2.5272345542907715\n",
      "Epoch [369/10], Loss: 1.777912974357605\n",
      "Epoch [370/10], Loss: 2.179386854171753\n",
      "Epoch [371/10], Loss: 2.0120718479156494\n",
      "Epoch [372/10], Loss: 1.682439923286438\n",
      "Epoch [373/10], Loss: 1.737492322921753\n",
      "Epoch [374/10], Loss: 2.6381607055664062\n",
      "Epoch [375/10], Loss: 1.6563435792922974\n",
      "Epoch [376/10], Loss: 2.2782368659973145\n",
      "Epoch [377/10], Loss: 1.9437751770019531\n",
      "Epoch [378/10], Loss: 2.0918805599212646\n",
      "Epoch [379/10], Loss: 1.8564099073410034\n",
      "Epoch [380/10], Loss: 1.9279634952545166\n",
      "Epoch [381/10], Loss: 2.2009472846984863\n",
      "Epoch [382/10], Loss: 2.0266716480255127\n",
      "Epoch [383/10], Loss: 2.129279613494873\n",
      "Epoch [384/10], Loss: 1.9074029922485352\n",
      "Epoch [385/10], Loss: 1.1739766597747803\n",
      "Epoch [386/10], Loss: 1.833380937576294\n",
      "Epoch [387/10], Loss: 2.8538637161254883\n",
      "Epoch [388/10], Loss: 1.9217215776443481\n",
      "Epoch [389/10], Loss: 2.3835599422454834\n",
      "Epoch [390/10], Loss: 2.2614645957946777\n",
      "Epoch [391/10], Loss: 1.9259577989578247\n",
      "Epoch [392/10], Loss: 2.481414794921875\n",
      "Epoch [393/10], Loss: 1.7400332689285278\n",
      "Epoch [394/10], Loss: 1.5915476083755493\n",
      "Epoch [395/10], Loss: 2.046870708465576\n",
      "Epoch [396/10], Loss: 1.7420856952667236\n",
      "Epoch [397/10], Loss: 2.03840970993042\n",
      "Epoch [398/10], Loss: 1.8548297882080078\n",
      "Epoch [399/10], Loss: 2.4832942485809326\n",
      "Epoch [400/10], Loss: 2.3177998065948486\n",
      "Epoch [401/10], Loss: 1.7529346942901611\n",
      "Epoch [402/10], Loss: 2.1025328636169434\n",
      "Epoch [403/10], Loss: 1.6760667562484741\n",
      "Epoch [404/10], Loss: 1.9212243556976318\n",
      "Epoch [405/10], Loss: 1.9141976833343506\n",
      "Epoch [406/10], Loss: 2.1590957641601562\n",
      "Epoch [407/10], Loss: 2.077582359313965\n",
      "Epoch [408/10], Loss: 1.9034528732299805\n",
      "Epoch [409/10], Loss: 1.5469233989715576\n",
      "Epoch [410/10], Loss: 1.7574124336242676\n",
      "Epoch [411/10], Loss: 1.8282055854797363\n",
      "Epoch [412/10], Loss: 1.860351800918579\n",
      "Epoch [413/10], Loss: 1.8319690227508545\n",
      "Epoch [414/10], Loss: 2.04728627204895\n",
      "Epoch [415/10], Loss: 1.6925764083862305\n",
      "Epoch [416/10], Loss: 1.5791164636611938\n",
      "Epoch [417/10], Loss: 2.0875329971313477\n",
      "Epoch [418/10], Loss: 1.8503172397613525\n",
      "Epoch [419/10], Loss: 1.4755467176437378\n",
      "Epoch [420/10], Loss: 1.5378392934799194\n",
      "Epoch [421/10], Loss: 1.2183424234390259\n",
      "Epoch [422/10], Loss: 2.4744577407836914\n",
      "Epoch [423/10], Loss: 2.8434324264526367\n",
      "Epoch [424/10], Loss: 1.895506739616394\n",
      "Epoch [425/10], Loss: 2.4917678833007812\n",
      "Epoch [426/10], Loss: 1.1556167602539062\n",
      "Epoch [427/10], Loss: 1.612372636795044\n",
      "Epoch [428/10], Loss: 2.491687774658203\n",
      "Epoch [429/10], Loss: 1.9867348670959473\n",
      "Epoch [430/10], Loss: 0.8362109661102295\n",
      "Epoch [431/10], Loss: 2.4770150184631348\n",
      "Epoch [432/10], Loss: 3.0251173973083496\n",
      "Epoch [433/10], Loss: 1.939023494720459\n",
      "Epoch [434/10], Loss: 2.0731966495513916\n",
      "Epoch [435/10], Loss: 1.5538344383239746\n",
      "Epoch [436/10], Loss: 2.3292078971862793\n",
      "Epoch [437/10], Loss: 1.7507984638214111\n",
      "Epoch [438/10], Loss: 1.099500298500061\n",
      "Epoch [439/10], Loss: 2.2256627082824707\n",
      "Epoch [440/10], Loss: 2.697141170501709\n",
      "Epoch [441/10], Loss: 1.8740363121032715\n",
      "Epoch [442/10], Loss: 2.1378378868103027\n",
      "Epoch [443/10], Loss: 2.867011070251465\n",
      "Epoch [444/10], Loss: 2.243535041809082\n",
      "Epoch [445/10], Loss: 2.4889769554138184\n",
      "Epoch [446/10], Loss: 2.0225019454956055\n",
      "Epoch [447/10], Loss: 1.6259685754776\n",
      "Epoch [448/10], Loss: 1.8734517097473145\n",
      "Epoch [449/10], Loss: 1.5760642290115356\n",
      "Epoch [450/10], Loss: 2.100742816925049\n",
      "Epoch [451/10], Loss: 2.2098379135131836\n",
      "Epoch [452/10], Loss: 1.8106577396392822\n",
      "Epoch [453/10], Loss: 2.110555410385132\n",
      "Epoch [454/10], Loss: 1.7439708709716797\n",
      "Epoch [455/10], Loss: 1.8207690715789795\n",
      "Epoch [456/10], Loss: 1.9349100589752197\n",
      "Epoch [457/10], Loss: 1.37139093875885\n",
      "Epoch [458/10], Loss: 1.9342615604400635\n",
      "Epoch [459/10], Loss: 2.585763454437256\n",
      "Epoch [460/10], Loss: 1.7296806573867798\n",
      "Epoch [461/10], Loss: 2.2850496768951416\n",
      "Epoch [462/10], Loss: 1.327337622642517\n",
      "Epoch [463/10], Loss: 2.2303075790405273\n",
      "Epoch [464/10], Loss: 1.8640692234039307\n",
      "Epoch [465/10], Loss: 2.5924293994903564\n",
      "Epoch [466/10], Loss: 1.9946043491363525\n",
      "Epoch [467/10], Loss: 1.945548176765442\n",
      "Epoch [468/10], Loss: 2.2635693550109863\n",
      "Epoch [469/10], Loss: 1.8879553079605103\n",
      "Epoch [470/10], Loss: 2.1696972846984863\n",
      "Epoch [471/10], Loss: 3.0788698196411133\n",
      "Epoch [472/10], Loss: 1.9595482349395752\n",
      "Epoch [473/10], Loss: 1.859560489654541\n",
      "Epoch [474/10], Loss: 1.8511165380477905\n",
      "Epoch [475/10], Loss: 2.2285242080688477\n",
      "Epoch [476/10], Loss: 2.0215466022491455\n",
      "Epoch [477/10], Loss: 2.0462687015533447\n",
      "Epoch [478/10], Loss: 2.240922212600708\n",
      "Epoch [479/10], Loss: 2.1358654499053955\n",
      "Epoch [480/10], Loss: 1.7813067436218262\n",
      "Epoch [481/10], Loss: 1.9921208620071411\n",
      "Epoch [482/10], Loss: 2.3074097633361816\n",
      "Epoch [483/10], Loss: 2.2962095737457275\n",
      "Epoch [484/10], Loss: 2.006131649017334\n",
      "Epoch [485/10], Loss: 1.6025707721710205\n",
      "Epoch [486/10], Loss: 1.8029773235321045\n",
      "Epoch [487/10], Loss: 1.5603145360946655\n",
      "Epoch [488/10], Loss: 1.8152832984924316\n",
      "Epoch [489/10], Loss: 1.820528268814087\n",
      "Epoch [490/10], Loss: 2.252608299255371\n",
      "Epoch [491/10], Loss: 1.3903687000274658\n",
      "Epoch [492/10], Loss: 1.6377809047698975\n",
      "Epoch [493/10], Loss: 2.5494956970214844\n",
      "Epoch [494/10], Loss: 1.760873794555664\n",
      "Epoch [495/10], Loss: 2.1179919242858887\n",
      "Epoch [496/10], Loss: 1.9564520120620728\n",
      "Epoch [497/10], Loss: 2.035385847091675\n",
      "Epoch [498/10], Loss: 1.8115527629852295\n",
      "Epoch [499/10], Loss: 1.8270280361175537\n",
      "Epoch [500/10], Loss: 2.8123936653137207\n",
      "Epoch [501/10], Loss: 2.091510534286499\n",
      "Epoch [502/10], Loss: 2.0766987800598145\n",
      "Epoch [503/10], Loss: 1.6972887516021729\n",
      "Epoch [504/10], Loss: 2.9809207916259766\n",
      "Epoch [505/10], Loss: 2.573350429534912\n",
      "Epoch [506/10], Loss: 2.0809617042541504\n",
      "Epoch [507/10], Loss: 1.7231719493865967\n",
      "Epoch [508/10], Loss: 1.5642017126083374\n",
      "Epoch [509/10], Loss: 2.266193389892578\n",
      "Epoch [510/10], Loss: 2.0950779914855957\n",
      "Epoch [511/10], Loss: 1.9693169593811035\n",
      "Epoch [512/10], Loss: 1.6747945547103882\n",
      "Epoch [513/10], Loss: 1.4672529697418213\n",
      "Epoch [514/10], Loss: 1.8631606101989746\n",
      "Epoch [515/10], Loss: 1.5991487503051758\n",
      "Epoch [516/10], Loss: 1.5214277505874634\n",
      "Epoch [517/10], Loss: 1.4022586345672607\n",
      "Epoch [518/10], Loss: 1.3972153663635254\n",
      "Epoch [519/10], Loss: 2.365117073059082\n",
      "Epoch [520/10], Loss: 2.1092326641082764\n",
      "Epoch [521/10], Loss: 1.221616506576538\n",
      "Epoch [522/10], Loss: 1.6777253150939941\n",
      "Epoch [523/10], Loss: 3.622450113296509\n",
      "Epoch [524/10], Loss: 2.3667492866516113\n",
      "Epoch [525/10], Loss: 1.4549715518951416\n",
      "Epoch [526/10], Loss: 1.6182899475097656\n",
      "Epoch [527/10], Loss: 2.4352447986602783\n",
      "Epoch [528/10], Loss: 2.6970224380493164\n",
      "Epoch [529/10], Loss: 2.0345053672790527\n",
      "Epoch [530/10], Loss: 2.0912554264068604\n",
      "Epoch [531/10], Loss: 1.7336777448654175\n",
      "Epoch [532/10], Loss: 2.004802703857422\n",
      "Epoch [533/10], Loss: 1.8082300424575806\n",
      "Epoch [534/10], Loss: 2.0355734825134277\n",
      "Epoch [535/10], Loss: 1.894006371498108\n",
      "Epoch [536/10], Loss: 1.7559552192687988\n",
      "Epoch [537/10], Loss: 2.8636770248413086\n",
      "Epoch [538/10], Loss: 1.632054090499878\n",
      "Epoch [539/10], Loss: 2.2461979389190674\n",
      "Epoch [540/10], Loss: 1.8663740158081055\n",
      "Epoch [541/10], Loss: 2.230245351791382\n",
      "Epoch [542/10], Loss: 2.49155855178833\n",
      "Epoch [543/10], Loss: 2.435081958770752\n",
      "Epoch [544/10], Loss: 1.3094801902770996\n",
      "Epoch [545/10], Loss: 1.6214210987091064\n",
      "Epoch [546/10], Loss: 1.5563961267471313\n",
      "Epoch [547/10], Loss: 1.6239609718322754\n",
      "Epoch [548/10], Loss: 2.2851099967956543\n",
      "Epoch [549/10], Loss: 1.7122043371200562\n",
      "Epoch [550/10], Loss: 1.975883960723877\n",
      "Epoch [551/10], Loss: 1.9878945350646973\n",
      "Epoch [552/10], Loss: 2.5820624828338623\n",
      "Epoch [553/10], Loss: 1.3179811239242554\n",
      "Epoch [554/10], Loss: 1.6784189939498901\n",
      "Epoch [555/10], Loss: 1.9597151279449463\n",
      "Epoch [556/10], Loss: 1.906802773475647\n",
      "Epoch [557/10], Loss: 1.3794798851013184\n",
      "Epoch [558/10], Loss: 1.8685050010681152\n",
      "Epoch [559/10], Loss: 2.4932000637054443\n",
      "Epoch [560/10], Loss: 3.939732551574707\n",
      "Epoch [561/10], Loss: 1.635943055152893\n",
      "Epoch [562/10], Loss: 2.0977821350097656\n",
      "Epoch [563/10], Loss: 1.72404944896698\n",
      "Epoch [564/10], Loss: 1.8492517471313477\n",
      "Epoch [565/10], Loss: 2.7000608444213867\n",
      "Epoch [566/10], Loss: 2.999816417694092\n",
      "Epoch [567/10], Loss: 1.5559059381484985\n",
      "Epoch [568/10], Loss: 1.6077828407287598\n",
      "Epoch [569/10], Loss: 1.799217700958252\n",
      "Epoch [570/10], Loss: 2.030694007873535\n",
      "Epoch [571/10], Loss: 1.5303881168365479\n",
      "Epoch [572/10], Loss: 1.7299823760986328\n",
      "Epoch [573/10], Loss: 2.0422208309173584\n",
      "Epoch [574/10], Loss: 1.9675489664077759\n",
      "Epoch [575/10], Loss: 1.5571768283843994\n",
      "Epoch [576/10], Loss: 1.6400914192199707\n",
      "Epoch [577/10], Loss: 1.9400413036346436\n",
      "Epoch [578/10], Loss: 2.1550064086914062\n",
      "Epoch [579/10], Loss: 1.8944568634033203\n",
      "Epoch [580/10], Loss: 2.1295361518859863\n",
      "Epoch [581/10], Loss: 1.6215273141860962\n",
      "Epoch [582/10], Loss: 1.9997317790985107\n",
      "Epoch [583/10], Loss: 2.0878994464874268\n",
      "Epoch [584/10], Loss: 2.416994571685791\n",
      "Epoch [585/10], Loss: 1.766899824142456\n",
      "Epoch [586/10], Loss: 2.6536965370178223\n",
      "Epoch [587/10], Loss: 2.1656289100646973\n",
      "Epoch [588/10], Loss: 1.267878770828247\n",
      "Epoch [589/10], Loss: 1.827113151550293\n",
      "Epoch [590/10], Loss: 1.7321995496749878\n",
      "Epoch [591/10], Loss: 1.8201704025268555\n",
      "Epoch [592/10], Loss: 2.3869516849517822\n",
      "Epoch [593/10], Loss: 1.5946438312530518\n",
      "Epoch [594/10], Loss: 2.306865692138672\n",
      "Epoch [595/10], Loss: 1.964282512664795\n",
      "Epoch [596/10], Loss: 1.8855037689208984\n",
      "Epoch [597/10], Loss: 1.7726500034332275\n",
      "Epoch [598/10], Loss: 1.8979887962341309\n",
      "Epoch [599/10], Loss: 1.7354991436004639\n",
      "Epoch [600/10], Loss: 1.860952377319336\n",
      "Epoch [601/10], Loss: 2.3194704055786133\n",
      "Epoch [602/10], Loss: 2.1642496585845947\n",
      "Epoch [603/10], Loss: 3.1403114795684814\n",
      "Epoch [604/10], Loss: 2.1088790893554688\n",
      "Epoch [605/10], Loss: 2.119241952896118\n",
      "Epoch [606/10], Loss: 2.5988049507141113\n",
      "Epoch [607/10], Loss: 1.3891265392303467\n",
      "Epoch [608/10], Loss: 1.9112569093704224\n",
      "Epoch [609/10], Loss: 1.331487774848938\n",
      "Epoch [610/10], Loss: 1.9790093898773193\n",
      "Epoch [611/10], Loss: 1.5701913833618164\n",
      "Epoch [612/10], Loss: 1.6363744735717773\n",
      "Epoch [613/10], Loss: 1.9680448770523071\n",
      "Epoch [614/10], Loss: 1.6239372491836548\n",
      "Epoch [615/10], Loss: 2.200745105743408\n",
      "Epoch [616/10], Loss: 1.8615381717681885\n",
      "Epoch [617/10], Loss: 1.7305488586425781\n",
      "Epoch [618/10], Loss: 1.6016457080841064\n",
      "Epoch [619/10], Loss: 1.4194256067276\n",
      "Epoch [620/10], Loss: 1.7929296493530273\n",
      "Epoch [621/10], Loss: 1.8730995655059814\n",
      "Epoch [622/10], Loss: 1.651803970336914\n",
      "Epoch [623/10], Loss: 1.4539425373077393\n",
      "Epoch [624/10], Loss: 2.118605136871338\n",
      "Epoch [625/10], Loss: 2.0090997219085693\n",
      "Epoch [626/10], Loss: 2.4505929946899414\n",
      "Epoch [627/10], Loss: 1.8226823806762695\n",
      "Epoch [628/10], Loss: 3.277353286743164\n",
      "Epoch [629/10], Loss: 2.4802794456481934\n",
      "Epoch [630/10], Loss: 1.7464895248413086\n",
      "Epoch [631/10], Loss: 1.7693710327148438\n",
      "Epoch [632/10], Loss: 2.0445404052734375\n",
      "Epoch [633/10], Loss: 2.042895793914795\n",
      "Epoch [634/10], Loss: 2.0124311447143555\n",
      "Epoch [635/10], Loss: 1.4034249782562256\n",
      "Epoch [636/10], Loss: 2.1684858798980713\n",
      "Epoch [637/10], Loss: 2.312993049621582\n",
      "Epoch [638/10], Loss: 1.6576117277145386\n",
      "Epoch [639/10], Loss: 2.0029296875\n",
      "Epoch [640/10], Loss: 1.9220881462097168\n",
      "Epoch [641/10], Loss: 1.5144914388656616\n",
      "Epoch [642/10], Loss: 1.8145911693572998\n",
      "Epoch [643/10], Loss: 1.9314241409301758\n",
      "Epoch [644/10], Loss: 1.6218743324279785\n",
      "Epoch [645/10], Loss: 2.7816290855407715\n",
      "Epoch [646/10], Loss: 2.5636656284332275\n",
      "Epoch [647/10], Loss: 1.8392173051834106\n",
      "Epoch [648/10], Loss: 1.3073633909225464\n",
      "Epoch [649/10], Loss: 1.8467953205108643\n",
      "Epoch [650/10], Loss: 2.567603826522827\n",
      "Epoch [651/10], Loss: 1.8294296264648438\n",
      "Epoch [652/10], Loss: 2.348027229309082\n",
      "Epoch [653/10], Loss: 1.959939956665039\n",
      "Epoch [654/10], Loss: 2.2170841693878174\n",
      "Epoch [655/10], Loss: 1.6299893856048584\n",
      "Epoch [656/10], Loss: 1.5408262014389038\n",
      "Epoch [657/10], Loss: 1.5743532180786133\n",
      "Epoch [658/10], Loss: 2.0084221363067627\n",
      "Epoch [659/10], Loss: 2.112086534500122\n",
      "Epoch [660/10], Loss: 2.5567376613616943\n",
      "Epoch [661/10], Loss: 1.920925259590149\n",
      "Epoch [662/10], Loss: 1.4994274377822876\n",
      "Epoch [663/10], Loss: 1.6812686920166016\n",
      "Epoch [664/10], Loss: 2.1267478466033936\n",
      "Epoch [665/10], Loss: 1.6260662078857422\n",
      "Epoch [666/10], Loss: 2.117180347442627\n",
      "Epoch [667/10], Loss: 1.8897119760513306\n",
      "Epoch [668/10], Loss: 1.5958352088928223\n",
      "Epoch [669/10], Loss: 2.0960981845855713\n",
      "Epoch [670/10], Loss: 1.5281370878219604\n",
      "Epoch [671/10], Loss: 1.9774092435836792\n",
      "Epoch [672/10], Loss: 2.082615375518799\n",
      "Epoch [673/10], Loss: 2.032050609588623\n",
      "Epoch [674/10], Loss: 1.632380485534668\n",
      "Epoch [675/10], Loss: 2.379767894744873\n",
      "Epoch [676/10], Loss: 1.6847814321517944\n",
      "Epoch [677/10], Loss: 1.6602345705032349\n",
      "Epoch [678/10], Loss: 1.6497581005096436\n",
      "Epoch [679/10], Loss: 1.6267931461334229\n",
      "Epoch [680/10], Loss: 2.366748332977295\n",
      "Epoch [681/10], Loss: 1.6128379106521606\n",
      "Epoch [682/10], Loss: 1.5392634868621826\n",
      "Epoch [683/10], Loss: 1.9694052934646606\n",
      "Epoch [684/10], Loss: 2.0371036529541016\n",
      "Epoch [685/10], Loss: 1.4998546838760376\n",
      "Epoch [686/10], Loss: 2.9361205101013184\n",
      "Epoch [687/10], Loss: 1.4773244857788086\n",
      "Epoch [688/10], Loss: 1.8722940683364868\n",
      "Epoch [689/10], Loss: 3.0122475624084473\n",
      "Epoch [690/10], Loss: 2.014448642730713\n",
      "Epoch [691/10], Loss: 1.9010363817214966\n",
      "Epoch [692/10], Loss: 1.7127457857131958\n",
      "Epoch [693/10], Loss: 1.8603429794311523\n",
      "Epoch [694/10], Loss: 1.2948726415634155\n",
      "Epoch [695/10], Loss: 1.3949975967407227\n",
      "Epoch [696/10], Loss: 1.6592661142349243\n",
      "Epoch [697/10], Loss: 2.4936397075653076\n",
      "Epoch [698/10], Loss: 2.7333593368530273\n",
      "Epoch [699/10], Loss: 2.0135555267333984\n",
      "Epoch [700/10], Loss: 2.177485942840576\n",
      "Epoch [701/10], Loss: 1.5851572751998901\n",
      "Epoch [702/10], Loss: 2.1811742782592773\n",
      "Epoch [703/10], Loss: 1.7513697147369385\n",
      "Epoch [704/10], Loss: 1.4673941135406494\n",
      "Epoch [705/10], Loss: 1.8505690097808838\n",
      "Epoch [706/10], Loss: 2.218134880065918\n",
      "Epoch [707/10], Loss: 2.017317771911621\n",
      "Epoch [708/10], Loss: 1.6740591526031494\n",
      "Epoch [709/10], Loss: 1.7090163230895996\n",
      "Epoch [710/10], Loss: 1.7602779865264893\n",
      "Epoch [711/10], Loss: 1.5712981224060059\n",
      "Epoch [712/10], Loss: 2.0238513946533203\n",
      "Epoch [713/10], Loss: 1.9360101222991943\n",
      "Epoch [714/10], Loss: 1.8518710136413574\n",
      "Epoch [715/10], Loss: 1.4929373264312744\n",
      "Epoch [716/10], Loss: 1.7656911611557007\n",
      "Epoch [717/10], Loss: 1.542026162147522\n",
      "Epoch [718/10], Loss: 2.0466814041137695\n",
      "Epoch [719/10], Loss: 2.986467123031616\n",
      "Epoch [720/10], Loss: 1.7764198780059814\n",
      "Epoch [721/10], Loss: 1.470702052116394\n",
      "Epoch [722/10], Loss: 1.6604669094085693\n",
      "Epoch [723/10], Loss: 1.642293930053711\n",
      "Epoch [724/10], Loss: 1.4257707595825195\n",
      "Epoch [725/10], Loss: 4.458741188049316\n",
      "Epoch [726/10], Loss: 1.2606983184814453\n",
      "Epoch [727/10], Loss: 2.289654493331909\n",
      "Epoch [728/10], Loss: 2.8981871604919434\n",
      "Epoch [729/10], Loss: 1.6899783611297607\n",
      "Epoch [730/10], Loss: 1.5006167888641357\n",
      "Epoch [731/10], Loss: 2.213120222091675\n",
      "Epoch [732/10], Loss: 2.206613540649414\n",
      "Epoch [733/10], Loss: 2.646350860595703\n",
      "Epoch [734/10], Loss: 1.222017765045166\n",
      "Epoch [735/10], Loss: 1.9278640747070312\n",
      "Epoch [736/10], Loss: 1.942260503768921\n",
      "Epoch [737/10], Loss: 1.5276362895965576\n",
      "Epoch [738/10], Loss: 1.8595664501190186\n",
      "Epoch [739/10], Loss: 1.7450822591781616\n",
      "Epoch [740/10], Loss: 1.3373526334762573\n",
      "Epoch [741/10], Loss: 1.21201753616333\n",
      "Epoch [742/10], Loss: 1.2952572107315063\n",
      "Epoch [743/10], Loss: 2.1813716888427734\n",
      "Epoch [744/10], Loss: 2.067615032196045\n",
      "Epoch [745/10], Loss: 2.7416892051696777\n",
      "Epoch [746/10], Loss: 1.8782057762145996\n",
      "Epoch [747/10], Loss: 2.248671293258667\n",
      "Epoch [748/10], Loss: 1.422940969467163\n",
      "Epoch [749/10], Loss: 1.5601863861083984\n",
      "Epoch [750/10], Loss: 1.8853378295898438\n",
      "Epoch [751/10], Loss: 1.5231446027755737\n",
      "Epoch [752/10], Loss: 1.735037088394165\n",
      "Epoch [753/10], Loss: 1.0769230127334595\n",
      "Epoch [754/10], Loss: 1.9468748569488525\n",
      "Epoch [755/10], Loss: 1.6028666496276855\n",
      "Epoch [756/10], Loss: 2.2209322452545166\n",
      "Epoch [757/10], Loss: 1.581986665725708\n",
      "Epoch [758/10], Loss: 1.445286512374878\n",
      "Epoch [759/10], Loss: 1.4852442741394043\n",
      "Epoch [760/10], Loss: 1.385577917098999\n",
      "Epoch [761/10], Loss: 2.3953750133514404\n",
      "Epoch [762/10], Loss: 1.3701176643371582\n",
      "Epoch [763/10], Loss: 1.5905370712280273\n",
      "Epoch [764/10], Loss: 2.5556633472442627\n",
      "Epoch [765/10], Loss: 2.445314645767212\n",
      "Epoch [766/10], Loss: 2.0293726921081543\n",
      "Epoch [767/10], Loss: 1.8876862525939941\n",
      "Epoch [768/10], Loss: 1.569812536239624\n",
      "Epoch [769/10], Loss: 2.0277493000030518\n",
      "Epoch [770/10], Loss: 2.7044897079467773\n",
      "Epoch [771/10], Loss: 1.6200224161148071\n",
      "Epoch [772/10], Loss: 1.799170732498169\n",
      "Epoch [773/10], Loss: 2.000610828399658\n",
      "Epoch [774/10], Loss: 2.067500591278076\n",
      "Epoch [775/10], Loss: 2.518596649169922\n",
      "Epoch [776/10], Loss: 2.142932176589966\n",
      "Epoch [777/10], Loss: 1.400612473487854\n",
      "Epoch [778/10], Loss: 2.1607658863067627\n",
      "Epoch [779/10], Loss: 1.564878225326538\n",
      "Epoch [780/10], Loss: 1.9702543020248413\n",
      "Epoch [781/10], Loss: 1.9859346151351929\n",
      "Epoch [782/10], Loss: 1.1725088357925415\n",
      "Epoch [783/10], Loss: 1.5398964881896973\n",
      "Epoch [784/10], Loss: 1.5376631021499634\n",
      "Epoch [785/10], Loss: 1.448746681213379\n",
      "Epoch [786/10], Loss: 1.8605273962020874\n",
      "Epoch [787/10], Loss: 2.288813352584839\n",
      "Epoch [788/10], Loss: 1.900599479675293\n",
      "Epoch [789/10], Loss: 1.6606452465057373\n",
      "Epoch [790/10], Loss: 1.9510853290557861\n",
      "Epoch [791/10], Loss: 1.7288954257965088\n",
      "Epoch [792/10], Loss: 2.4755101203918457\n",
      "Epoch [793/10], Loss: 2.4670939445495605\n",
      "Epoch [794/10], Loss: 2.6556057929992676\n",
      "Epoch [795/10], Loss: 1.4830175638198853\n",
      "Epoch [796/10], Loss: 2.0072829723358154\n",
      "Epoch [797/10], Loss: 1.1529755592346191\n",
      "Epoch [798/10], Loss: 1.6617006063461304\n",
      "Epoch [799/10], Loss: 1.9318839311599731\n",
      "Epoch [800/10], Loss: 1.9303452968597412\n",
      "Epoch [801/10], Loss: 2.6205506324768066\n",
      "Epoch [802/10], Loss: 1.5932170152664185\n",
      "Epoch [803/10], Loss: 2.2414631843566895\n",
      "Epoch [804/10], Loss: 2.7481837272644043\n",
      "Epoch [805/10], Loss: 1.7681987285614014\n",
      "Epoch [806/10], Loss: 1.5704498291015625\n",
      "Epoch [807/10], Loss: 2.2272284030914307\n",
      "Epoch [808/10], Loss: 1.5611724853515625\n",
      "Epoch [809/10], Loss: 1.9874752759933472\n",
      "Epoch [810/10], Loss: 1.530076026916504\n",
      "Epoch [811/10], Loss: 2.2019271850585938\n",
      "Epoch [812/10], Loss: 1.6991920471191406\n",
      "Epoch [813/10], Loss: 1.9377588033676147\n",
      "Epoch [814/10], Loss: 2.0892748832702637\n",
      "Epoch [815/10], Loss: 4.452725887298584\n",
      "Epoch [816/10], Loss: 1.714762568473816\n",
      "Epoch [817/10], Loss: 2.1092474460601807\n",
      "Epoch [818/10], Loss: 2.732276678085327\n",
      "Epoch [819/10], Loss: 1.1964633464813232\n",
      "Epoch [820/10], Loss: 1.4351071119308472\n",
      "Epoch [821/10], Loss: 1.6476833820343018\n",
      "Epoch [822/10], Loss: 2.272181510925293\n",
      "Epoch [823/10], Loss: 1.8858774900436401\n",
      "Epoch [824/10], Loss: 1.6698246002197266\n",
      "Epoch [825/10], Loss: 1.5459892749786377\n",
      "Epoch [826/10], Loss: 1.7310516834259033\n",
      "Epoch [827/10], Loss: 1.2044774293899536\n",
      "Epoch [828/10], Loss: 1.551454782485962\n",
      "Epoch [829/10], Loss: 1.5497963428497314\n",
      "Epoch [830/10], Loss: 2.562128782272339\n",
      "Epoch [831/10], Loss: 1.2583808898925781\n",
      "Epoch [832/10], Loss: 1.6211752891540527\n",
      "Epoch [833/10], Loss: 2.200432777404785\n",
      "Epoch [834/10], Loss: 2.40769100189209\n",
      "Epoch [835/10], Loss: 1.589095950126648\n",
      "Epoch [836/10], Loss: 2.252293586730957\n",
      "Epoch [837/10], Loss: 2.129424810409546\n",
      "Epoch [838/10], Loss: 2.035202741622925\n",
      "Epoch [839/10], Loss: 2.212522029876709\n",
      "Epoch [840/10], Loss: 1.597813367843628\n",
      "Epoch [841/10], Loss: 1.5411641597747803\n",
      "Epoch [842/10], Loss: 2.393745183944702\n",
      "Epoch [843/10], Loss: 2.5194969177246094\n",
      "Epoch [844/10], Loss: 2.120149850845337\n",
      "Epoch [845/10], Loss: 2.0237417221069336\n",
      "Epoch [846/10], Loss: 1.9530503749847412\n",
      "Epoch [847/10], Loss: 1.343926191329956\n",
      "Epoch [848/10], Loss: 1.4697487354278564\n",
      "Epoch [849/10], Loss: 1.7266509532928467\n",
      "Epoch [850/10], Loss: 1.8322426080703735\n",
      "Epoch [851/10], Loss: 1.6206247806549072\n",
      "Epoch [852/10], Loss: 1.9118562936782837\n",
      "Epoch [853/10], Loss: 1.8392972946166992\n",
      "Epoch [854/10], Loss: 2.5899033546447754\n",
      "Epoch [855/10], Loss: 2.6398980617523193\n",
      "Epoch [856/10], Loss: 1.5155308246612549\n",
      "Epoch [857/10], Loss: 1.2140567302703857\n",
      "Epoch [858/10], Loss: 1.9914255142211914\n",
      "Epoch [859/10], Loss: 1.725638508796692\n",
      "Epoch [860/10], Loss: 1.5649635791778564\n",
      "Epoch [861/10], Loss: 1.0736724138259888\n",
      "Epoch [862/10], Loss: 1.653206706047058\n",
      "Epoch [863/10], Loss: 1.6615631580352783\n",
      "Epoch [864/10], Loss: 1.3551855087280273\n",
      "Epoch [865/10], Loss: 1.6267094612121582\n",
      "Epoch [866/10], Loss: 0.9821606278419495\n",
      "Epoch [867/10], Loss: 1.4839613437652588\n",
      "Epoch [868/10], Loss: 1.9357761144638062\n",
      "Epoch [869/10], Loss: 2.130953550338745\n",
      "Epoch [870/10], Loss: 2.994258403778076\n",
      "Epoch [871/10], Loss: 1.6927604675292969\n",
      "Epoch [872/10], Loss: 2.2220916748046875\n",
      "Epoch [873/10], Loss: 1.8634531497955322\n",
      "Epoch [874/10], Loss: 2.1408793926239014\n",
      "Epoch [875/10], Loss: 2.268012762069702\n",
      "Epoch [876/10], Loss: 1.7184159755706787\n",
      "Epoch [877/10], Loss: 1.989442229270935\n",
      "Epoch [878/10], Loss: 2.081979751586914\n",
      "Epoch [879/10], Loss: 2.1811935901641846\n",
      "Epoch [880/10], Loss: 2.346601963043213\n",
      "Epoch [881/10], Loss: 1.430362343788147\n",
      "Epoch [882/10], Loss: 2.084636926651001\n",
      "Epoch [883/10], Loss: 1.6815974712371826\n",
      "Epoch [884/10], Loss: 1.9423134326934814\n",
      "Epoch [885/10], Loss: 2.1781058311462402\n",
      "Epoch [886/10], Loss: 1.3747109174728394\n",
      "Epoch [887/10], Loss: 1.357620358467102\n",
      "Epoch [888/10], Loss: 2.2925124168395996\n",
      "Epoch [889/10], Loss: 1.7194068431854248\n",
      "Epoch [890/10], Loss: 1.0690194368362427\n",
      "Epoch [891/10], Loss: 1.9157185554504395\n",
      "Epoch [892/10], Loss: 1.7300304174423218\n",
      "Epoch [893/10], Loss: 1.6206884384155273\n",
      "Epoch [894/10], Loss: 2.493471622467041\n",
      "Epoch [895/10], Loss: 1.625274896621704\n",
      "Epoch [896/10], Loss: 1.721166968345642\n",
      "Epoch [897/10], Loss: 1.7916858196258545\n",
      "Epoch [898/10], Loss: 1.6031019687652588\n",
      "Epoch [899/10], Loss: 1.9219497442245483\n",
      "Epoch [900/10], Loss: 1.8763763904571533\n",
      "Epoch [901/10], Loss: 1.6727479696273804\n",
      "Epoch [902/10], Loss: 1.7514019012451172\n",
      "Epoch [903/10], Loss: 2.066710948944092\n",
      "Epoch [904/10], Loss: 1.7385956048965454\n",
      "Epoch [905/10], Loss: 1.9310463666915894\n",
      "Epoch [906/10], Loss: 2.806157350540161\n",
      "Epoch [907/10], Loss: 3.003535747528076\n",
      "Epoch [908/10], Loss: 2.210500717163086\n",
      "Epoch [909/10], Loss: 1.9123728275299072\n",
      "Epoch [910/10], Loss: 2.6859524250030518\n",
      "Epoch [911/10], Loss: 1.5775195360183716\n",
      "Epoch [912/10], Loss: 1.942584753036499\n",
      "Epoch [913/10], Loss: 1.7547261714935303\n",
      "Epoch [914/10], Loss: 2.422102451324463\n",
      "Epoch [915/10], Loss: 1.7266626358032227\n",
      "Epoch [916/10], Loss: 1.5758804082870483\n",
      "Epoch [917/10], Loss: 2.3245911598205566\n",
      "Epoch [918/10], Loss: 1.2391389608383179\n",
      "Epoch [919/10], Loss: 1.4926061630249023\n",
      "Epoch [920/10], Loss: 1.8316985368728638\n",
      "Epoch [921/10], Loss: 1.5638833045959473\n",
      "Epoch [922/10], Loss: 2.084355115890503\n",
      "Epoch [923/10], Loss: 1.6326321363449097\n",
      "Epoch [924/10], Loss: 1.957313895225525\n",
      "Epoch [925/10], Loss: 1.950141191482544\n",
      "Epoch [926/10], Loss: 2.0036427974700928\n",
      "Epoch [927/10], Loss: 1.5912519693374634\n",
      "Epoch [928/10], Loss: 2.3764867782592773\n",
      "Epoch [929/10], Loss: 1.6450722217559814\n",
      "Epoch [930/10], Loss: 2.254161834716797\n",
      "Epoch [931/10], Loss: 1.8000653982162476\n",
      "Epoch [932/10], Loss: 2.2821364402770996\n",
      "Epoch [933/10], Loss: 1.9066293239593506\n",
      "Epoch [934/10], Loss: 2.1070573329925537\n",
      "Epoch [935/10], Loss: 1.324370265007019\n",
      "Epoch [936/10], Loss: 2.7556121349334717\n",
      "Epoch [937/10], Loss: 1.929457664489746\n",
      "Epoch [938/10], Loss: 2.3698291778564453\n",
      "Epoch [939/10], Loss: 1.5121307373046875\n",
      "Epoch [940/10], Loss: 1.729844331741333\n",
      "Epoch [941/10], Loss: 2.4767351150512695\n",
      "Epoch [942/10], Loss: 2.0288166999816895\n",
      "Epoch [943/10], Loss: 1.6576987504959106\n",
      "Epoch [944/10], Loss: 1.6669977903366089\n",
      "Epoch [945/10], Loss: 1.8480334281921387\n",
      "Epoch [946/10], Loss: 1.6719434261322021\n",
      "Epoch [947/10], Loss: 1.5662800073623657\n",
      "Epoch [948/10], Loss: 1.986380934715271\n",
      "Epoch [949/10], Loss: 2.505687713623047\n",
      "Epoch [950/10], Loss: 2.3340601921081543\n",
      "Epoch [951/10], Loss: 1.641485333442688\n",
      "Epoch [952/10], Loss: 1.4878872632980347\n",
      "Epoch [953/10], Loss: 1.619410514831543\n",
      "Epoch [954/10], Loss: 1.939144253730774\n",
      "Epoch [955/10], Loss: 2.7526376247406006\n",
      "Epoch [956/10], Loss: 1.6883797645568848\n",
      "Epoch [957/10], Loss: 1.4660632610321045\n",
      "Epoch [958/10], Loss: 1.6771948337554932\n",
      "Epoch [959/10], Loss: 2.644184112548828\n",
      "Epoch [960/10], Loss: 1.6584882736206055\n",
      "Epoch [961/10], Loss: 3.1739728450775146\n",
      "Epoch [962/10], Loss: 1.6888855695724487\n",
      "Epoch [963/10], Loss: 2.057955026626587\n",
      "Epoch [964/10], Loss: 1.8732134103775024\n",
      "Epoch [965/10], Loss: 1.9659852981567383\n",
      "Epoch [966/10], Loss: 1.7010349035263062\n",
      "Epoch [967/10], Loss: 1.3498802185058594\n",
      "Epoch [968/10], Loss: 1.514254093170166\n",
      "Epoch [969/10], Loss: 1.6727770566940308\n",
      "Epoch [970/10], Loss: 1.5555105209350586\n",
      "Epoch [971/10], Loss: 1.9679681062698364\n",
      "Epoch [972/10], Loss: 1.2844406366348267\n",
      "Epoch [973/10], Loss: 1.6431972980499268\n",
      "Epoch [974/10], Loss: 1.396484613418579\n",
      "Epoch [975/10], Loss: 2.326098918914795\n",
      "Epoch [976/10], Loss: 1.3858401775360107\n",
      "Epoch [977/10], Loss: 2.081460475921631\n",
      "Epoch [978/10], Loss: 2.239358425140381\n",
      "Epoch [979/10], Loss: 1.6044014692306519\n",
      "Epoch [980/10], Loss: 1.5155622959136963\n",
      "Epoch [981/10], Loss: 1.5269325971603394\n",
      "Epoch [982/10], Loss: 1.6893689632415771\n",
      "Epoch [983/10], Loss: 1.689719796180725\n",
      "Epoch [984/10], Loss: 1.5793761014938354\n",
      "Epoch [985/10], Loss: 1.8380743265151978\n",
      "Epoch [986/10], Loss: 2.5075831413269043\n",
      "Epoch [987/10], Loss: 1.918447732925415\n",
      "Epoch [988/10], Loss: 1.544697880744934\n",
      "Epoch [989/10], Loss: 1.5288249254226685\n",
      "Epoch [990/10], Loss: 1.2942419052124023\n",
      "Epoch [991/10], Loss: 1.7204493284225464\n",
      "Epoch [992/10], Loss: 1.6100268363952637\n",
      "Epoch [993/10], Loss: 1.8431391716003418\n",
      "Epoch [994/10], Loss: 1.954702377319336\n",
      "Epoch [995/10], Loss: 1.174422264099121\n",
      "Epoch [996/10], Loss: 1.2774479389190674\n",
      "Epoch [997/10], Loss: 1.829758644104004\n",
      "Epoch [998/10], Loss: 1.6561315059661865\n",
      "Epoch [999/10], Loss: 2.263021945953369\n",
      "Epoch [1000/10], Loss: 1.6097571849822998\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 64 * 64)  # Flatten the feature maps\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset (you can replace this with your own dataset)\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(root=\"D:\\Casper\\OTHER\\Data\\cifar\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (you can adjust the number of epochs)\n",
    "for epoch in range(1000):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on a validation/test dataset\n",
    "# (similar to the training loop but using validation/test data)\n",
    "# ...\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
