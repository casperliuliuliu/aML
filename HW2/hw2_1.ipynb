{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "path = \"D:\\Casper\\OTHER\\Data\\MNIST2\\HW2_MNIST_train\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "# print(f\"Files and directories in '{path}':\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.7734375\t0.4296875\t0.21875\t0.21875\n",
      "\n",
      "(128, 128, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAA70lEQVR4nO3SsUqCYRTG8QdpCIcKGhpd3JLadfVKqtG1oRuw7qALiMAl0CnEvZDGKHEvok9CP6hJzvkauoC35eWA/H/zy3n+wysBAAAAAAAAAAAAAAAA2dT7c/dpM2x/Z/DzfPm0Hm8H7bfeVz1JV3YSs3/8tupKarcegwI+7EzS3uJ7GRNwUV1L0qG7n4YETPxI0sHIirvdkICu30qNYWkvIfOSpuuiKP3LHqICmmOzUfvGzqMC/sw8GVDLGlBVySd5A/4hd0CZ+X7Cq+2H7nfckwFbWQvSf3DjP+H9MjTgsxx6zvsAAAAAAAAANsMvMtxFp2C/fUIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(f\"{path}/0000002.txt\", \"r\")\n",
    "print(f.read())\n",
    "img = Image.open(f'{path}/0121744.png')\n",
    "img = np.array(img)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img[:,:,2])\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(f'{path}/0121700.png')\n",
    "r, g, b, trash = img.split()\n",
    "\n",
    "r_array = np.array(r)\n",
    "g_array = np.array(g)\n",
    "b_array = np.array(b)\n",
    "\n",
    "# Check if all channels have the same array values\n",
    "are_channels_same = np.array_equal(r_array, g_array) and np.array_equal(g_array, b_array)\n",
    "are_channels_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_files = [f for f in os.listdir(data_folder) if f.endswith(\".png\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_folder, self.image_files[idx])\n",
    "        image = read_image(image_path)\n",
    "        image = image[0:3].float()\n",
    "        # image = image.reshape(1, 128, 128)\n",
    "        \n",
    "        txt_file = os.path.splitext(self.image_files[idx])[0] + \".txt\"\n",
    "        txt_path = os.path.join(self.data_folder, txt_file)\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "            class_label = int(first_line.split()[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_label\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((518, 518)),\n",
    "                                ])\n",
    "data_folder = path\n",
    "custom_dataset = CustomImageDataset(data_folder, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "def get_dataloaders(data_folder, transform, train_ratio, val_ratio, batch_size):\n",
    "    # Create a single merged dataset\n",
    "    train_dataset = CustomImageDataset(data_folder, transform)\n",
    "    val_dataset = CustomImageDataset(data_folder, transform)\n",
    "    test_dataset = CustomImageDataset(data_folder, transform)\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"Data Transform: {transform}\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [62101, 18620, 94004, 19872, 2395]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 97396 datapoints\n",
      "Number of train samples: 0 batches/ 0 datapoints\n",
      "Number of val samples: 1522 batches/ 97396 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "Data Transform: Compose(\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaders = get_dataloaders(data_folder, transform, 0, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFICIAL START, Focus on the 3rd problem\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def pprint(output = '\\n', show_time = False): # print and fprint at the same time\n",
    "    filename = \"hw2-1-1-MAR24.txt\"\n",
    "    print(output)\n",
    "    with open(filename, 'a') as f:\n",
    "        if show_time:\n",
    "            f.write(datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \"))\n",
    "\n",
    "        f.write(str(output))\n",
    "        f.write('\\n')\n",
    "pprint(\"OFFICIAL START, Focus on the 3rd problem\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model_lists, model_name):\n",
    "    model = model_lists[model_name]()\n",
    "    if 'res' in model_name:\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_features, 10)\n",
    "    elif 'swin' in model_name:\n",
    "        num_features = model.head.in_features\n",
    "        model.head = torch.nn.Linear(num_features, 10)\n",
    "\n",
    "\n",
    "    pprint(f\"Training model: {model_name}\", True)\n",
    "    model_parameters_amount = count_parameters(model)\n",
    "    pprint(f\"Total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr= 0.005\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)\n",
    "    pprint(f\"Learning rate={lr}\")\n",
    "    iteration = 0\n",
    "    epochs = 25\n",
    "    start = time.time()\n",
    "    phases = ['train', 'val']\n",
    "    for epoch in range(epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            correct_top3_predictions = 0\n",
    "            total_samples = 0\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            for images, labels in tqdm(loaders[phase]): # Iterate over data.\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if phase == 'train': # backward + optimize only if in training phase\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                \n",
    "                _, top3_preds = outputs.topk(3, 1, True, True)\n",
    "                correct_top3_predictions += sum([labels[i] in top3_preds[i] for i in range(labels.size(0))])\n",
    "\n",
    "                total_samples += labels.size(0)\n",
    "                iteration += 1\n",
    "\n",
    "            avg_loss = running_loss / total_samples\n",
    "            top1_accuracy = correct_predictions / total_samples * 100\n",
    "            top3_accuracy = correct_top3_predictions / total_samples * 100\n",
    "            pprint(f\"Epoch [{epoch+1}/{epochs}], phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, Top-1 Accuracy: {top1_accuracy:.2f}%, Top-3 Accuracy: {top3_accuracy:.2f}%\")\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    pprint(f\"Elapsed time: {duration} seconds\")\n",
    "    model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "    model_scripted.save(f'{model_name}.pt') # Save\n",
    "    pprint(f\"weight saved as: {model_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# model_lists = {\n",
    "#     'resnet18': lambda: models.resnet18(weights=models.ResNet18_Weights.DEFAULT),\n",
    "#     'resnet50': lambda: models.resnet50(weights=models.ResNet50_Weights.DEFAULT),\n",
    "#     'resnet101': lambda: models.resnet101(weights=models.ResNet101_Weights.DEFAULT),\n",
    "#     'resnet152': lambda: models.resnet152(weights=models.ResNet152_Weights.DEFAULT),\n",
    "#     'swin_v2_t': lambda: models.swin_v2_t(weights=models.Swin_V2_T_Weights.DEFAULT),\n",
    "#     'swin_v2_s': lambda: models.swin_v2_s(weights=models.Swin_V2_S_Weights.DEFAULT),\n",
    "#     'swin_v2_b': lambda: models.swin_v2_b(weights=models.Swin_V2_B_Weights.DEFAULT),\n",
    "# }\n",
    "\n",
    "# model_names = [\n",
    "#     'resnet18',\n",
    "#     'resnet50',\n",
    "#     'resnet101',\n",
    "#     'resnet152',\n",
    "#     'swin_v2_t',\n",
    "#     'swin_v2_s',\n",
    "#     'swin_v2_b',\n",
    "# ]\n",
    "# for model_name in model_names:\n",
    "#     train(model_lists, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(output_list):\n",
    "    stacked_outputs = torch.stack(output_list)\n",
    "    avg_probs = torch.mean(stacked_outputs, dim=0)\n",
    "    \n",
    "    return avg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval(model_list):\n",
    "    for model in model_list:\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start = time.time()\n",
    "    phases = ['val']\n",
    "    num_class = 10\n",
    "    for phase in phases:\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        correct_top3_predictions = 0\n",
    "        total_samples = 0\n",
    "        confus = torch.zeros(num_class, num_class,dtype=int)            \n",
    "        for images, labels in tqdm(loaders[phase]): # Iterate over data.\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            model_outputs = []\n",
    "            for model in model_list:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(images)\n",
    "                    model_outputs.append(outputs)\n",
    "            vote_outputs = majority_voting(model_outputs)\n",
    "            loss = criterion(vote_outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(vote_outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            _, top3_preds = outputs.topk(3, 1, True, True)\n",
    "            correct_top3_predictions += sum([labels[i] in top3_preds[i] for i in range(labels.size(0))])\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            for ii in range(len(predicted)):\n",
    "                confus[ labels.data[ii] ][ predicted[ii] ]+=1\n",
    "\n",
    "        avg_loss = running_loss / total_samples\n",
    "        top1_accuracy = correct_predictions / total_samples * 100\n",
    "        top3_accuracy = correct_top3_predictions / total_samples * 100\n",
    "        pprint(f\"Phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, Top-1 Accuracy: {top1_accuracy:.2f}%, Top-3 Accuracy: {top3_accuracy:.2f}%\")\n",
    "        for ii in range(num_class) :\n",
    "            pprint(f\"class {ii}:{confus.numpy()[ii]}\")\n",
    "        # pprint(confus)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    pprint(f\"Elapsed time: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2-1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1522/1522 [05:21<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: val, samples: 97396, Loss: 0.0000, Top-1 Accuracy: 99.96%, Top-3 Accuracy: 100.00%\n",
      "class 0:[10785     0     0     0     0     0     0     0     0     0]\n",
      "class 1:[    0 10826     0     0     3     0     0     6     0     0]\n",
      "class 2:[   0    0 9529    0    0    0    0    2    0    0]\n",
      "class 3:[   0    0    1 9809    0    1    0    0    1    0]\n",
      "class 4:[   0    0    0    0 9312    0    0    1    0    1]\n",
      "class 5:[   0    0    0    2    0 8683    2    0    0    0]\n",
      "class 6:[   1    2    0    1    0    1 9469    0    0    0]\n",
      "class 7:[    0     0     0     0     0     0     0 10007     0     0]\n",
      "class 8:[   0    0    1    0    0    0    0    0 9425    1]\n",
      "class 9:[   0    0    0    0    7    0    0    1    0 9516]\n",
      "Elapsed time: 321.2854561805725 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "path = \"D:\\\\Casper\\\\aML\\\\HW2\\\\\"\n",
    "path2 = \"D:\\\\Casper\\\\aML\\\\HW2\\\\\"\n",
    "model_list = [\n",
    "    torch.jit.load(f'{path}resnet152_hw1_99.87268470984435.pt'),\n",
    "    torch.jit.load(f'{path}resnet152_hw1_99.7576902542199.pt'),\n",
    "    torch.jit.load(f'{path}resnet152_hw1_99.8398291510945.pt'),\n",
    "    torch.jit.load(f'{path}resnet152_hw1_99.79362602160253.pt'),\n",
    "    torch.jit.load(f'{path}resnet152_hw1_99.81621421824305.pt'),\n",
    "]\n",
    "\n",
    "# model_name = [\n",
    "#     'resnet152',\n",
    "# ]\n",
    "eval(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2-1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2_1_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m, in \u001b[0;36mevaluation\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m     13\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "loaders = get_dataloaders(data_folder, transform, 1, 0.5, 1)\n",
    "model2_1_2 = torch.jit.load(f'D:\\\\Casper\\\\aML\\\\HW2\\\\OneParaModel.pt')\n",
    "\n",
    "def evaluation(model, data_loader):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader):\n",
    "            outputs = model(inputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Total accuracy: {accuracy*100:.2f}%\")\n",
    "evaluation(model2_1_2, loaders['train'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2-1-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torchvision/models/resnet.py\", line 22, in forward\n  def forward(self: __torch__.torchvision.models.resnet.ResNet,\n    x: Tensor) -> Tensor:\n    return (self)._forward_impl(x, )\n            ~~~~~~~~~~~~~~~~~~~ <--- HERE\n  def _forward_impl(self: __torch__.torchvision.models.resnet.ResNet,\n    x: Tensor) -> Tensor:\n  File \"code/__torch__/torchvision/models/resnet.py\", line 26, in _forward_impl\n    x: Tensor) -> Tensor:\n    conv1 = self.conv1\n    x0 = (conv1).forward(x, )\n          ~~~~~~~~~~~~~~ <--- HERE\n    bn1 = self.bn1\n    x1 = (bn1).forward(x0, )\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [2, 2], [3, 3], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 285, in forward\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 268, in _forward_impl\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n            ~~~~~~~~~~ <--- HERE\n        x = self.bn1(x)\n        x = self.relu(x)\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 460, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 456, in _conv_forward\n                            weight, bias, self.stride,\n                            _pair(0), self.dilation, self.groups)\n        return F.conv2d(input, weight, bias, self.stride,\n               ~~~~~~~~ <--- HERE\n                        self.padding, self.dilation, self.groups)\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mresnet152_hw1_99.87268470984435.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mpredict_folder_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m, in \u001b[0;36mpredict_folder_images\u001b[1;34m(folder_path, model, output_csv_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         model_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n\u001b[0;32m     29\u001b[0m vote_outputs \u001b[38;5;241m=\u001b[39m majority_voting(model_outputs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torchvision/models/resnet.py\", line 22, in forward\n  def forward(self: __torch__.torchvision.models.resnet.ResNet,\n    x: Tensor) -> Tensor:\n    return (self)._forward_impl(x, )\n            ~~~~~~~~~~~~~~~~~~~ <--- HERE\n  def _forward_impl(self: __torch__.torchvision.models.resnet.ResNet,\n    x: Tensor) -> Tensor:\n  File \"code/__torch__/torchvision/models/resnet.py\", line 26, in _forward_impl\n    x: Tensor) -> Tensor:\n    conv1 = self.conv1\n    x0 = (conv1).forward(x, )\n          ~~~~~~~~~~~~~~ <--- HERE\n    bn1 = self.bn1\n    x1 = (bn1).forward(x0, )\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [2, 2], [3, 3], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 285, in forward\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 268, in _forward_impl\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n            ~~~~~~~~~~ <--- HERE\n        x = self.bn1(x)\n        x = self.relu(x)\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 460, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"c:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 456, in _conv_forward\n                            weight, bias, self.stride,\n                            _pair(0), self.dilation, self.groups)\n        return F.conv2d(input, weight, bias, self.stride,\n               ~~~~~~~~ <--- HERE\n                        self.padding, self.dilation, self.groups)\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n"
     ]
    }
   ],
   "source": [
    "# def predict_folder_images(folder_path, model, output_csv_path):\n",
    "#     # Ensure the model is in evaluation mode\n",
    "#     model.eval()\n",
    "\n",
    "#     # Define the transformation for the input images\n",
    "#     transform = transforms.Compose([\n",
    "#         # transforms.Resize((224, 224)),  # Resize images\n",
    "#         transforms.ToTensor(),  # Convert to tensor\n",
    "#         # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "#     ])\n",
    "    \n",
    "#     results = []\n",
    "\n",
    "#     # Loop through all PNG files in the folder\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith('.png'):\n",
    "#             # Construct the full path to the image file\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "#             # Load and transform the image\n",
    "#             image = Image.open(file_path).convert('RGB')\n",
    "#             image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "                        \n",
    "#             model_outputs = []\n",
    "#             for model in model_list:\n",
    "#                 with torch.no_grad():\n",
    "#                     outputs = model(image_tensor)\n",
    "#                     model_outputs.append(outputs)\n",
    "#             vote_outputs = majority_voting(model_outputs)\n",
    "#             _, predicted = torch.max(vote_outputs, 1)\n",
    "\n",
    "#             # Save the filename and the predicted class\n",
    "#             results.append([filename, predicted])\n",
    "\n",
    "#     # Write results to a CSV file\n",
    "#     with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow(['image', 'class'])  # Write the header\n",
    "#         writer.writerows(results)\n",
    "\n",
    "# folder_path = 'D:\\\\Casper\\\\OTHER\\\\Data\\\\MNIST2\\\\HW2_MNIST_test'\n",
    "# output_csv_path = 'predictions.csv'\n",
    "# model = torch.jit.load(f'{path}resnet152_hw1_99.87268470984435.pt')\n",
    "# predict_folder_images(folder_path, model, output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
